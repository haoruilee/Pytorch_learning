{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型的评价\n",
    "\n",
    "perplexity\n",
    "\n",
    "$\\begin{aligned} P P(W) &=P\\left(w_{1} w_{2} \\ldots w_{N}\\right)^{-\\frac{1}{N}} \\\\ &=\\sqrt{\\frac{1}{P\\left(w_{1} w_{2} \\ldots w_{N}\\right)}} \\\\ \\operatorname{PP}(W) &=\\sqrt[n]{\\prod_{i=1}^{N} \\frac{1}{P\\left(w_{i} | w_{1} \\ldots w_{i-1}\\right)}} \\end{aligned}$\n",
    "\n",
    "\n",
    "-1是因为数字很小\n",
    "\n",
    "\n",
    "1/N是一段话长N个单词\n",
    "\n",
    "越低越好\n",
    "\n",
    "$J=-\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{j=1}^{|V|} y_{t, j} \\log \\hat{y}_{t, j}$\n",
    "\n",
    "perplexity=2^J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三课 语言模型和torchtext(专门为NLP开发的库）\n",
    "\n",
    "褚则伟 zeweichu@gmail.com\n",
    "\n",
    "学习目标\n",
    "- 学习语言模型，以及如何训练一个语言模型\n",
    "- 学习torchtext的基本使用方法\n",
    "    - 构建 vocabulary\n",
    "    - word to inde 和 index to word\n",
    "- 学习torch.nn的一些基本模型\n",
    "    - Linear\n",
    "    - RNN\n",
    "    - LSTM\n",
    "    - GRU\n",
    "- RNN的训练技巧\n",
    "    - Gradient Clipping\n",
    "- 如何保存和读取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值\n",
    "random.seed(53113)\n",
    "np.random.seed(53113)\n",
    "torch.manual_seed(53113)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(53113)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 100\n",
    "MAX_VOCAB_SIZE = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 16:06:25.186644 140020917724928 file_utils.py:38] PyTorch version 1.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们会继续使用上次的text8作为我们的训练，验证和测试数据\n",
    "- TorchText的一个重要概念是`Field`，它决定了你的数据会如何被处理。我们使用`TEXT`这个field来处理文本数据。我们的`TEXT` field有`lower=True`这个参数，所以所有的单词都会被lowercase。\n",
    "- torchtext提供了LanguageModelingDataset这个class来帮助我们处理语言模型数据集。\n",
    "- `build_vocab`可以根据我们提供的训练数据集来创建最高频单词的单词表，`max_size`帮助我们限定单词总量。\n",
    "- BPTTIterator可以连续地得到连贯的句子，BPTT的全程是back propagation through time。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 50002\n"
     ]
    }
   ],
   "source": [
    "#用torch.text完成把文本读进来的问题\n",
    "#第一步：定义Field,相当于告诉程序，我输入的是一些什么\n",
    "#因为全都是单词，所以可以lowercase\n",
    "TEXT = torchtext.data.Field(lower=True)\n",
    "#torch中的LanguageModelingDataset是专门给文本文件写的Dataset,#path=\".\"就是全在当前文件夹下\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(path=\".\", \n",
    "    train=\"text8.train.txt\", validation=\"text8.dev.txt\", test=\"text8.test.txt\", text_field=TEXT)\n",
    "#pytorch写好了如何构建单词表\n",
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
    "print(\"vocabulary size: {}\".format(len(TEXT.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10]#unk表示不常见或者不认识的字词，<pad>是为了防止句子太短"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi[\"<unk>\"]#字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1273"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi[\"apple\"]#字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=BATCH_SIZE, device = torch.device(\"cuda:3\"), bptt_len=32, repeat=False, shuffle=True)\n",
    "#bptt_len是指：根据时间步的反向传播BPTT的长度\n",
    "device = torch.device(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".text\n",
      "had dropped to just three zero zero zero k it was then cool enough to allow the nuclei to capture electrons this process is called recombination during which the first neutral atoms\n",
      "target\n",
      "dropped to just three zero zero zero k it was then cool enough to allow the nuclei to capture electrons this process is called recombination during which the first neutral atoms took\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it)\n",
    "print(\".text\")#当前的文本\n",
    "print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:,1].data]))\n",
    "print(\"target\")#需要预测的文本\n",
    "print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:,1].data]))\n",
    "#发现只差了一个位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :\n",
      "at first contact with the japanese people was friendly and both were equals in a trade relationship however eventually the japanese started to dominate the relationship and soon established large settlements on\n",
      "\n",
      "first contact with the japanese people was friendly and both were equals in a trade relationship however eventually the japanese started to dominate the relationship and soon established large settlements on the\n",
      "1 :\n",
      "the outskirts of ainu territory as the japanese moved north and took control over their traditional lands the ainu often gave up without resistance but there was occasional resistance as exemplified in\n",
      "\n",
      "outskirts of ainu territory as the japanese moved north and took control over their traditional lands the ainu often gave up without resistance but there was occasional resistance as exemplified in wars\n",
      "2 :\n",
      "wars in one four five seven one six six nine and one seven eight nine all of which were lost by the ainu japanese policies became increasingly aimed at assimilating the ainu\n",
      "\n",
      "in one four five seven one six six nine and one seven eight nine all of which were lost by the ainu japanese policies became increasingly aimed at assimilating the ainu in\n",
      "3 :\n",
      "in the meiji period outlawing their language and restricting them to farming on government provided plots ainu were also used in near slavery conditions in the japanese fishing industry the island of\n",
      "\n",
      "the meiji period outlawing their language and restricting them to farming on government provided plots ainu were also used in near slavery conditions in the japanese fishing industry the island of hokkaido\n",
      "4 :\n",
      "hokkaido was called <unk> or <unk> chi during the edo period its name was changed to hokkaido during the meiji restoration as part of the programme to unify the japanese national character\n",
      "\n",
      "was called <unk> or <unk> chi during the edo period its name was changed to hokkaido during the meiji restoration as part of the programme to unify the japanese national character under\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    batch = next(it)\n",
    "    print(i,\":\")\n",
    "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:,2].data]))\n",
    "    print()\n",
    "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:,2].data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\" 一个简单的循环神经网络\"\"\"\n",
    "    #rnn_type是指循环神经网络的类型，ntoken是指vocab_size\n",
    "    #nlayers是指可以一层套一层\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        ''' 该模型包含以下几层:\n",
    "            - 词嵌入层\n",
    "            - 一个循环神经网络层(RNN, LSTM, GRU)\n",
    "            - 一个线性层，从hidden state到输出单词表\n",
    "            - 一个dropout层，用来做regularization\n",
    "        '''\n",
    "        super(RNNModel, self).__init__()#这段背下来就行了\n",
    "        ###########################\n",
    "        #pytorch自己有LSTM\n",
    "        ##########################\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        ''' Forward pass:\n",
    "            - word embedding\n",
    "            - 输入循环神经网络\n",
    "            - 一个线性层从hidden state转化为输出单词表\n",
    "        '''\n",
    "        #习惯把size写下来text:seq_length*batch_size,默认batch_size是在后面的\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        #output:seq_length*batch_size*hidden_size\n",
    "        #hidden有两个，因为LSTM传递了两个:(1*batch_size*hidden_size，1*batch_size*hidden_size)\n",
    "        #用1乘是因为只有一层，其实可以有好多层好多方向\n",
    "        \n",
    "        output = self.drop(output)\n",
    "        \n",
    "        #decode:(seq_length*batch_size)*vocab_size\n",
    "        #这里其实可以加激活层，但是一般都不加，相当于惯例\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        #因为decode是线性变换（其实应该叫linear），只能两维？？所以要先view?  视频50:58\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz, requires_grad=True):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            #########################\n",
    "            #使用weight.new_zeros()创建纯零矩阵，因为不知道数据在GPU还是CPU上，所以创建一个相同类型的\n",
    "            ##################\n",
    "            return (weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad),\n",
    "                    weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad))\n",
    "        else:\n",
    "            return weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如果使用模型读取的方法，则可直接去运行模型读取部分的kernal，不必初始化\n",
    "\n",
    "# 但如果想查看模型的pp等，还是要初始化并定于检查函数等\n",
    "初始化一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda(device = torch.device(\"cuda:3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0148, -0.0025, -0.0085,  ..., -0.0337,  0.0600, -0.0214],\n",
       "        [-0.0485, -0.1032, -0.0134,  ..., -0.1146,  0.1165, -0.0539],\n",
       "        [ 0.2117, -0.2793, -0.3717,  ..., -0.4380,  0.0778, -0.4988],\n",
       "        ...,\n",
       "        [-0.2929,  0.1851, -0.0309,  ...,  0.0359, -0.0244, -0.1295],\n",
       "        [ 0.0136, -0.0324, -0.0006,  ...,  0.0716, -0.0505, -0.1268],\n",
       "        [-0.1441,  0.1106,  0.0614,  ...,  0.0405, -0.0568, -0.0181]],\n",
       "       device='cuda:3', requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看一下模型在哪\n",
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型，所有模型训练都是大同小异\n",
    "\n",
    "- 我们首先定义评估模型的代码。\n",
    "- 模型的评估和模型的训练逻辑基本相同，唯一的区别是我们只需要forward pass，不需要backward pass，还有进来的时候首先model.eval()，最后改回model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    it = iter(data)\n",
    "    total_count = 0.\n",
    "    with torch.no_grad():#因为在评价，所以要确保没有梯度\n",
    "        hidden = model.init_hidden(BATCH_SIZE, requires_grad=False)\n",
    "        for i, batch in enumerate(it):\n",
    "            data, target = batch.text, batch.target\n",
    "            if USE_CUDA:\n",
    "                data, target = data.cuda(device = torch.device(\"cuda:3\")), target.cuda(device = torch.device(\"cuda:3\"))\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            with torch.no_grad():\n",
    "                output, hidden = model(data, hidden)\n",
    "            loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
    "            total_count += np.multiply(*data.size())\n",
    "            total_loss += loss.item()*np.multiply(*data.size())\n",
    "            \n",
    "    loss = total_loss / total_count\n",
    "    model.train()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要定义下面的一个function，帮助我们把一个hidden state和计算图之前的历史分离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this part\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义loss function和optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#scheduler有很多玩法，比如三次loss不下降就把lr砍半等等\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)#0.5指减半"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型：\n",
    "### 如果使用读取模型的方法，则下面不需要运行\n",
    "- 模型一般需要训练若干个epoch\n",
    "- 每个epoch我们都把所有的数据分成若干个batch\n",
    "- 把每个batch的输入和输出都包装成cuda tensor\n",
    "- forward pass，通过输入的句子预测每个单词的下一个单词\n",
    "- 用模型的预测和正确的下一个单词计算cross entropy loss\n",
    "- 清空模型当前gradient\n",
    "- backward pass\n",
    "- gradient clipping，防止梯度爆炸\n",
    "- 更新模型参数\n",
    "- 每隔一定的iteration输出模型在当前iteration的loss，以及在验证集上做模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0 loss 6.193421840667725\n",
      "best model, val loss:  5.618237644952179\n",
      "epoch 0 iter 1000 loss 6.182553291320801\n",
      "epoch 0 iter 2000 loss 6.2674174308776855\n",
      "epoch 0 iter 3000 loss 6.141019344329834\n",
      "epoch 0 iter 4000 loss 5.621214389801025\n",
      "epoch 0 iter 5000 loss 6.087187767028809\n",
      "epoch 0 iter 6000 loss 6.171429634094238\n",
      "epoch 0 iter 7000 loss 5.931452751159668\n",
      "epoch 0 iter 8000 loss 6.111051559448242\n",
      "epoch 0 iter 9000 loss 5.865108966827393\n",
      "epoch 0 iter 10000 loss 5.980441570281982\n",
      "best model, val loss:  5.586545678821775\n",
      "epoch 0 iter 11000 loss 6.097061634063721\n",
      "epoch 0 iter 12000 loss 6.233752727508545\n",
      "epoch 0 iter 13000 loss 5.877536773681641\n",
      "epoch 0 iter 14000 loss 5.826540470123291\n",
      "epoch 1 iter 0 loss 6.1389923095703125\n",
      "best model, val loss:  5.55586278224108\n",
      "epoch 1 iter 1000 loss 6.06961727142334\n",
      "epoch 1 iter 2000 loss 6.127335548400879\n",
      "epoch 1 iter 3000 loss 6.035078525543213\n",
      "epoch 1 iter 4000 loss 5.511624813079834\n",
      "epoch 1 iter 5000 loss 6.100916385650635\n",
      "epoch 1 iter 6000 loss 6.041910171508789\n",
      "epoch 1 iter 7000 loss 5.819006443023682\n",
      "epoch 1 iter 8000 loss 6.025644302368164\n",
      "epoch 1 iter 9000 loss 5.816326141357422\n",
      "epoch 1 iter 10000 loss 5.932995319366455\n",
      "best model, val loss:  5.5048297632983365\n",
      "epoch 1 iter 11000 loss 6.012798309326172\n",
      "epoch 1 iter 12000 loss 6.091536521911621\n",
      "epoch 1 iter 13000 loss 5.844158172607422\n",
      "epoch 1 iter 14000 loss 5.77318811416626\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "GRAD_CLIP = 1.\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "val_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    it = iter(train_iter)\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    for i, batch in enumerate(it):\n",
    "        data, target = batch.text, batch.target\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(device = torch.device(\"cuda:3\")), target.cuda(device = torch.device(\"cuda:3\"))\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "        if i % 1000 == 0:\n",
    "            print(\"epoch\", epoch, \"iter\", i, \"loss\", loss.item())\n",
    "    \n",
    "        if i % 10000 == 0:\n",
    "            val_loss = evaluate(model, val_iter)\n",
    "            ###########################\n",
    "            #保存模型\n",
    "            #############\n",
    "            #但一般不会每个模型都保存，根据val_loss，把最好的模型存下来\n",
    "            if len(val_losses) == 0 or val_loss < min(val_losses):\n",
    "                print(\"best model, val loss: \", val_loss)\n",
    "                torch.save(model.state_dict(), \"lm-best.th\")\n",
    "            else:\n",
    "                ################################\n",
    "                #如果loss不下降，需要更精细的lr，pytorch提供了scheduler\n",
    "                ################################\n",
    "                scheduler.step()#指lr下降一些，如果参数是0.5就下降50%\n",
    "                \n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型所有的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.weight',\n",
       "              tensor([[-0.0148, -0.0025, -0.0085,  ..., -0.0337,  0.0600, -0.0214],\n",
       "                      [-0.0485, -0.1032, -0.0134,  ..., -0.1146,  0.1165, -0.0539],\n",
       "                      [ 0.2117, -0.2793, -0.3717,  ..., -0.4380,  0.0778, -0.4988],\n",
       "                      ...,\n",
       "                      [-0.2929,  0.1851, -0.0309,  ...,  0.0359, -0.0244, -0.1295],\n",
       "                      [ 0.0136, -0.0324, -0.0006,  ...,  0.0716, -0.0505, -0.1268],\n",
       "                      [-0.1441,  0.1106,  0.0614,  ...,  0.0405, -0.0568, -0.0181]],\n",
       "                     device='cuda:3')),\n",
       "             ('rnn.weight_ih_l0',\n",
       "              tensor([[-0.0071, -0.1687,  0.1917,  ...,  0.1233, -0.0971, -0.1381],\n",
       "                      [-0.2608,  0.5052,  0.5019,  ...,  0.3304,  0.2009,  0.3287],\n",
       "                      [ 0.8329,  0.2731,  0.3885,  ...,  0.0246,  0.4055,  0.9056],\n",
       "                      ...,\n",
       "                      [ 0.0065,  0.5008,  0.1155,  ...,  0.2647, -0.3502,  0.0219],\n",
       "                      [-0.5868, -0.4060, -0.0380,  ..., -0.2942,  0.1100, -0.4752],\n",
       "                      [-0.0474, -0.2396, -0.1005,  ..., -0.1383,  0.2596, -0.1615]],\n",
       "                     device='cuda:3')),\n",
       "             ('rnn.weight_hh_l0',\n",
       "              tensor([[ 0.2967,  0.1671,  0.0400,  ..., -0.3057, -0.5989,  0.4837],\n",
       "                      [ 0.2881, -0.5334, -0.4469,  ..., -0.1473,  0.3824,  0.1059],\n",
       "                      [ 0.7086, -0.0639, -0.8862,  ...,  0.1775, -0.0702, -0.2521],\n",
       "                      ...,\n",
       "                      [-0.3738, -0.3504, -0.0622,  ..., -0.1815, -0.1614,  0.1372],\n",
       "                      [-0.4914, -0.5782, -0.0591,  ..., -1.3827,  0.1088,  0.3999],\n",
       "                      [ 0.3865, -0.1092, -0.3800,  ..., -0.1601, -0.0393, -0.2829]],\n",
       "                     device='cuda:3')),\n",
       "             ('rnn.bias_ih_l0',\n",
       "              tensor([-7.5365e-02, -3.8089e-01, -7.3059e-01, -2.0454e-01, -4.4150e-02,\n",
       "                      -5.7799e-02, -3.4064e-01, -3.6226e-01, -7.1916e-02, -2.3579e-04,\n",
       "                      -4.8816e-02,  1.6520e-01,  9.9243e-02,  8.7254e-02,  2.3892e-01,\n",
       "                      -5.3480e-02, -5.4657e-01, -1.8907e-01,  3.1147e-02, -3.2579e-01,\n",
       "                      -3.0632e-01, -7.9391e-02,  9.8090e-02, -4.6427e-01, -2.7095e-02,\n",
       "                      -3.6774e-04, -1.8680e-01, -4.4320e-01,  4.7197e-01, -2.8331e-01,\n",
       "                      -4.0843e-01, -4.3629e-01,  1.7565e-02, -2.7923e-01,  4.9906e-02,\n",
       "                      -4.2769e-01, -1.0349e+00,  2.1056e-02, -1.5236e-01, -2.2095e-01,\n",
       "                      -8.5238e-02, -2.2135e-01, -1.7143e-01, -2.7209e-01, -5.6838e-01,\n",
       "                       1.0359e-01, -5.3167e-01, -1.9764e-01, -1.8622e-01, -7.9949e-01,\n",
       "                      -1.9426e-01, -5.1772e-02, -1.0205e-01, -1.0380e-01, -1.0920e-01,\n",
       "                      -2.5236e-01, -2.5119e-01,  2.3389e-01, -8.3564e-01, -1.4957e-01,\n",
       "                       2.6916e-01,  1.3345e-01, -7.2726e-01, -2.2777e-01, -5.1331e-01,\n",
       "                      -4.0364e-01, -4.7314e-01, -3.9143e-01, -2.4696e-01, -2.0061e-01,\n",
       "                      -4.0106e-01,  1.3135e-03,  4.9789e-02,  7.7149e-02, -1.5917e-01,\n",
       "                       1.5217e-03, -2.9471e-01,  7.1458e-03,  7.9834e-02, -2.6760e-01,\n",
       "                      -3.4402e-02, -1.9281e-01, -8.5618e-02, -1.3131e-01,  8.9754e-02,\n",
       "                      -7.3928e-01,  3.1215e-01,  3.3365e-01, -5.8406e-01,  1.4018e-02,\n",
       "                      -8.5046e-03, -1.2555e-01,  8.0735e-02, -1.4696e-01, -5.7587e-01,\n",
       "                       1.3928e-01, -7.3960e-01, -5.0489e-01, -2.2572e-01,  7.0398e-02,\n",
       "                       1.1883e-02, -7.8649e-01, -4.8012e-01, -7.1778e-01, -3.0420e-01,\n",
       "                      -6.3984e-01, -3.4101e-01, -7.2061e-01, -4.2049e-01, -5.2798e-01,\n",
       "                      -5.3502e-01,  4.2226e-01, -3.6601e-01, -3.4524e-01, -4.4495e-01,\n",
       "                      -7.7064e-01, -9.3681e-02,  2.9156e-01, -1.2925e+00,  5.7451e-02,\n",
       "                      -1.5406e-01, -7.2977e-01, -6.0671e-01, -4.9997e-01,  8.9212e-02,\n",
       "                       4.2126e-02, -4.1999e-01, -4.1332e-01, -4.9641e-01, -8.4762e-01,\n",
       "                       2.8519e-01, -9.0709e-01, -5.0121e-01, -3.2826e-01,  2.2842e-01,\n",
       "                      -1.5565e-01, -2.1876e-01, -5.5325e-01, -1.6743e-01, -6.3438e-02,\n",
       "                      -7.8689e-01,  3.8224e-01, -5.0811e-01, -6.4110e-01, -4.9855e-01,\n",
       "                      -1.9900e-01, -6.7183e-01, -2.2898e-01,  1.3516e-01, -1.8682e-01,\n",
       "                      -7.8929e-01, -5.1821e-01,  2.6981e-02, -5.1521e-01, -6.3148e-02,\n",
       "                      -4.9734e-01,  1.1985e-01, -2.1486e-01, -6.4650e-01, -5.9457e-01,\n",
       "                      -3.0065e-01, -2.4544e-01, -1.8554e-01, -5.0123e-01, -6.6118e-01,\n",
       "                      -5.0214e-01, -7.2717e-01, -7.2057e-01, -1.1045e+00, -4.2187e-01,\n",
       "                      -2.6156e-01, -4.6056e-02, -2.6966e-01,  7.3920e-02,  6.0253e-02,\n",
       "                      -7.2984e-01, -5.0030e-01, -3.3168e-02, -5.0584e-01, -7.6291e-01,\n",
       "                      -5.3130e-01, -5.6327e-01, -4.2701e-01, -3.8754e-01, -5.0738e-01,\n",
       "                      -8.7404e-01, -3.8550e-01,  3.0047e-01, -2.0911e-01, -1.7777e-01,\n",
       "                      -3.6323e-01, -4.8911e-01, -5.0790e-01, -3.9207e-01,  5.0925e-01,\n",
       "                      -1.4325e-01, -3.8628e-01, -5.3036e-01, -3.5543e-01, -5.1072e-01,\n",
       "                       6.2746e-02,  2.7488e-01,  1.9558e-01,  8.6804e-02,  1.4053e-01,\n",
       "                      -5.1004e-02, -1.2383e-01, -7.0401e-02,  3.9579e-02, -1.6611e-01,\n",
       "                      -9.2388e-02,  5.0602e-01, -1.3776e-02,  7.8183e-02,  1.8754e-01,\n",
       "                       8.3174e-02, -7.9115e-02,  5.1583e-02, -5.7317e-02,  9.2828e-02,\n",
       "                       6.4313e-02, -9.3829e-02, -8.1726e-02,  6.0656e-02,  2.1169e-01,\n",
       "                       3.9703e-01,  2.3952e-02, -2.3959e-02, -6.9777e-02, -1.5265e-01,\n",
       "                       2.4317e-02, -2.8171e-01, -1.0629e-01,  7.3548e-02, -2.9615e-01,\n",
       "                       1.9966e-01,  1.0241e-01, -9.7132e-02,  2.1160e-01,  1.6719e-01,\n",
       "                       3.1057e-02,  1.0596e-01,  2.5500e-01,  1.8583e-01,  5.9370e-02,\n",
       "                      -1.2525e-01, -8.7505e-03,  3.8372e-01,  2.3785e-01, -9.9394e-02,\n",
       "                      -9.2110e-02, -2.6891e-02, -2.6984e-01,  9.0741e-02,  4.9116e-02,\n",
       "                       1.1018e-01, -2.3326e-01,  1.1508e-03, -6.3416e-02, -2.5423e-01,\n",
       "                       2.6950e-01,  1.0162e-02, -1.5332e-01,  5.4435e-02,  2.0655e-01,\n",
       "                       3.6807e-02, -1.6045e-01, -6.4692e-02,  1.0946e-01,  2.0043e-01,\n",
       "                      -1.1478e-02,  1.0884e-01,  1.5650e-01, -2.7718e-01,  1.9027e-01,\n",
       "                      -6.4005e-02, -3.5552e-01, -1.5451e-01, -1.6633e-01, -2.9406e-01,\n",
       "                       1.8228e-01, -2.7376e-01,  6.1973e-02,  6.1585e-02, -2.8452e-01,\n",
       "                      -1.2056e-01,  2.1448e-01,  3.0494e-01, -2.1629e-02, -5.9701e-02,\n",
       "                      -8.0970e-02,  2.9807e-01,  5.5677e-02,  2.5120e-01,  7.1725e-02,\n",
       "                      -1.4403e-01,  1.5186e-02,  3.0636e-01,  1.3777e-02, -5.4598e-03,\n",
       "                       9.9514e-02, -2.1997e-01, -6.5050e-01, -7.1734e-02, -1.0043e+00,\n",
       "                      -4.1273e-01, -5.4240e-01,  2.0577e-02, -8.1498e-02, -3.6072e-02,\n",
       "                      -8.4020e-02,  8.6058e-02, -5.0401e-01,  4.8559e-02, -4.8158e-01,\n",
       "                      -1.6570e-01, -6.4854e-01, -6.7525e-01, -1.6732e-01, -9.9316e-01,\n",
       "                      -8.4441e-01, -2.1079e-01,  1.7141e-01, -3.6789e-01, -1.1961e+00,\n",
       "                      -1.2106e+00, -3.0452e-01, -8.0812e-01, -9.1596e-01, -1.6507e-01,\n",
       "                       1.7178e-01, -7.3646e-01, -1.8640e-01, -4.5041e-01, -1.3187e+00,\n",
       "                       3.5239e-01, -2.7206e-01,  2.4168e-01, -2.1405e-01, -2.9817e-01,\n",
       "                       3.4173e-01, -1.4030e+00,  2.5489e-01, -6.7213e-01, -5.4022e-01,\n",
       "                      -2.4674e-01, -6.0313e-03, -4.9434e-01, -1.0727e+00, -6.5813e-01,\n",
       "                       1.4297e-03, -1.7324e-01,  1.1381e-01, -2.9700e-01, -6.9184e-01,\n",
       "                      -6.8736e-01, -3.0534e-01,  1.9155e-01, -4.2851e-01,  1.7774e-01,\n",
       "                      -1.1867e-01, -2.2760e-01,  3.2539e-01,  1.4986e-03, -4.0182e-01,\n",
       "                      -3.4822e-01, -5.3166e-01,  2.0992e-01, -4.0926e-01,  2.3330e-01,\n",
       "                      -2.2902e-01, -6.6867e-01, -1.5852e+00, -1.1039e+00, -6.4827e-01,\n",
       "                      -4.3460e-02, -1.0900e-01, -1.6487e-01,  1.6595e-01,  1.4106e-01,\n",
       "                       7.5156e-02, -4.3014e-01, -1.0284e-01,  1.9190e-01, -2.0717e-01,\n",
       "                      -3.4576e-01, -9.5168e-01,  3.0286e-02, -8.3959e-01, -6.6162e-02,\n",
       "                      -1.4677e-01, -4.4775e-01,  3.1780e-02, -6.7772e-01,  4.0094e-01,\n",
       "                      -9.1331e-01, -8.7815e-01, -5.4424e-02,  1.4771e-01, -1.0482e+00],\n",
       "                     device='cuda:3')),\n",
       "             ('rnn.bias_hh_l0',\n",
       "              tensor([-1.1612e-01, -3.6618e-01, -6.3494e-01, -1.6931e-01,  2.9317e-02,\n",
       "                       5.8787e-02, -4.7549e-01, -3.8819e-01, -1.2742e-01, -1.2327e-01,\n",
       "                       6.4824e-02,  1.9392e-01,  4.3887e-02, -5.1070e-02,  1.6162e-01,\n",
       "                      -2.0122e-01, -5.8817e-01, -2.4282e-01,  1.6799e-01, -3.9862e-01,\n",
       "                      -3.3096e-01, -2.1465e-01, -6.6744e-04, -4.1213e-01,  3.4378e-02,\n",
       "                      -1.4489e-01, -1.8193e-01, -4.6429e-01,  5.0838e-01, -3.2129e-01,\n",
       "                      -3.3180e-01, -4.7941e-01, -1.1370e-01, -2.3670e-01,  1.0898e-02,\n",
       "                      -3.0175e-01, -1.0177e+00,  6.6714e-02, -2.5693e-01, -3.2957e-01,\n",
       "                      -3.5057e-02, -1.3519e-01, -2.4317e-01, -2.4501e-01, -5.9799e-01,\n",
       "                       4.2917e-02, -5.9832e-01, -2.0530e-01, -1.6366e-01, -8.2206e-01,\n",
       "                      -2.6068e-01,  2.6224e-02, -4.6118e-02, -2.6103e-01, -2.4795e-02,\n",
       "                      -3.4310e-01, -1.6163e-01,  3.3290e-01, -9.6729e-01, -1.1526e-01,\n",
       "                       3.1865e-01,  9.0218e-02, -6.7735e-01, -7.3808e-02, -4.9036e-01,\n",
       "                      -3.0080e-01, -4.1080e-01, -3.2947e-01, -2.9492e-01, -2.0783e-01,\n",
       "                      -3.6177e-01,  1.9830e-02, -2.3594e-02, -7.6720e-03,  1.0620e-02,\n",
       "                       1.3166e-03, -3.5542e-01,  1.2746e-01,  5.1775e-02, -2.8750e-01,\n",
       "                       3.5904e-02, -5.9275e-02,  3.3915e-03, -5.2991e-02,  2.2006e-01,\n",
       "                      -6.5210e-01,  1.2413e-01,  3.6968e-01, -5.8774e-01,  2.0241e-02,\n",
       "                      -1.3111e-01, -1.1672e-01,  3.8155e-02, -2.1547e-01, -6.1266e-01,\n",
       "                       3.6937e-02, -7.3301e-01, -4.6040e-01, -2.8891e-01,  5.3192e-02,\n",
       "                      -6.4924e-02, -8.5232e-01, -4.4097e-01, -7.3280e-01, -3.7904e-01,\n",
       "                      -4.8750e-01, -4.4815e-01, -7.0635e-01, -4.8358e-01, -5.1111e-01,\n",
       "                      -6.4861e-01,  4.4017e-01, -2.7060e-01, -4.4942e-01, -3.7357e-01,\n",
       "                      -6.9478e-01, -1.4387e-01,  2.7593e-01, -1.2987e+00, -5.3346e-02,\n",
       "                      -1.8489e-01, -8.0077e-01, -4.9903e-01, -3.9923e-01,  1.3413e-01,\n",
       "                      -5.5689e-02, -5.7581e-01, -5.4299e-01, -4.5577e-01, -7.4721e-01,\n",
       "                       2.6930e-01, -9.1251e-01, -3.3914e-01, -4.9459e-01,  2.1468e-01,\n",
       "                      -1.8982e-01, -3.9016e-01, -4.5506e-01, -7.4192e-02, -8.2268e-02,\n",
       "                      -7.9809e-01,  4.6513e-01, -6.0846e-01, -5.8065e-01, -4.4975e-01,\n",
       "                      -2.6847e-01, -8.2828e-01, -1.6941e-01,  1.0263e-01, -2.3745e-01,\n",
       "                      -8.0879e-01, -4.7959e-01, -1.1137e-01, -4.6683e-01, -2.2130e-01,\n",
       "                      -4.3121e-01,  1.8169e-01, -1.9910e-01, -6.6160e-01, -5.9076e-01,\n",
       "                      -2.7271e-01, -1.0508e-01, -2.5115e-01, -5.2362e-01, -7.6685e-01,\n",
       "                      -6.0126e-01, -6.1278e-01, -8.3385e-01, -1.1364e+00, -4.8797e-01,\n",
       "                      -2.6903e-01, -1.2451e-02, -2.5072e-01,  1.9085e-01,  1.3627e-01,\n",
       "                      -7.0736e-01, -5.4652e-01, -7.3355e-02, -4.8120e-01, -8.2419e-01,\n",
       "                      -4.8360e-01, -5.9266e-01, -4.5405e-01, -3.2136e-01, -6.6062e-01,\n",
       "                      -9.3659e-01, -5.2189e-01,  3.5282e-01, -3.8284e-01, -6.4875e-02,\n",
       "                      -5.0961e-01, -4.9702e-01, -5.0247e-01, -3.7337e-01,  5.8755e-01,\n",
       "                      -1.5734e-01, -4.3401e-01, -6.5043e-01, -3.1716e-01, -5.8310e-01,\n",
       "                      -1.8866e-02,  2.2557e-01,  1.9219e-01,  2.8988e-02,  9.2486e-02,\n",
       "                      -1.2790e-02, -8.5370e-02, -2.0160e-03,  1.5462e-01, -1.0123e-01,\n",
       "                       8.0141e-02,  4.6063e-01, -6.0190e-02,  4.8617e-02,  2.3004e-01,\n",
       "                       4.6325e-03, -2.1678e-01, -3.9183e-02,  9.9542e-02,  1.9873e-01,\n",
       "                      -9.8833e-02, -1.9681e-01,  2.4369e-02,  1.4032e-01,  2.8834e-01,\n",
       "                       4.2960e-01,  1.0223e-01, -6.8612e-02, -1.3474e-01, -6.4038e-02,\n",
       "                      -1.3174e-02, -4.2403e-01, -1.0572e-01,  1.1408e-01, -1.9101e-01,\n",
       "                       1.0283e-01,  3.1830e-02,  5.9073e-02,  1.8974e-01, -4.2437e-03,\n",
       "                       1.1495e-01,  1.6512e-01,  1.3379e-01,  2.7745e-01, -2.8601e-02,\n",
       "                      -2.1287e-01, -3.3023e-02,  4.1745e-01,  2.9773e-01, -8.9267e-02,\n",
       "                      -2.3733e-01, -6.8533e-02, -3.6249e-01, -2.0548e-02,  1.4625e-01,\n",
       "                       7.4475e-02, -1.8615e-01,  7.2275e-02, -3.1315e-02, -2.4252e-01,\n",
       "                       8.8904e-02,  8.0311e-04, -2.0107e-01,  9.6398e-02,  2.6958e-01,\n",
       "                       1.6322e-02, -2.2940e-01, -1.2931e-01,  1.0112e-01,  1.0872e-01,\n",
       "                      -3.5929e-03,  1.0648e-01,  2.1089e-01, -3.2020e-01,  1.3534e-01,\n",
       "                       3.7416e-02, -3.3159e-01, -5.2408e-02, -9.6303e-02, -3.0703e-01,\n",
       "                       1.7635e-01, -2.8075e-01, -3.1315e-02,  3.9545e-02, -1.9993e-01,\n",
       "                      -6.1630e-02,  2.0274e-01,  2.1212e-01, -1.4361e-01, -6.5014e-02,\n",
       "                      -8.6725e-02,  2.1422e-01,  4.5075e-02,  1.0374e-01,  8.4076e-02,\n",
       "                      -2.5416e-01,  9.6827e-02,  2.5976e-01, -3.8010e-02,  4.7865e-02,\n",
       "                      -3.2783e-02, -3.3301e-01, -6.7203e-01,  9.1734e-03, -1.0503e+00,\n",
       "                      -5.3169e-01, -4.2721e-01,  3.7896e-02, -3.2975e-02, -6.2772e-02,\n",
       "                      -6.2600e-02,  1.1768e-01, -5.0431e-01,  2.8362e-02, -4.3873e-01,\n",
       "                      -7.4226e-02, -6.5163e-01, -6.1774e-01, -2.8261e-01, -8.3222e-01,\n",
       "                      -8.9062e-01, -5.8840e-02,  3.0335e-01, -2.5692e-01, -1.1292e+00,\n",
       "                      -1.1694e+00, -4.0737e-01, -7.0774e-01, -9.4061e-01, -1.4781e-01,\n",
       "                       2.4052e-01, -6.2928e-01, -2.4520e-01, -3.5751e-01, -1.3119e+00,\n",
       "                       3.4583e-01, -2.9019e-01,  2.9213e-01, -1.5717e-01, -2.8598e-01,\n",
       "                       1.9863e-01, -1.3121e+00,  1.0584e-01, -5.9416e-01, -5.2480e-01,\n",
       "                      -2.9956e-01,  9.8901e-03, -5.0826e-01, -9.4511e-01, -6.7873e-01,\n",
       "                       4.7094e-02, -1.3216e-01,  1.6741e-01, -4.2084e-01, -7.5850e-01,\n",
       "                      -6.8324e-01, -2.2868e-01,  8.9306e-02, -3.5782e-01,  1.3191e-01,\n",
       "                      -2.0969e-02, -1.5991e-01,  1.3865e-01, -1.2796e-02, -3.1884e-01,\n",
       "                      -4.6935e-01, -5.7018e-01,  1.0524e-01, -2.8187e-01,  1.8929e-01,\n",
       "                      -2.4737e-01, -6.7981e-01, -1.6242e+00, -1.0384e+00, -6.8412e-01,\n",
       "                       2.2509e-02, -9.0231e-02, -1.9654e-01,  2.6367e-01,  2.0813e-01,\n",
       "                       9.2214e-02, -5.2035e-01, -1.2387e-01,  1.7948e-01, -2.3316e-01,\n",
       "                      -3.7480e-01, -1.0118e+00, -2.8363e-02, -7.3652e-01, -7.6010e-02,\n",
       "                      -1.3274e-01, -4.7413e-01, -5.8350e-02, -6.9920e-01,  3.7777e-01,\n",
       "                      -8.9004e-01, -9.8905e-01, -3.1540e-02,  1.6364e-01, -9.7602e-01],\n",
       "                     device='cuda:3')),\n",
       "             ('rnn.weight_ih_l1',\n",
       "              tensor([[ 0.2500,  0.1885, -0.4502,  ...,  0.0674, -0.0473, -0.0305],\n",
       "                      [ 0.3024, -0.0710, -0.4256,  ...,  0.1629,  0.0438, -0.2396],\n",
       "                      [ 0.1106,  0.1863, -0.0149,  ..., -0.0144, -0.0914, -0.0808],\n",
       "                      ...,\n",
       "                      [ 0.0639, -0.0918, -0.1552,  ..., -0.0036,  0.0145, -0.1507],\n",
       "                      [ 0.0421,  0.0913,  0.0700,  ...,  0.1385,  0.0714,  0.2048],\n",
       "                      [-0.0723, -0.0932,  0.1902,  ..., -0.0139,  0.0353, -0.5865]],\n",
       "                     device='cuda:3')),\n",
       "             ('rnn.weight_hh_l1',\n",
       "              tensor([[-0.1551,  0.3453,  0.0950,  ..., -0.6264, -0.0584, -0.4835],\n",
       "                      [ 0.4597,  0.3149,  0.4423,  ..., -0.1520, -0.1617, -0.0090],\n",
       "                      [-0.5719,  0.1004, -0.1290,  ...,  0.1290,  0.0382,  0.0906],\n",
       "                      ...,\n",
       "                      [-0.1451, -0.0880,  0.3679,  ..., -0.3849, -0.0557, -0.0413],\n",
       "                      [-0.4154,  0.2763,  0.0334,  ..., -0.0659, -0.1132,  0.0466],\n",
       "                      [-0.2079,  0.0962,  0.1920,  ..., -0.0638,  0.1244, -0.1438]],\n",
       "                     device='cuda:3')),\n",
       "             ('rnn.bias_ih_l1',\n",
       "              tensor([-7.5075e-01, -2.3701e-01,  3.9200e-02, -2.5876e-01,  1.3879e-01,\n",
       "                       9.3947e-02, -2.6408e-02, -7.0690e-01, -3.4384e-01, -3.7256e-01,\n",
       "                       5.3484e-02,  4.2976e-01, -5.6366e-02, -7.8381e-02,  1.5574e-01,\n",
       "                      -1.8005e-01, -6.5960e-02,  9.7923e-02,  2.9210e-01, -8.7804e-02,\n",
       "                       1.2589e-01, -2.0124e-02, -2.8337e-01,  4.2771e-01,  2.8004e-02,\n",
       "                      -5.6500e-02, -1.2664e-01, -1.1472e-01,  1.9638e-02,  9.1953e-02,\n",
       "                      -2.1866e-02, -1.5372e-01,  1.5314e-01, -2.4846e-01,  2.9795e-02,\n",
       "                      -2.8707e-01,  1.2304e-02, -1.0197e-01,  7.3939e-02, -1.9452e-01,\n",
       "                       2.3660e-01, -1.8904e-01, -4.4460e-01, -9.1236e-02,  4.3367e-02,\n",
       "                      -1.4907e-01, -2.0392e-01, -1.0052e-01, -5.3169e-01, -1.1219e-01,\n",
       "                      -3.5028e-01, -6.8734e-01,  1.2425e-01, -3.0748e-01, -2.1755e-01,\n",
       "                      -1.5581e-01,  2.4817e-01,  1.6452e-01, -5.5095e-01, -1.6799e-01,\n",
       "                       1.0691e-01, -8.0302e-02,  5.6417e-02,  3.4636e-01, -1.0146e-01,\n",
       "                      -5.1871e-02, -1.9948e-01,  6.7893e-04, -5.7173e-02, -6.5346e-02,\n",
       "                      -2.2683e-01, -4.1641e-03,  1.1392e-01, -3.7769e-01, -1.0658e-01,\n",
       "                      -2.0841e-01,  2.3057e-02, -1.9692e-01, -1.2480e-01, -1.4180e-01,\n",
       "                       2.3613e-01, -6.6845e-01, -1.6120e-01, -7.5655e-01, -5.3526e-02,\n",
       "                      -2.3243e-01, -1.3814e-01, -9.6677e-02, -3.3167e-01,  1.7917e-01,\n",
       "                       1.6300e-01,  2.6799e-01,  1.9750e-01,  1.0958e-01, -8.7431e-02,\n",
       "                      -1.4964e-01, -2.1591e-01,  4.5943e-02,  2.4906e-02, -1.0914e-02,\n",
       "                       5.5030e-01,  2.6481e-01, -1.2398e-01, -1.6002e-01, -4.0622e-01,\n",
       "                      -4.8663e-01, -2.1336e-01,  7.3763e-01,  2.5257e-01,  1.5919e-01,\n",
       "                      -4.9611e-01, -1.2116e-01, -1.5015e-01, -4.0747e-01, -1.3420e-01,\n",
       "                      -1.5903e-01, -3.2872e-01, -4.6438e-01, -1.1341e-01, -4.2262e-01,\n",
       "                      -4.0080e-01, -2.7832e-01, -7.1708e-01, -5.0110e-01, -4.0006e-01,\n",
       "                      -8.9763e-02, -5.6094e-01,  2.0887e-01, -2.3943e-01, -1.6637e-01,\n",
       "                      -6.8941e-01, -4.4041e-01, -1.2628e-01,  4.0974e-01, -5.6733e-01,\n",
       "                      -3.3392e-01, -6.5639e-01, -2.1916e-01, -2.8726e-01, -6.0749e-02,\n",
       "                      -2.4543e-01, -2.3838e-01,  2.0170e-01, -9.2809e-02, -5.3793e-01,\n",
       "                      -1.9292e-01,  3.1081e-01, -7.2977e-01,  5.2317e-01, -6.4496e-01,\n",
       "                      -2.3892e-01,  4.8635e-01, -5.9877e-01,  2.5444e-01, -3.7687e-01,\n",
       "                      -1.7455e-01, -1.7940e-01, -3.7799e-01,  7.2514e-01, -4.3434e-01,\n",
       "                       6.7459e-02, -4.9609e-01,  1.6421e-01, -5.7179e-01, -4.1958e-01,\n",
       "                      -7.4105e-01, -6.4017e-02, -1.4727e-02, -7.1316e-01, -3.1698e-01,\n",
       "                      -2.6118e-01, -2.9544e-01, -1.3891e-01,  2.2650e-01, -3.5856e-01,\n",
       "                      -2.8244e-01, -5.8452e-01, -5.5576e-01, -4.3869e-01, -5.3128e-01,\n",
       "                      -2.5864e-01,  6.8363e-01,  1.5899e-01,  7.0020e-01, -4.1704e-01,\n",
       "                      -4.3613e-01, -3.8644e-02, -4.7155e-01,  1.4352e-01, -5.1210e-02,\n",
       "                      -2.8329e-01, -4.5128e-01, -2.7124e-01, -2.0058e-01, -5.2954e-01,\n",
       "                      -5.9242e-01, -2.5146e-01, -4.2191e-01, -2.1263e-01, -3.2286e-01,\n",
       "                      -8.4507e-02, -4.8761e-02, -1.6364e-01,  1.5690e-01,  7.0328e-02,\n",
       "                      -1.7794e-01,  1.8050e-01, -7.7884e-02, -9.2287e-02,  1.0116e-01,\n",
       "                       1.0968e-01,  1.3412e-01,  2.3456e-01,  8.7077e-02,  1.7753e-02,\n",
       "                       2.5859e-01, -4.3393e-02, -5.6360e-02, -2.0771e-01, -5.8291e-02,\n",
       "                       5.2671e-03,  1.1778e-01, -1.4536e-01,  1.1968e-01, -6.9544e-03,\n",
       "                       5.6688e-02, -1.4213e-01, -7.0899e-02, -1.2626e-01,  1.7006e-01,\n",
       "                       2.0242e-01,  2.3270e-01,  9.6823e-02,  1.3066e-01,  1.5819e-01,\n",
       "                      -3.5827e-02,  1.9376e-01, -2.1998e-01,  3.7838e-02, -2.6556e-01,\n",
       "                       6.9181e-03, -2.3085e-01, -1.3116e-01, -7.7119e-02, -1.1819e-01,\n",
       "                      -1.2937e-01,  4.6211e-02, -1.1726e-01, -8.3814e-02,  2.6248e-02,\n",
       "                       3.4896e-05, -1.5374e-01, -1.4587e-01,  1.2128e-01, -3.3867e-01,\n",
       "                       1.2765e-01,  2.2858e-01,  7.1237e-03,  9.2035e-02, -1.4787e-01,\n",
       "                       1.9350e-01, -1.0546e-01,  1.7264e-01, -1.3391e-01, -3.6322e-02,\n",
       "                       1.5521e-02,  1.0328e-01, -2.4004e-01, -1.6855e-01, -2.5703e-01,\n",
       "                       2.8810e-02, -1.0116e-02, -6.6854e-02,  1.9543e-01,  1.3359e-01,\n",
       "                       1.5735e-01, -5.2260e-02, -5.1921e-02,  2.8269e-03,  2.2808e-02,\n",
       "                      -1.4340e-01, -8.3639e-03,  2.6822e-01, -4.7361e-02,  4.2605e-02,\n",
       "                       1.0765e-01,  3.0220e-01, -8.0624e-02,  2.2174e-01,  3.0423e-02,\n",
       "                       7.5546e-02,  1.8747e-01,  1.6241e-02, -2.1378e-01,  1.1395e-02,\n",
       "                      -3.4233e-02,  1.1176e-01, -9.7549e-02,  1.6769e-01,  1.5783e-01,\n",
       "                       6.3771e-01,  3.5570e-01, -1.4699e-01, -1.8268e-01, -1.6939e-01,\n",
       "                       1.6000e-01, -2.2055e-01,  6.5653e-01, -6.2689e-01,  3.7738e-01,\n",
       "                       1.5003e-01,  8.8208e-02, -3.0173e-01, -4.0693e-01,  3.1152e-01,\n",
       "                      -6.4816e-01,  3.5958e-01,  1.9830e-02,  3.9200e-01, -4.0073e-01,\n",
       "                       3.8048e-01,  4.3175e-01, -4.5226e-01,  1.3384e-01, -2.0364e-01,\n",
       "                       6.3701e-02, -1.7029e-01,  2.5344e-01,  1.3533e-01, -5.3290e-01,\n",
       "                      -2.0460e-01, -1.0419e-01,  2.9876e-01, -2.6578e-01,  4.9362e-01,\n",
       "                      -2.0581e-01, -3.1780e-02, -5.2825e-01, -8.9316e-02, -5.9877e-01,\n",
       "                       3.3248e-01, -5.2728e-01, -3.3803e-01,  3.6121e-01, -3.6215e-01,\n",
       "                      -1.5749e-01,  1.4833e-01, -6.0290e-02,  5.6720e-01,  3.1399e-01,\n",
       "                       3.7937e-02, -2.7413e-01,  4.3466e-02, -5.3579e-01, -2.6950e-01,\n",
       "                      -5.3120e-01, -6.4277e-01, -1.2912e-02,  6.9765e-01,  2.0758e-01,\n",
       "                       1.8202e-01, -3.3158e-02,  5.3564e-01,  3.3817e-01, -3.6129e-02,\n",
       "                      -1.4312e-01, -4.8376e-01, -6.0821e-01, -6.7920e-02, -6.4588e-01,\n",
       "                      -2.2120e-01, -4.1463e-01,  5.1878e-01, -1.4571e-01, -2.9621e-01,\n",
       "                      -6.6142e-01, -3.9403e-01, -2.6077e-01, -5.8692e-02, -1.2271e-01,\n",
       "                       9.9628e-02,  6.7202e-01, -6.0529e-01,  6.7659e-01, -2.3396e-01,\n",
       "                      -4.0333e-01, -8.3740e-01, -2.5316e-01, -4.9660e-01,  5.3024e-01,\n",
       "                      -1.6543e-01, -5.3872e-01, -4.2626e-01, -3.0116e-01,  1.9189e-01,\n",
       "                      -2.7919e-02,  1.6303e-01, -2.3089e-01, -3.8819e-03, -2.8447e-01],\n",
       "                     device='cuda:3')),\n",
       "             ('rnn.bias_hh_l1',\n",
       "              tensor([-6.8235e-01, -5.5024e-02,  3.8645e-02, -1.6154e-01,  2.0911e-01,\n",
       "                       7.5742e-02,  1.2260e-01, -6.1705e-01, -3.6734e-01, -4.0431e-01,\n",
       "                       2.6161e-02,  3.7644e-01, -2.2656e-02, -8.2835e-02,  6.7150e-02,\n",
       "                      -5.2153e-02,  5.4385e-02,  1.3869e-01,  2.8092e-01, -1.0189e-01,\n",
       "                       2.8673e-02, -5.9696e-02, -1.9091e-01,  6.0154e-01,  3.2582e-02,\n",
       "                      -2.2942e-01, -9.0178e-02, -2.3883e-01, -1.8376e-02,  2.4261e-02,\n",
       "                      -7.1758e-02, -2.4024e-02,  1.6615e-02, -2.8308e-01, -3.6937e-02,\n",
       "                      -3.4904e-01,  1.4185e-02, -1.6770e-01, -4.0545e-03, -2.1714e-01,\n",
       "                       2.3512e-01, -1.8764e-01, -4.6412e-01, -1.6551e-01,  6.0972e-02,\n",
       "                      -1.3813e-01, -2.8622e-01,  7.9244e-02, -4.7136e-01, -1.9930e-01,\n",
       "                      -3.4892e-01, -8.2482e-01,  2.4564e-01, -3.0899e-01, -2.3269e-01,\n",
       "                      -6.2610e-02,  1.1552e-01,  6.6045e-02, -6.0157e-01, -2.6435e-01,\n",
       "                       2.2791e-03, -1.6861e-01,  2.6779e-02,  2.1304e-01, -3.1106e-02,\n",
       "                      -1.4845e-01, -9.0427e-02,  1.1300e-01,  5.4818e-02,  3.4498e-02,\n",
       "                      -2.9496e-01, -9.5228e-02,  2.4732e-01, -3.8233e-01,  1.7710e-02,\n",
       "                      -6.2045e-02, -4.4601e-02, -1.3771e-01, -2.0725e-01,  8.5043e-03,\n",
       "                       1.3948e-01, -7.3479e-01, -7.6954e-02, -6.6190e-01,  1.9327e-02,\n",
       "                      -1.3258e-01, -1.4925e-01, -1.0947e-01, -3.2192e-01,  1.8575e-01,\n",
       "                       8.6394e-02,  2.4112e-01,  1.3215e-01,  9.2082e-02, -6.8964e-02,\n",
       "                      -1.5897e-01, -2.0235e-01,  6.1097e-02,  1.6208e-01,  6.6063e-02,\n",
       "                       7.1728e-01,  4.4931e-01, -4.0826e-02, -1.0675e-01, -3.3929e-01,\n",
       "                      -4.7472e-01, -9.2971e-02,  6.7728e-01,  3.6433e-01,  2.7383e-01,\n",
       "                      -3.6729e-01, -4.3004e-02, -2.4488e-01, -3.7478e-01, -1.7177e-01,\n",
       "                      -1.5066e-01, -3.6047e-01, -4.6150e-01, -1.7697e-01, -4.5270e-01,\n",
       "                      -4.2875e-01, -3.7165e-01, -7.4114e-01, -4.6679e-01, -4.4270e-01,\n",
       "                      -8.4531e-02, -4.9804e-01,  3.7282e-01, -3.3487e-01, -1.7253e-01,\n",
       "                      -7.0000e-01, -3.6163e-01, -2.3311e-04,  3.9599e-01, -4.2030e-01,\n",
       "                      -4.2833e-01, -5.0947e-01, -3.2200e-01, -2.3452e-01,  2.3112e-02,\n",
       "                      -4.2474e-01, -1.2743e-01,  2.5123e-01,  8.4299e-02, -6.5492e-01,\n",
       "                      -1.6327e-01,  2.6829e-01, -7.0504e-01,  5.5060e-01, -5.5636e-01,\n",
       "                      -2.9002e-01,  4.6501e-01, -5.7210e-01,  2.3616e-01, -3.8122e-01,\n",
       "                      -2.0427e-01, -1.5747e-01, -4.2965e-01,  6.8738e-01, -4.4332e-01,\n",
       "                       1.3467e-01, -4.4425e-01,  1.3953e-01, -4.2352e-01, -3.3281e-01,\n",
       "                      -8.0051e-01,  3.3197e-02, -2.5270e-02, -5.7871e-01, -3.1703e-01,\n",
       "                      -4.0944e-01, -3.1190e-01, -2.7397e-01,  1.2188e-01, -3.3772e-01,\n",
       "                      -1.5405e-01, -5.4962e-01, -6.0179e-01, -4.8824e-01, -5.0955e-01,\n",
       "                      -2.9537e-01,  8.0714e-01,  7.8617e-02,  8.1852e-01, -3.8585e-01,\n",
       "                      -3.8409e-01, -1.2932e-01, -5.4464e-01,  9.5683e-02,  1.5653e-02,\n",
       "                      -2.7926e-01, -3.4351e-01, -4.6427e-01, -7.6437e-02, -3.9515e-01,\n",
       "                      -4.7859e-01, -2.4741e-01, -3.9000e-01, -3.9291e-01, -2.7061e-01,\n",
       "                      -6.8423e-02, -7.0041e-02, -8.5915e-02,  8.4505e-02, -1.2065e-01,\n",
       "                      -1.1512e-01,  2.0322e-02,  2.5538e-02, -1.6239e-01,  9.3467e-05,\n",
       "                       7.0522e-02,  4.5828e-02,  1.2236e-01,  2.8809e-02, -9.2695e-02,\n",
       "                       2.6255e-01, -1.7892e-02,  9.4133e-02, -2.1793e-02, -7.8579e-02,\n",
       "                       6.5458e-02,  6.1423e-02, -1.4631e-01,  1.2323e-01, -1.2735e-01,\n",
       "                      -3.1383e-02, -9.3872e-02, -6.7076e-02,  4.7906e-02,  2.4878e-01,\n",
       "                       7.3553e-02,  1.7403e-01, -5.0254e-02,  4.4790e-02,  1.1068e-01,\n",
       "                       2.4866e-02,  8.3223e-02, -1.1724e-01, -4.1031e-02, -1.2035e-01,\n",
       "                      -1.1620e-01, -1.6545e-01, -2.8015e-02, -1.8987e-01, -1.1995e-01,\n",
       "                      -8.2109e-02, -5.7319e-03, -8.6897e-02,  1.2963e-02,  1.1758e-01,\n",
       "                      -1.4975e-01, -1.5867e-01, -1.8008e-01,  1.9729e-01, -2.9440e-01,\n",
       "                       1.5194e-01,  8.0459e-02,  4.2196e-02, -3.5721e-02, -7.5243e-02,\n",
       "                       8.3692e-02, -8.4247e-04,  4.9567e-02, -8.7787e-02, -1.6992e-02,\n",
       "                       1.0321e-01,  2.0259e-01, -1.8210e-01, -1.5471e-01, -2.3045e-01,\n",
       "                       1.2822e-01,  4.7817e-02, -4.5715e-02,  2.9747e-01,  1.1269e-01,\n",
       "                       7.5460e-02, -4.0057e-02,  1.0826e-01, -7.2034e-02,  9.6061e-02,\n",
       "                      -2.3598e-01,  2.2053e-02,  1.2565e-01,  5.1916e-02,  1.5282e-02,\n",
       "                       1.5473e-02,  2.6756e-01, -1.2705e-01,  1.0170e-01, -3.0794e-02,\n",
       "                       1.1949e-01,  2.4700e-01, -9.0789e-02, -1.8907e-01, -7.8179e-02,\n",
       "                       6.3763e-02,  1.0857e-01,  1.3874e-02,  9.1740e-02,  7.0486e-02,\n",
       "                       5.2103e-01,  3.5651e-01, -2.4692e-01, -1.3371e-01, -1.2405e-01,\n",
       "                       1.4180e-01, -3.0480e-01,  6.0732e-01, -6.2766e-01,  4.3348e-01,\n",
       "                       2.3031e-01,  1.2958e-01, -3.6920e-01, -3.5268e-01,  3.7279e-01,\n",
       "                      -6.5464e-01,  4.4995e-01,  1.4779e-02,  3.3423e-01, -4.3281e-01,\n",
       "                       4.3144e-01,  3.0660e-01, -5.3078e-01,  1.3300e-01, -2.2018e-01,\n",
       "                      -7.0774e-02, -3.0488e-01,  3.9743e-01,  3.2322e-01, -6.0972e-01,\n",
       "                      -3.1645e-02, -6.3097e-02,  3.5976e-01, -1.5995e-01,  4.7805e-01,\n",
       "                      -8.1458e-02,  9.3368e-02, -6.0031e-01, -2.3570e-01, -6.2122e-01,\n",
       "                       3.6406e-01, -6.2395e-01, -4.0238e-01,  3.4707e-01, -3.8846e-01,\n",
       "                      -7.1054e-02,  9.8207e-02,  2.0664e-02,  4.3810e-01,  2.0103e-01,\n",
       "                       9.9996e-03, -3.3543e-01, -9.4931e-02, -4.8624e-01, -3.2176e-01,\n",
       "                      -7.1258e-01, -6.1743e-01, -6.0151e-03,  6.5487e-01,  3.5240e-01,\n",
       "                       2.8079e-01, -1.4292e-02,  5.5548e-01,  3.0414e-01, -1.8759e-02,\n",
       "                       2.5746e-03, -3.4889e-01, -5.9544e-01, -8.4557e-02, -5.9525e-01,\n",
       "                      -2.9570e-01, -3.6924e-01,  4.1415e-01, -1.2411e-01, -1.2435e-01,\n",
       "                      -6.3809e-01, -5.3235e-01, -1.8681e-01, -7.4250e-02, -1.1867e-01,\n",
       "                       1.0726e-01,  6.5945e-01, -7.7450e-01,  6.7976e-01, -9.8501e-02,\n",
       "                      -4.3833e-01, -6.8325e-01, -1.5189e-01, -5.8438e-01,  4.0524e-01,\n",
       "                      -1.0979e-01, -6.5658e-01, -3.2104e-01, -2.2916e-01,  1.4406e-01,\n",
       "                       1.3713e-01,  7.9359e-02, -2.3817e-01,  7.0044e-02, -3.2178e-01],\n",
       "                     device='cuda:3')),\n",
       "             ('decoder.weight',\n",
       "              tensor([[ 0.3238,  0.1927,  0.3402,  ..., -0.3088, -0.4912, -0.0190],\n",
       "                      [ 0.9236,  0.6859,  1.1090,  ..., -1.2243, -0.5586, -1.0622],\n",
       "                      [ 0.0605,  0.2517,  0.1247,  ..., -1.0356,  0.1354, -0.5637],\n",
       "                      ...,\n",
       "                      [ 0.6032,  0.6387,  0.7361,  ..., -1.6622, -0.4536, -2.0136],\n",
       "                      [ 1.1065,  0.5999,  0.6884,  ..., -0.9169, -0.3523, -0.9494],\n",
       "                      [ 0.9194,  0.4905,  1.3125,  ..., -1.0806, -0.5659, -1.1287]],\n",
       "                     device='cuda:3')),\n",
       "             ('decoder.bias',\n",
       "              tensor([ 6.2830, -4.7245,  5.9880,  ..., -1.0457, -3.8611, -4.7119],\n",
       "                     device='cuda:3'))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load模型\n",
    "- 先新定义一个模型\n",
    "- 用model.load_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load模型\n",
    "best_model = RNNModel(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
    "if USE_CUDA:\n",
    "    best_model = best_model.cuda(device = torch.device(\"cuda:3\"))\n",
    "best_model.load_state_dict(torch.load(\"lm-best.th\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用最好的模型在valid数据上计算perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:  287.3899864294951\n"
     ]
    }
   ],
   "source": [
    "#好的结果是140,170等\n",
    "val_loss = evaluate(best_model, val_iter)\n",
    "print(\"perplexity: \", np.exp(val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用最好的模型在测试数据上计算perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:  340.36256797664157\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_iter)\n",
    "print(\"perplexity: \", np.exp(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用训练好的模型生成一些句子\n",
    "- 这里只用了1个hidden_state\n",
    "- 把hidden_state传入best_model里面\n",
    "- 用multinomial来sample一下（也可以用argmax，预测最大的单词概率）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explosives are goss the water press that franks within their culture were the people made the politics of game at potency and hospital this commander was depicted to some so you have collectively become the body potential member of the open association organization in turn of this would be divided out for two proudly rise the flop browser list of user and substitution alone qc or processing which considerations also designation generated in line and a two this video rewarding effects that sides that precludes overlapping regions smelling but anxiety within a large ratio of the maple duck <unk> there\n"
     ]
    }
   ],
   "source": [
    "hidden = best_model.init_hidden(1)\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "input = torch.randint(VOCAB_SIZE, (1, 1), dtype=torch.long).to(device)\n",
    "words = []\n",
    "for i in range(100):\n",
    "    #run forward pass\n",
    "    output, hidden = best_model(input, hidden)\n",
    "    #logists exp\n",
    "    word_weights = output.squeeze().exp().cpu()\n",
    "    #multunomial sampling 拿到一个单词，也可以argmax，但是argmax往往不是最好\n",
    "    #用argmax或者beam search的话每次生成的理论上是同一句话\n",
    "    #但是如果翻译之类的任务会喜欢用argmax或者beam search，因为比较diterminastic\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    #fill in the current predicted word to the current input\n",
    "    #接下来再用这个input去预测下一个input\n",
    "    input.fill_(word_idx)\n",
    "    word = TEXT.vocab.itos[word_idx]\n",
    "    words.append(word)\n",
    "print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
