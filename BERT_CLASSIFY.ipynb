{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/06/2020 09:00:10 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/srtp/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_int_a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from random import randrange\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text ='what is a pug'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayerNorm(nn.Module):\n",
    "        def __init__(self, hidden_size, eps=1e-12):\n",
    "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "            \"\"\"\n",
    "            super(BertLayerNorm, self).__init__()\n",
    "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "            self.variance_epsilon = eps\n",
    "\n",
    "        def forward(self, x):\n",
    "            u = x.mean(-1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "            return self.weight * x + self.bias\n",
    "        \n",
    "\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_labels].\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, num_labels].\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "    num_labels = 2\n",
    "    model = BertForSequenceClassification(config, num_labels)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/06/2020 09:00:28 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/srtp/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "03/06/2020 09:00:28 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/srtp/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpgxdgwxu_\n",
      "03/06/2020 09:00:31 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_labels = 2\n",
    "model = BertForSequenceClassification(num_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([tokenizer.convert_tokens_to_ids(zz)])\n",
    "\n",
    "logits = model(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2959,  0.0543]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dat = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a',\n",
       "  'wonderful',\n",
       "  'little',\n",
       "  'production',\n",
       "  '.',\n",
       "  '<',\n",
       "  'br',\n",
       "  '/',\n",
       "  '>',\n",
       "  '<',\n",
       "  'br',\n",
       "  '/',\n",
       "  '>',\n",
       "  'the',\n",
       "  'filming',\n",
       "  'technique',\n",
       "  'is',\n",
       "  'very',\n",
       "  'una',\n",
       "  '##ss',\n",
       "  '##uming',\n",
       "  '-',\n",
       "  'very',\n",
       "  'old',\n",
       "  '-',\n",
       "  'time',\n",
       "  '-',\n",
       "  'bbc',\n",
       "  'fashion',\n",
       "  'and',\n",
       "  'gives',\n",
       "  'a',\n",
       "  'comforting',\n",
       "  ',',\n",
       "  'and',\n",
       "  'sometimes',\n",
       "  'discomfort',\n",
       "  '##ing',\n",
       "  ',',\n",
       "  'sense',\n",
       "  'of',\n",
       "  'realism',\n",
       "  'to',\n",
       "  'the',\n",
       "  'entire',\n",
       "  'piece',\n",
       "  '.',\n",
       "  '<',\n",
       "  'br',\n",
       "  '/',\n",
       "  '>',\n",
       "  '<',\n",
       "  'br',\n",
       "  '/',\n",
       "  '>',\n",
       "  'the',\n",
       "  'actors',\n",
       "  'are',\n",
       "  'extremely',\n",
       "  'well',\n",
       "  'chosen',\n",
       "  '-',\n",
       "  'michael',\n",
       "  'sheen',\n",
       "  'not',\n",
       "  'only',\n",
       "  '\"',\n",
       "  'has',\n",
       "  'got',\n",
       "  'all',\n",
       "  'the',\n",
       "  'polar',\n",
       "  '##i',\n",
       "  '\"',\n",
       "  'but',\n",
       "  'he',\n",
       "  'has',\n",
       "  'all',\n",
       "  'the',\n",
       "  'voices',\n",
       "  'down',\n",
       "  'pat',\n",
       "  'too',\n",
       "  '!',\n",
       "  'you',\n",
       "  'can',\n",
       "  'truly',\n",
       "  'see',\n",
       "  'the',\n",
       "  'seam',\n",
       "  '##less',\n",
       "  'editing',\n",
       "  'guided',\n",
       "  'by',\n",
       "  'the',\n",
       "  'references',\n",
       "  'to',\n",
       "  'williams',\n",
       "  \"'\",\n",
       "  'diary',\n",
       "  'entries',\n",
       "  ',',\n",
       "  'not',\n",
       "  'only',\n",
       "  'is',\n",
       "  'it',\n",
       "  'well',\n",
       "  'worth',\n",
       "  'the',\n",
       "  'watching',\n",
       "  'but',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'terrific',\n",
       "  '##ly',\n",
       "  'written',\n",
       "  'and',\n",
       "  'performed',\n",
       "  'piece',\n",
       "  '.',\n",
       "  'a',\n",
       "  'master',\n",
       "  '##ful',\n",
       "  'production',\n",
       "  'about',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'great',\n",
       "  'master',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'of',\n",
       "  'comedy',\n",
       "  'and',\n",
       "  'his',\n",
       "  'life',\n",
       "  '.',\n",
       "  '<',\n",
       "  'br',\n",
       "  '/',\n",
       "  '>',\n",
       "  '<',\n",
       "  'br',\n",
       "  '/',\n",
       "  '>',\n",
       "  'the',\n",
       "  'realism',\n",
       "  'really',\n",
       "  'comes',\n",
       "  'home',\n",
       "  'with',\n",
       "  'the',\n",
       "  'little',\n",
       "  'things',\n",
       "  ':',\n",
       "  'the',\n",
       "  'fantasy',\n",
       "  'of',\n",
       "  'the',\n",
       "  'guard',\n",
       "  'which',\n",
       "  ',',\n",
       "  'rather',\n",
       "  'than',\n",
       "  'use',\n",
       "  'the',\n",
       "  'traditional',\n",
       "  \"'\",\n",
       "  'dream',\n",
       "  \"'\",\n",
       "  'techniques',\n",
       "  'remains',\n",
       "  'solid',\n",
       "  'then',\n",
       "  'disappears',\n",
       "  '.',\n",
       "  'it',\n",
       "  'plays',\n",
       "  'on',\n",
       "  'our',\n",
       "  'knowledge',\n",
       "  'and',\n",
       "  'our',\n",
       "  'senses',\n",
       "  ',',\n",
       "  'particularly',\n",
       "  'with',\n",
       "  'the',\n",
       "  'scenes',\n",
       "  'concerning',\n",
       "  'orton',\n",
       "  'and',\n",
       "  'hall',\n",
       "  '##i',\n",
       "  '##well',\n",
       "  'and',\n",
       "  'the',\n",
       "  'sets',\n",
       "  '(',\n",
       "  'particularly',\n",
       "  'of',\n",
       "  'their',\n",
       "  'flat',\n",
       "  'with',\n",
       "  'hall',\n",
       "  '##i',\n",
       "  '##well',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'murals',\n",
       "  'decor',\n",
       "  '##ating',\n",
       "  'every',\n",
       "  'surface',\n",
       "  ')',\n",
       "  'are',\n",
       "  'terribly',\n",
       "  'well',\n",
       "  'done',\n",
       "  '.'],\n",
       " ['basically',\n",
       "  'there',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'a',\n",
       "  'family',\n",
       "  'where',\n",
       "  'a',\n",
       "  'little',\n",
       "  'boy',\n",
       "  '(',\n",
       "  'jake',\n",
       "  ')',\n",
       "  'thinks',\n",
       "  'there',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'a',\n",
       "  'zombie',\n",
       "  'in',\n",
       "  'his',\n",
       "  'closet',\n",
       "  '&',\n",
       "  'his',\n",
       "  'parents',\n",
       "  'are',\n",
       "  'fighting',\n",
       "  'all',\n",
       "  'the',\n",
       "  'time',\n",
       "  '.',\n",
       "  '<',\n",
       "  'br',\n",
       "  '/',\n",
       "  '>',\n",
       "  '<',\n",
       "  'br',\n",
       "  '/',\n",
       "  '>',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'slower',\n",
       "  'than',\n",
       "  'a',\n",
       "  'soap',\n",
       "  'opera',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'and',\n",
       "  'suddenly',\n",
       "  ',',\n",
       "  'jake',\n",
       "  'decides',\n",
       "  'to',\n",
       "  'become',\n",
       "  'ram',\n",
       "  '##bo',\n",
       "  'and',\n",
       "  'kill',\n",
       "  'the',\n",
       "  'zombie',\n",
       "  '.',\n",
       "  '<',\n",
       "  'br',\n",
       "  '/',\n",
       "  '>',\n",
       "  '<',\n",
       "  'br',\n",
       "  '/',\n",
       "  '>',\n",
       "  'ok',\n",
       "  ',',\n",
       "  'first',\n",
       "  'of',\n",
       "  'all',\n",
       "  'when',\n",
       "  'you',\n",
       "  \"'\",\n",
       "  're',\n",
       "  'going',\n",
       "  'to',\n",
       "  'make',\n",
       "  'a',\n",
       "  'film',\n",
       "  'you',\n",
       "  'must',\n",
       "  'decide',\n",
       "  'if',\n",
       "  'its',\n",
       "  'a',\n",
       "  'thriller',\n",
       "  'or',\n",
       "  'a',\n",
       "  'drama',\n",
       "  '!',\n",
       "  'as',\n",
       "  'a',\n",
       "  'drama',\n",
       "  'the',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'watch',\n",
       "  '##able',\n",
       "  '.',\n",
       "  'parents',\n",
       "  'are',\n",
       "  'di',\n",
       "  '##vor',\n",
       "  '##cing',\n",
       "  '&',\n",
       "  'arguing',\n",
       "  'like',\n",
       "  'in',\n",
       "  'real',\n",
       "  'life',\n",
       "  '.',\n",
       "  'and',\n",
       "  'then',\n",
       "  'we',\n",
       "  'have',\n",
       "  'jake',\n",
       "  'with',\n",
       "  'his',\n",
       "  'closet',\n",
       "  'which',\n",
       "  'totally',\n",
       "  'ruins',\n",
       "  'all',\n",
       "  'the',\n",
       "  'film',\n",
       "  '!',\n",
       "  'i',\n",
       "  'expected',\n",
       "  'to',\n",
       "  'see',\n",
       "  'a',\n",
       "  'boo',\n",
       "  '##ge',\n",
       "  '##yman',\n",
       "  'similar',\n",
       "  'movie',\n",
       "  ',',\n",
       "  'and',\n",
       "  'instead',\n",
       "  'i',\n",
       "  'watched',\n",
       "  'a',\n",
       "  'drama',\n",
       "  'with',\n",
       "  'some',\n",
       "  'meaningless',\n",
       "  'thriller',\n",
       "  'spots',\n",
       "  '.',\n",
       "  '<',\n",
       "  'br',\n",
       "  '/',\n",
       "  '>',\n",
       "  '<',\n",
       "  'br',\n",
       "  '/',\n",
       "  '>',\n",
       "  '3',\n",
       "  'out',\n",
       "  'of',\n",
       "  '10',\n",
       "  'just',\n",
       "  'for',\n",
       "  'the',\n",
       "  'well',\n",
       "  'playing',\n",
       "  'parents',\n",
       "  '&',\n",
       "  'descent',\n",
       "  'dial',\n",
       "  '##og',\n",
       "  '##s',\n",
       "  '.',\n",
       "  'as',\n",
       "  'for',\n",
       "  'the',\n",
       "  'shots',\n",
       "  'with',\n",
       "  'jake',\n",
       "  ':',\n",
       "  'just',\n",
       "  'ignore',\n",
       "  'them',\n",
       "  '.']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz = tokenizer.tokenize(dat.review[1])\n",
    "z1z = tokenizer.tokenize(dat.review[3])\n",
    "\n",
    "[zz,z1z ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "zzz = tokenizer.convert_tokens_to_ids(zz)\n",
    "zzzz = tokenizer.convert_tokens_to_ids(z1z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1037,\n",
       " 6919,\n",
       " 2210,\n",
       " 2537,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1996,\n",
       " 7467,\n",
       " 6028,\n",
       " 2003,\n",
       " 2200,\n",
       " 14477,\n",
       " 4757,\n",
       " 24270,\n",
       " 1011,\n",
       " 2200,\n",
       " 2214,\n",
       " 1011,\n",
       " 2051,\n",
       " 1011,\n",
       " 4035,\n",
       " 4827,\n",
       " 1998,\n",
       " 3957,\n",
       " 1037,\n",
       " 16334,\n",
       " 1010,\n",
       " 1998,\n",
       " 2823,\n",
       " 17964,\n",
       " 2075,\n",
       " 1010,\n",
       " 3168,\n",
       " 1997,\n",
       " 15650,\n",
       " 2000,\n",
       " 1996,\n",
       " 2972,\n",
       " 3538,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1996,\n",
       " 5889,\n",
       " 2024,\n",
       " 5186,\n",
       " 2092,\n",
       " 4217,\n",
       " 1011,\n",
       " 2745,\n",
       " 20682,\n",
       " 2025,\n",
       " 2069,\n",
       " 1000,\n",
       " 2038,\n",
       " 2288,\n",
       " 2035,\n",
       " 1996,\n",
       " 11508,\n",
       " 2072,\n",
       " 1000,\n",
       " 2021,\n",
       " 2002,\n",
       " 2038,\n",
       " 2035,\n",
       " 1996,\n",
       " 5755,\n",
       " 2091,\n",
       " 6986,\n",
       " 2205,\n",
       " 999,\n",
       " 2017,\n",
       " 2064,\n",
       " 5621,\n",
       " 2156,\n",
       " 1996,\n",
       " 25180,\n",
       " 3238,\n",
       " 9260,\n",
       " 8546,\n",
       " 2011,\n",
       " 1996,\n",
       " 7604,\n",
       " 2000,\n",
       " 3766,\n",
       " 1005,\n",
       " 9708,\n",
       " 10445,\n",
       " 1010,\n",
       " 2025,\n",
       " 2069,\n",
       " 2003,\n",
       " 2009,\n",
       " 2092,\n",
       " 4276,\n",
       " 1996,\n",
       " 3666,\n",
       " 2021,\n",
       " 2009,\n",
       " 2003,\n",
       " 1037,\n",
       " 27547,\n",
       " 2135,\n",
       " 2517,\n",
       " 1998,\n",
       " 2864,\n",
       " 3538,\n",
       " 1012,\n",
       " 1037,\n",
       " 3040,\n",
       " 3993,\n",
       " 2537,\n",
       " 2055,\n",
       " 2028,\n",
       " 1997,\n",
       " 1996,\n",
       " 2307,\n",
       " 3040,\n",
       " 1005,\n",
       " 1055,\n",
       " 1997,\n",
       " 4038,\n",
       " 1998,\n",
       " 2010,\n",
       " 2166,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1996,\n",
       " 15650,\n",
       " 2428,\n",
       " 3310,\n",
       " 2188,\n",
       " 2007,\n",
       " 1996,\n",
       " 2210,\n",
       " 2477,\n",
       " 1024,\n",
       " 1996,\n",
       " 5913,\n",
       " 1997,\n",
       " 1996,\n",
       " 3457,\n",
       " 2029,\n",
       " 1010,\n",
       " 2738,\n",
       " 2084,\n",
       " 2224,\n",
       " 1996,\n",
       " 3151,\n",
       " 1005,\n",
       " 3959,\n",
       " 1005,\n",
       " 5461,\n",
       " 3464,\n",
       " 5024,\n",
       " 2059,\n",
       " 17144,\n",
       " 1012,\n",
       " 2009,\n",
       " 3248,\n",
       " 2006,\n",
       " 2256,\n",
       " 3716,\n",
       " 1998,\n",
       " 2256,\n",
       " 9456,\n",
       " 1010,\n",
       " 3391,\n",
       " 2007,\n",
       " 1996,\n",
       " 5019,\n",
       " 7175,\n",
       " 25161,\n",
       " 1998,\n",
       " 2534,\n",
       " 2072,\n",
       " 4381,\n",
       " 1998,\n",
       " 1996,\n",
       " 4520,\n",
       " 1006,\n",
       " 3391,\n",
       " 1997,\n",
       " 2037,\n",
       " 4257,\n",
       " 2007,\n",
       " 2534,\n",
       " 2072,\n",
       " 4381,\n",
       " 1005,\n",
       " 1055,\n",
       " 19016,\n",
       " 25545,\n",
       " 5844,\n",
       " 2296,\n",
       " 3302,\n",
       " 1007,\n",
       " 2024,\n",
       " 16668,\n",
       " 2092,\n",
       " 2589,\n",
       " 1012]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokens_tensor = torch.tensor([zzz,zzz])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1037,  6919,  2210,  2537,  1012,  1026,  7987,  1013,  1028,  1026,\n",
       "          7987,  1013,  1028,  1996,  7467,  6028,  2003,  2200, 14477,  4757,\n",
       "         24270,  1011,  2200,  2214,  1011,  2051,  1011,  4035,  4827,  1998,\n",
       "          3957,  1037, 16334,  1010,  1998,  2823, 17964,  2075,  1010,  3168,\n",
       "          1997, 15650,  2000,  1996,  2972,  3538,  1012,  1026,  7987,  1013,\n",
       "          1028,  1026,  7987,  1013,  1028,  1996,  5889,  2024,  5186,  2092,\n",
       "          4217,  1011,  2745, 20682,  2025,  2069,  1000,  2038,  2288,  2035,\n",
       "          1996, 11508,  2072,  1000,  2021,  2002,  2038,  2035,  1996,  5755,\n",
       "          2091,  6986,  2205,   999,  2017,  2064,  5621,  2156,  1996, 25180,\n",
       "          3238,  9260,  8546,  2011,  1996,  7604,  2000,  3766,  1005,  9708,\n",
       "         10445,  1010,  2025,  2069,  2003,  2009,  2092,  4276,  1996,  3666,\n",
       "          2021,  2009,  2003,  1037, 27547,  2135,  2517,  1998,  2864,  3538,\n",
       "          1012,  1037,  3040,  3993,  2537,  2055,  2028,  1997,  1996,  2307,\n",
       "          3040,  1005,  1055,  1997,  4038,  1998,  2010,  2166,  1012,  1026,\n",
       "          7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996, 15650,  2428,\n",
       "          3310,  2188,  2007,  1996,  2210,  2477,  1024,  1996,  5913,  1997,\n",
       "          1996,  3457,  2029,  1010,  2738,  2084,  2224,  1996,  3151,  1005,\n",
       "          3959,  1005,  5461,  3464,  5024,  2059, 17144,  1012,  2009,  3248,\n",
       "          2006,  2256,  3716,  1998,  2256,  9456,  1010,  3391,  2007,  1996,\n",
       "          5019,  7175, 25161,  1998,  2534,  2072,  4381,  1998,  1996,  4520,\n",
       "          1006,  3391,  1997,  2037,  4257,  2007,  2534,  2072,  4381,  1005,\n",
       "          1055, 19016, 25545,  5844,  2296,  3302,  1007,  2024, 16668,  2092,\n",
       "          2589,  1012],\n",
       "        [ 1037,  6919,  2210,  2537,  1012,  1026,  7987,  1013,  1028,  1026,\n",
       "          7987,  1013,  1028,  1996,  7467,  6028,  2003,  2200, 14477,  4757,\n",
       "         24270,  1011,  2200,  2214,  1011,  2051,  1011,  4035,  4827,  1998,\n",
       "          3957,  1037, 16334,  1010,  1998,  2823, 17964,  2075,  1010,  3168,\n",
       "          1997, 15650,  2000,  1996,  2972,  3538,  1012,  1026,  7987,  1013,\n",
       "          1028,  1026,  7987,  1013,  1028,  1996,  5889,  2024,  5186,  2092,\n",
       "          4217,  1011,  2745, 20682,  2025,  2069,  1000,  2038,  2288,  2035,\n",
       "          1996, 11508,  2072,  1000,  2021,  2002,  2038,  2035,  1996,  5755,\n",
       "          2091,  6986,  2205,   999,  2017,  2064,  5621,  2156,  1996, 25180,\n",
       "          3238,  9260,  8546,  2011,  1996,  7604,  2000,  3766,  1005,  9708,\n",
       "         10445,  1010,  2025,  2069,  2003,  2009,  2092,  4276,  1996,  3666,\n",
       "          2021,  2009,  2003,  1037, 27547,  2135,  2517,  1998,  2864,  3538,\n",
       "          1012,  1037,  3040,  3993,  2537,  2055,  2028,  1997,  1996,  2307,\n",
       "          3040,  1005,  1055,  1997,  4038,  1998,  2010,  2166,  1012,  1026,\n",
       "          7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996, 15650,  2428,\n",
       "          3310,  2188,  2007,  1996,  2210,  2477,  1024,  1996,  5913,  1997,\n",
       "          1996,  3457,  2029,  1010,  2738,  2084,  2224,  1996,  3151,  1005,\n",
       "          3959,  1005,  5461,  3464,  5024,  2059, 17144,  1012,  2009,  3248,\n",
       "          2006,  2256,  3716,  1998,  2256,  9456,  1010,  3391,  2007,  1996,\n",
       "          5019,  7175, 25161,  1998,  2534,  2072,  4381,  1998,  1996,  4520,\n",
       "          1006,  3391,  1997,  2037,  4257,  2007,  2534,  2072,  4381,  1005,\n",
       "          1055, 19016, 25545,  5844,  2296,  3302,  1007,  2024, 16668,  2092,\n",
       "          2589,  1012]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8152,  0.4565],\n",
       "        [-0.4468,  1.8472]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(tokens_tensor)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2190, 0.7810],\n",
       "        [0.0916, 0.9084]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.softmax(logits,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = dat['review']\n",
    "y = dat['sentiment']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values.tolist()\n",
    "X_test = X_test.values.tolist()\n",
    "\n",
    "y_train = pd.get_dummies(y_train).values.tolist()\n",
    "y_test = pd.get_dummies(y_test).values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_seq_length = 256\n",
    "class text_dataset(Dataset):\n",
    "    def __init__(self,x_y_list, transform=None):\n",
    "        \n",
    "        self.x_y_list = x_y_list\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])\n",
    "        \n",
    "        if len(tokenized_review) > max_seq_length:\n",
    "            tokenized_review = tokenized_review[:max_seq_length]\n",
    "            \n",
    "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n",
    "\n",
    "        padding = [0] * (max_seq_length - len(ids_review))\n",
    "        \n",
    "        ids_review += padding\n",
    "        \n",
    "        assert len(ids_review) == max_seq_length\n",
    "        \n",
    "        #print(ids_review)\n",
    "        ids_review = torch.tensor(ids_review)\n",
    "        \n",
    "        sentiment = self.x_y_list[1][index] # color        \n",
    "        list_of_labels = [torch.from_numpy(np.array(sentiment))]\n",
    "        \n",
    "        \n",
    "        return ids_review, list_of_labels[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_lists = [X_train, y_train]\n",
    "test_lists = [X_test, y_test]\n",
    "\n",
    "training_dataset = text_dataset(x_y_list = train_lists )\n",
    "\n",
    "test_dataset = text_dataset(x_y_list = test_lists )\n",
    "\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                   }\n",
    "dataset_sizes = {'train':len(train_lists[0]),\n",
    "                'val':len(test_lists[0])}\n",
    "\n",
    "device = torch.device( \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    print('starting')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            sentiment_corrects = 0\n",
    "            \n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, sentiment in dataloaders_dict[phase]:\n",
    "                #inputs = inputs\n",
    "                #print(len(inputs),type(inputs),inputs)\n",
    "                #inputs = torch.from_numpy(np.array(inputs)).to(device) \n",
    "                inputs = inputs.to(device) \n",
    "\n",
    "                sentiment = sentiment.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    #print(inputs)\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    outputs = F.softmax(outputs,dim=1)\n",
    "                    \n",
    "                    loss = criterion(outputs, torch.max(sentiment.float(), 1)[1])\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        print(\"training\")\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                print(\"current loss:\",loss.item() * inputs.size(0))\n",
    "\n",
    "                \n",
    "                sentiment_corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(sentiment, 1)[1])\n",
    "\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            \n",
    "            sentiment_acc = sentiment_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
    "            print('{} sentiment_acc: {:.4f}'.format(\n",
    "                phase, sentiment_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print('saving with loss of {}'.format(epoch_loss),\n",
    "                      'improved over previous {}'.format(best_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), 'bert_model_test.pth')\n",
    "\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(float(best_loss)))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.4)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "#model.freeze_bert_encoder()\n",
    "#model.classifier.weight.requires_grad = True\n",
    "model\n",
    "\n",
    "#    def freeze_bert_encoder(self):\n",
    "#        for param in self.bert.parameters():\n",
    "#            param.requires_grad = False\n",
    "#    \n",
    "#    def unfreeze_bert_encoder(self):\n",
    "#        for param in self.bert.parameters():\n",
    "#            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrlast = .001\n",
    "lrmain = .00001\n",
    "optim1 = optim.Adam(\n",
    "    [\n",
    "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
    "       \n",
    "   ])\n",
    "\n",
    "#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Epoch 0/4\n",
      "----------\n",
      "training\n",
      "current loss: 11.638134956359863\n",
      "training\n",
      "current loss: 9.375847816467285\n",
      "training\n",
      "current loss: 9.472310066223145\n",
      "training\n",
      "current loss: 9.69799518585205\n",
      "training\n",
      "current loss: 10.83901596069336\n",
      "training\n",
      "current loss: 10.129119873046875\n",
      "training\n",
      "current loss: 11.161550521850586\n",
      "training\n",
      "current loss: 9.692031860351562\n",
      "training\n",
      "current loss: 8.49592113494873\n",
      "training\n",
      "current loss: 11.329404830932617\n",
      "training\n",
      "current loss: 8.406257629394531\n",
      "training\n",
      "current loss: 10.83920669555664\n",
      "training\n",
      "current loss: 10.086922645568848\n",
      "training\n",
      "current loss: 10.588093757629395\n",
      "training\n",
      "current loss: 10.32459545135498\n",
      "training\n",
      "current loss: 9.913704872131348\n",
      "training\n",
      "current loss: 11.75322437286377\n",
      "training\n",
      "current loss: 10.181161880493164\n",
      "training\n",
      "current loss: 8.828126907348633\n",
      "training\n",
      "current loss: 8.242478370666504\n",
      "training\n",
      "current loss: 8.881961822509766\n",
      "training\n",
      "current loss: 6.912471771240234\n",
      "training\n",
      "current loss: 7.8965959548950195\n",
      "training\n",
      "current loss: 9.482566833496094\n",
      "training\n",
      "current loss: 9.485844612121582\n",
      "training\n",
      "current loss: 9.633105278015137\n",
      "training\n",
      "current loss: 8.340408325195312\n",
      "training\n",
      "current loss: 5.815203666687012\n",
      "training\n",
      "current loss: 7.0785908699035645\n",
      "training\n",
      "current loss: 5.977071762084961\n",
      "training\n",
      "current loss: 10.64717960357666\n",
      "training\n",
      "current loss: 6.325911998748779\n",
      "training\n",
      "current loss: 8.067964553833008\n",
      "training\n",
      "current loss: 7.900328636169434\n",
      "training\n",
      "current loss: 8.107542991638184\n",
      "training\n",
      "current loss: 6.9704976081848145\n",
      "training\n",
      "current loss: 8.755104064941406\n",
      "training\n",
      "current loss: 6.455801010131836\n",
      "training\n",
      "current loss: 9.314974784851074\n",
      "training\n",
      "current loss: 6.347672939300537\n",
      "training\n",
      "current loss: 6.850069999694824\n",
      "training\n",
      "current loss: 8.500053405761719\n",
      "training\n",
      "current loss: 8.034756660461426\n",
      "training\n",
      "current loss: 5.665473461151123\n",
      "training\n",
      "current loss: 6.492170333862305\n",
      "training\n",
      "current loss: 7.2639617919921875\n",
      "training\n",
      "current loss: 10.718475341796875\n",
      "training\n",
      "current loss: 8.572235107421875\n",
      "training\n",
      "current loss: 10.273321151733398\n",
      "training\n",
      "current loss: 8.39767074584961\n",
      "training\n",
      "current loss: 8.58184814453125\n",
      "training\n",
      "current loss: 9.058067321777344\n",
      "training\n",
      "current loss: 6.147879123687744\n",
      "training\n",
      "current loss: 9.278017044067383\n",
      "training\n",
      "current loss: 7.795605182647705\n",
      "training\n",
      "current loss: 7.232346534729004\n",
      "training\n",
      "current loss: 10.97635555267334\n",
      "training\n",
      "current loss: 7.243105888366699\n",
      "training\n",
      "current loss: 6.399818420410156\n",
      "training\n",
      "current loss: 8.659483909606934\n",
      "training\n",
      "current loss: 6.54744815826416\n",
      "training\n",
      "current loss: 6.4485368728637695\n",
      "training\n",
      "current loss: 7.061574935913086\n",
      "training\n",
      "current loss: 8.063887596130371\n",
      "training\n",
      "current loss: 5.982352256774902\n",
      "training\n",
      "current loss: 5.702578544616699\n",
      "training\n",
      "current loss: 8.172706604003906\n",
      "training\n",
      "current loss: 7.1354451179504395\n",
      "training\n",
      "current loss: 7.926266670227051\n",
      "training\n",
      "current loss: 8.43593978881836\n",
      "training\n",
      "current loss: 7.170423984527588\n",
      "training\n",
      "current loss: 6.9399309158325195\n",
      "training\n",
      "current loss: 6.445767879486084\n",
      "training\n",
      "current loss: 7.107603073120117\n",
      "training\n",
      "current loss: 5.8024139404296875\n",
      "training\n",
      "current loss: 6.86002254486084\n",
      "training\n",
      "current loss: 5.071030139923096\n",
      "training\n",
      "current loss: 6.931985855102539\n",
      "training\n",
      "current loss: 7.044201374053955\n",
      "training\n",
      "current loss: 7.145303726196289\n",
      "training\n",
      "current loss: 10.42892837524414\n",
      "training\n",
      "current loss: 9.524736404418945\n",
      "training\n",
      "current loss: 7.856823921203613\n",
      "training\n",
      "current loss: 9.985673904418945\n",
      "training\n",
      "current loss: 9.076868057250977\n",
      "training\n",
      "current loss: 5.31633996963501\n",
      "training\n",
      "current loss: 5.888694763183594\n",
      "training\n",
      "current loss: 8.719011306762695\n",
      "training\n",
      "current loss: 7.995753765106201\n",
      "training\n",
      "current loss: 8.14998722076416\n",
      "training\n",
      "current loss: 10.941513061523438\n",
      "training\n",
      "current loss: 11.591439247131348\n",
      "training\n",
      "current loss: 8.98914623260498\n",
      "training\n",
      "current loss: 5.583703994750977\n",
      "training\n",
      "current loss: 8.106493949890137\n",
      "training\n",
      "current loss: 6.6190080642700195\n",
      "training\n",
      "current loss: 6.607075214385986\n",
      "training\n",
      "current loss: 6.084953784942627\n",
      "training\n",
      "current loss: 10.879016876220703\n",
      "training\n",
      "current loss: 6.885839462280273\n",
      "training\n",
      "current loss: 10.957697868347168\n",
      "training\n",
      "current loss: 7.457812786102295\n",
      "training\n",
      "current loss: 6.231926918029785\n",
      "training\n",
      "current loss: 7.809866905212402\n",
      "training\n",
      "current loss: 7.974437713623047\n",
      "training\n",
      "current loss: 6.389004707336426\n",
      "training\n",
      "current loss: 7.6773362159729\n",
      "training\n",
      "current loss: 6.063589572906494\n",
      "training\n",
      "current loss: 6.6475419998168945\n",
      "training\n",
      "current loss: 7.289941787719727\n",
      "training\n",
      "current loss: 10.893250465393066\n",
      "training\n",
      "current loss: 9.214999198913574\n",
      "training\n",
      "current loss: 9.425756454467773\n",
      "training\n",
      "current loss: 8.982776641845703\n",
      "training\n",
      "current loss: 6.999070167541504\n",
      "training\n",
      "current loss: 8.22754192352295\n",
      "training\n",
      "current loss: 7.516728401184082\n",
      "training\n",
      "current loss: 8.744257926940918\n",
      "training\n",
      "current loss: 7.832950115203857\n",
      "training\n",
      "current loss: 6.938533306121826\n",
      "training\n",
      "current loss: 6.417032241821289\n",
      "training\n",
      "current loss: 8.720476150512695\n",
      "training\n",
      "current loss: 7.5790276527404785\n",
      "training\n",
      "current loss: 7.7929205894470215\n",
      "training\n",
      "current loss: 6.112021446228027\n",
      "training\n",
      "current loss: 5.07367467880249\n",
      "training\n",
      "current loss: 6.89838981628418\n",
      "training\n",
      "current loss: 5.052876949310303\n",
      "training\n",
      "current loss: 6.95937442779541\n",
      "training\n",
      "current loss: 8.413999557495117\n",
      "training\n",
      "current loss: 7.017292022705078\n",
      "training\n",
      "current loss: 8.850302696228027\n",
      "training\n",
      "current loss: 5.684991359710693\n",
      "training\n",
      "current loss: 8.363197326660156\n",
      "training\n",
      "current loss: 7.674472332000732\n",
      "training\n",
      "current loss: 8.8809175491333\n",
      "training\n",
      "current loss: 6.487804412841797\n",
      "training\n",
      "current loss: 6.113783836364746\n",
      "training\n",
      "current loss: 6.543643951416016\n",
      "training\n",
      "current loss: 7.9060893058776855\n",
      "training\n",
      "current loss: 7.8948259353637695\n",
      "training\n",
      "current loss: 8.37934398651123\n",
      "training\n",
      "current loss: 6.9488205909729\n",
      "training\n",
      "current loss: 6.018072128295898\n",
      "training\n",
      "current loss: 5.084378242492676\n",
      "training\n",
      "current loss: 8.07891845703125\n",
      "training\n",
      "current loss: 8.967727661132812\n",
      "training\n",
      "current loss: 7.501540184020996\n",
      "training\n",
      "current loss: 6.689255714416504\n",
      "training\n",
      "current loss: 7.634978294372559\n",
      "training\n",
      "current loss: 5.0691046714782715\n",
      "training\n",
      "current loss: 6.925792694091797\n",
      "training\n",
      "current loss: 6.895659923553467\n",
      "training\n",
      "current loss: 8.691301345825195\n",
      "training\n",
      "current loss: 6.713260650634766\n",
      "training\n",
      "current loss: 7.409734725952148\n",
      "training\n",
      "current loss: 6.449250221252441\n",
      "training\n",
      "current loss: 6.486462116241455\n",
      "training\n",
      "current loss: 5.631995677947998\n",
      "training\n",
      "current loss: 7.173041343688965\n",
      "training\n",
      "current loss: 7.041808128356934\n",
      "training\n",
      "current loss: 9.382702827453613\n",
      "training\n",
      "current loss: 7.0095977783203125\n",
      "training\n",
      "current loss: 6.499257564544678\n",
      "training\n",
      "current loss: 8.066040992736816\n",
      "training\n",
      "current loss: 6.701223373413086\n",
      "training\n",
      "current loss: 9.025607109069824\n",
      "training\n",
      "current loss: 7.835051536560059\n",
      "training\n",
      "current loss: 5.970536231994629\n",
      "training\n",
      "current loss: 5.023900985717773\n",
      "training\n",
      "current loss: 9.367372512817383\n",
      "training\n",
      "current loss: 5.494686126708984\n",
      "training\n",
      "current loss: 6.573202133178711\n",
      "training\n",
      "current loss: 7.86196756362915\n",
      "training\n",
      "current loss: 7.542037487030029\n",
      "training\n",
      "current loss: 7.584680557250977\n",
      "training\n",
      "current loss: 9.595694541931152\n",
      "training\n",
      "current loss: 8.120990753173828\n",
      "training\n",
      "current loss: 9.39992904663086\n",
      "training\n",
      "current loss: 6.014173984527588\n",
      "training\n",
      "current loss: 9.86677074432373\n",
      "training\n",
      "current loss: 5.822944641113281\n",
      "training\n",
      "current loss: 5.042667865753174\n",
      "training\n",
      "current loss: 6.156965732574463\n",
      "training\n",
      "current loss: 5.2904815673828125\n",
      "training\n",
      "current loss: 6.124546051025391\n",
      "training\n",
      "current loss: 6.85579252243042\n",
      "training\n",
      "current loss: 8.675052642822266\n",
      "training\n",
      "current loss: 6.130226135253906\n",
      "training\n",
      "current loss: 7.207198619842529\n",
      "training\n",
      "current loss: 6.447915554046631\n",
      "training\n",
      "current loss: 7.877424240112305\n",
      "training\n",
      "current loss: 6.8372883796691895\n",
      "training\n",
      "current loss: 6.011794567108154\n",
      "training\n",
      "current loss: 6.365792274475098\n",
      "training\n",
      "current loss: 7.020858287811279\n",
      "training\n",
      "current loss: 7.827483177185059\n",
      "training\n",
      "current loss: 5.7207794189453125\n",
      "training\n",
      "current loss: 7.26861047744751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "current loss: 8.710936546325684\n",
      "training\n",
      "current loss: 5.064635276794434\n",
      "training\n",
      "current loss: 7.431754112243652\n",
      "training\n",
      "current loss: 6.528200626373291\n",
      "training\n",
      "current loss: 7.495674133300781\n",
      "training\n",
      "current loss: 5.717403411865234\n",
      "training\n",
      "current loss: 6.933420181274414\n",
      "training\n",
      "current loss: 7.253873825073242\n",
      "training\n",
      "current loss: 5.704046726226807\n",
      "training\n",
      "current loss: 7.1401591300964355\n",
      "training\n",
      "current loss: 5.4821929931640625\n",
      "training\n",
      "current loss: 9.383604049682617\n",
      "training\n",
      "current loss: 7.231834411621094\n",
      "training\n",
      "current loss: 6.4801130294799805\n",
      "training\n",
      "current loss: 7.199197292327881\n",
      "training\n",
      "current loss: 5.137447357177734\n",
      "training\n",
      "current loss: 7.814896106719971\n",
      "training\n",
      "current loss: 7.687524795532227\n",
      "training\n",
      "current loss: 8.139082908630371\n",
      "training\n",
      "current loss: 8.423921585083008\n",
      "training\n",
      "current loss: 6.0358476638793945\n",
      "training\n",
      "current loss: 6.71282958984375\n",
      "training\n",
      "current loss: 10.62513256072998\n",
      "training\n",
      "current loss: 5.9373250007629395\n",
      "training\n",
      "current loss: 7.42109489440918\n",
      "training\n",
      "current loss: 6.761051654815674\n",
      "training\n",
      "current loss: 7.003853797912598\n",
      "training\n",
      "current loss: 6.890597343444824\n",
      "training\n",
      "current loss: 8.020683288574219\n",
      "training\n",
      "current loss: 8.18598747253418\n",
      "training\n",
      "current loss: 5.1822404861450195\n",
      "training\n",
      "current loss: 7.085418701171875\n",
      "training\n",
      "current loss: 7.868527412414551\n",
      "training\n",
      "current loss: 6.034058094024658\n",
      "training\n",
      "current loss: 8.03640079498291\n",
      "training\n",
      "current loss: 7.327632904052734\n",
      "training\n",
      "current loss: 9.060050964355469\n",
      "training\n",
      "current loss: 8.98731803894043\n",
      "training\n",
      "current loss: 7.822871208190918\n",
      "training\n",
      "current loss: 7.912649154663086\n",
      "training\n",
      "current loss: 7.912583351135254\n",
      "training\n",
      "current loss: 7.936631679534912\n",
      "training\n",
      "current loss: 9.44532585144043\n",
      "training\n",
      "current loss: 5.872478485107422\n",
      "training\n",
      "current loss: 6.66237735748291\n",
      "training\n",
      "current loss: 6.764595031738281\n",
      "training\n",
      "current loss: 7.034720420837402\n",
      "training\n",
      "current loss: 8.168962478637695\n",
      "training\n",
      "current loss: 7.035469055175781\n",
      "training\n",
      "current loss: 8.755261421203613\n",
      "training\n",
      "current loss: 8.85399055480957\n",
      "training\n",
      "current loss: 10.66797924041748\n",
      "training\n",
      "current loss: 9.302701950073242\n",
      "training\n",
      "current loss: 6.025006294250488\n",
      "training\n",
      "current loss: 6.221290588378906\n",
      "training\n",
      "current loss: 7.907022476196289\n",
      "training\n",
      "current loss: 5.308591842651367\n",
      "training\n",
      "current loss: 7.7738823890686035\n",
      "training\n",
      "current loss: 7.617861270904541\n",
      "training\n",
      "current loss: 6.418749809265137\n",
      "training\n",
      "current loss: 9.857800483703613\n",
      "training\n",
      "current loss: 6.0117034912109375\n",
      "training\n",
      "current loss: 7.496664047241211\n",
      "training\n",
      "current loss: 6.438821792602539\n",
      "training\n",
      "current loss: 12.785327911376953\n",
      "training\n",
      "current loss: 5.895174503326416\n",
      "training\n",
      "current loss: 6.233779430389404\n",
      "training\n",
      "current loss: 6.014035224914551\n",
      "training\n",
      "current loss: 8.066636085510254\n",
      "training\n",
      "current loss: 6.3495330810546875\n",
      "training\n",
      "current loss: 6.293204307556152\n",
      "training\n",
      "current loss: 6.676784515380859\n",
      "training\n",
      "current loss: 6.834296703338623\n",
      "training\n",
      "current loss: 8.039673805236816\n",
      "training\n",
      "current loss: 5.2587809562683105\n",
      "training\n",
      "current loss: 6.161568641662598\n",
      "training\n",
      "current loss: 8.048868179321289\n",
      "training\n",
      "current loss: 8.664905548095703\n",
      "training\n",
      "current loss: 9.008947372436523\n",
      "training\n",
      "current loss: 8.705038070678711\n",
      "training\n",
      "current loss: 7.9581804275512695\n",
      "training\n",
      "current loss: 5.091457366943359\n",
      "training\n",
      "current loss: 10.264357566833496\n",
      "training\n",
      "current loss: 7.62148380279541\n",
      "training\n",
      "current loss: 6.6884918212890625\n",
      "training\n",
      "current loss: 6.257464408874512\n",
      "training\n",
      "current loss: 7.093449115753174\n",
      "training\n",
      "current loss: 8.554689407348633\n",
      "training\n",
      "current loss: 7.948211193084717\n",
      "training\n",
      "current loss: 5.015578746795654\n",
      "training\n",
      "current loss: 6.304067134857178\n",
      "training\n",
      "current loss: 6.021248817443848\n",
      "training\n",
      "current loss: 7.117831230163574\n",
      "training\n",
      "current loss: 8.017618179321289\n",
      "training\n",
      "current loss: 8.088568687438965\n",
      "training\n",
      "current loss: 6.0548014640808105\n",
      "training\n",
      "current loss: 6.925351142883301\n",
      "training\n",
      "current loss: 5.975385665893555\n",
      "training\n",
      "current loss: 5.1331987380981445\n",
      "training\n",
      "current loss: 8.789715766906738\n",
      "training\n",
      "current loss: 6.017595291137695\n",
      "training\n",
      "current loss: 7.16094446182251\n",
      "training\n",
      "current loss: 8.91169261932373\n",
      "training\n",
      "current loss: 5.018266201019287\n",
      "training\n",
      "current loss: 6.171491622924805\n",
      "training\n",
      "current loss: 5.797943592071533\n",
      "training\n",
      "current loss: 6.655128479003906\n",
      "training\n",
      "current loss: 7.395473957061768\n",
      "training\n",
      "current loss: 6.427402973175049\n",
      "training\n",
      "current loss: 7.1501288414001465\n",
      "training\n",
      "current loss: 6.130326271057129\n",
      "training\n",
      "current loss: 5.871685028076172\n",
      "training\n",
      "current loss: 7.032922744750977\n",
      "training\n",
      "current loss: 5.244501113891602\n",
      "training\n",
      "current loss: 5.567820072174072\n",
      "training\n",
      "current loss: 6.406212329864502\n",
      "training\n",
      "current loss: 5.341945171356201\n",
      "training\n",
      "current loss: 6.7483978271484375\n",
      "training\n",
      "current loss: 6.080389022827148\n",
      "training\n",
      "current loss: 7.0203962326049805\n",
      "training\n",
      "current loss: 8.598365783691406\n",
      "training\n",
      "current loss: 7.909198760986328\n",
      "training\n",
      "current loss: 5.246685981750488\n",
      "training\n",
      "current loss: 6.907150745391846\n",
      "training\n",
      "current loss: 7.112293243408203\n",
      "training\n",
      "current loss: 7.793806552886963\n",
      "training\n",
      "current loss: 6.443551540374756\n",
      "training\n",
      "current loss: 5.053773880004883\n",
      "training\n",
      "current loss: 6.868704795837402\n",
      "training\n",
      "current loss: 8.063179016113281\n",
      "training\n",
      "current loss: 6.263810157775879\n",
      "training\n",
      "current loss: 7.859985828399658\n",
      "training\n",
      "current loss: 6.011555194854736\n",
      "training\n",
      "current loss: 6.534670352935791\n",
      "training\n",
      "current loss: 6.991883754730225\n",
      "training\n",
      "current loss: 6.015678405761719\n",
      "training\n",
      "current loss: 6.529268741607666\n",
      "training\n",
      "current loss: 8.96378231048584\n",
      "training\n",
      "current loss: 9.918473243713379\n",
      "training\n",
      "current loss: 8.520030975341797\n",
      "training\n",
      "current loss: 9.119462013244629\n",
      "training\n",
      "current loss: 8.121970176696777\n",
      "training\n",
      "current loss: 7.993563175201416\n",
      "training\n",
      "current loss: 6.59402322769165\n",
      "training\n",
      "current loss: 8.120327949523926\n",
      "training\n",
      "current loss: 6.013838768005371\n",
      "training\n",
      "current loss: 7.978827476501465\n",
      "training\n",
      "current loss: 6.736376762390137\n",
      "training\n",
      "current loss: 7.010078430175781\n",
      "training\n",
      "current loss: 5.87492561340332\n",
      "training\n",
      "current loss: 6.509601593017578\n",
      "training\n",
      "current loss: 6.0130181312561035\n",
      "training\n",
      "current loss: 6.020358085632324\n",
      "training\n",
      "current loss: 5.016748428344727\n",
      "training\n",
      "current loss: 5.272161960601807\n",
      "training\n",
      "current loss: 5.0201239585876465\n",
      "training\n",
      "current loss: 5.012744903564453\n",
      "training\n",
      "current loss: 6.441649913787842\n",
      "training\n",
      "current loss: 7.148655891418457\n",
      "training\n",
      "current loss: 5.687571048736572\n",
      "training\n",
      "current loss: 6.904429912567139\n",
      "training\n",
      "current loss: 9.55296802520752\n",
      "training\n",
      "current loss: 6.022871494293213\n",
      "training\n",
      "current loss: 7.210235595703125\n",
      "training\n",
      "current loss: 6.013908386230469\n",
      "training\n",
      "current loss: 8.005435943603516\n",
      "training\n",
      "current loss: 6.708449363708496\n",
      "training\n",
      "current loss: 9.980154991149902\n",
      "training\n",
      "current loss: 6.817233085632324\n",
      "training\n",
      "current loss: 5.881332874298096\n",
      "training\n",
      "current loss: 5.127824306488037\n",
      "training\n",
      "current loss: 7.002789497375488\n",
      "training\n",
      "current loss: 5.2537736892700195\n",
      "training\n",
      "current loss: 6.037470817565918\n",
      "training\n",
      "current loss: 6.237301826477051\n",
      "training\n",
      "current loss: 7.988271236419678\n",
      "training\n",
      "current loss: 8.961628913879395\n",
      "training\n",
      "current loss: 6.1208672523498535\n",
      "training\n",
      "current loss: 8.35810661315918\n",
      "training\n",
      "current loss: 6.076127052307129\n",
      "training\n",
      "current loss: 6.033349990844727\n",
      "training\n",
      "current loss: 7.009312152862549\n",
      "training\n",
      "current loss: 8.61461067199707\n",
      "training\n",
      "current loss: 8.290112495422363\n",
      "training\n",
      "current loss: 8.015458106994629\n",
      "training\n",
      "current loss: 5.997804164886475\n",
      "training\n",
      "current loss: 6.707587242126465\n",
      "training\n",
      "current loss: 5.1925249099731445\n",
      "training\n",
      "current loss: 7.009360313415527\n",
      "training\n",
      "current loss: 7.073019027709961\n",
      "training\n",
      "current loss: 5.016787528991699\n",
      "training\n",
      "current loss: 5.827548503875732\n",
      "training\n",
      "current loss: 5.950137615203857\n",
      "training\n",
      "current loss: 8.773736953735352\n",
      "training\n",
      "current loss: 6.031129360198975\n",
      "training\n",
      "current loss: 6.559621334075928\n",
      "training\n",
      "current loss: 7.157581329345703\n",
      "training\n",
      "current loss: 8.046873092651367\n",
      "training\n",
      "current loss: 7.002348899841309\n",
      "training\n",
      "current loss: 5.123006343841553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "current loss: 6.039558410644531\n",
      "training\n",
      "current loss: 7.396637916564941\n",
      "training\n",
      "current loss: 7.93662166595459\n",
      "training\n",
      "current loss: 9.012825965881348\n",
      "training\n",
      "current loss: 5.112208366394043\n",
      "training\n",
      "current loss: 6.032895088195801\n",
      "training\n",
      "current loss: 5.02143669128418\n",
      "training\n",
      "current loss: 6.985262393951416\n",
      "training\n",
      "current loss: 6.159646034240723\n",
      "training\n",
      "current loss: 6.66090726852417\n",
      "training\n",
      "current loss: 7.011622428894043\n",
      "training\n",
      "current loss: 7.07234525680542\n",
      "training\n",
      "current loss: 5.761606693267822\n",
      "training\n",
      "current loss: 7.877546787261963\n",
      "training\n",
      "current loss: 7.01157283782959\n",
      "training\n",
      "current loss: 5.646257400512695\n",
      "training\n",
      "current loss: 6.043929100036621\n",
      "training\n",
      "current loss: 5.300237655639648\n",
      "training\n",
      "current loss: 6.842912197113037\n",
      "training\n",
      "current loss: 5.022723197937012\n",
      "training\n",
      "current loss: 6.412811756134033\n",
      "training\n",
      "current loss: 7.003622531890869\n",
      "training\n",
      "current loss: 6.025242805480957\n",
      "training\n",
      "current loss: 5.285910129547119\n",
      "training\n",
      "current loss: 8.930438041687012\n",
      "training\n",
      "current loss: 6.994601726531982\n",
      "training\n",
      "current loss: 6.048333168029785\n",
      "training\n",
      "current loss: 5.147225379943848\n",
      "training\n",
      "current loss: 7.0185866355896\n",
      "training\n",
      "current loss: 7.978076934814453\n",
      "training\n",
      "current loss: 8.027159690856934\n",
      "training\n",
      "current loss: 6.948246955871582\n",
      "training\n",
      "current loss: 5.840329170227051\n",
      "training\n",
      "current loss: 6.013872146606445\n",
      "training\n",
      "current loss: 5.984688758850098\n",
      "training\n",
      "current loss: 7.0165486335754395\n",
      "training\n",
      "current loss: 6.016526699066162\n",
      "training\n",
      "current loss: 6.2815327644348145\n",
      "training\n",
      "current loss: 6.013965606689453\n",
      "training\n",
      "current loss: 10.30241870880127\n",
      "training\n",
      "current loss: 8.043024063110352\n",
      "training\n",
      "current loss: 7.070916175842285\n",
      "training\n",
      "current loss: 7.1951141357421875\n",
      "training\n",
      "current loss: 6.3756937980651855\n",
      "training\n",
      "current loss: 6.177397727966309\n",
      "training\n",
      "current loss: 7.717858791351318\n",
      "training\n",
      "current loss: 5.922479152679443\n",
      "training\n",
      "current loss: 7.869126796722412\n",
      "training\n",
      "current loss: 7.04473876953125\n",
      "training\n",
      "current loss: 5.878102779388428\n",
      "training\n",
      "current loss: 5.36071252822876\n",
      "training\n",
      "current loss: 6.246522903442383\n",
      "training\n",
      "current loss: 6.951771259307861\n",
      "training\n",
      "current loss: 7.4257378578186035\n",
      "training\n",
      "current loss: 6.030909061431885\n",
      "training\n",
      "current loss: 5.7360334396362305\n",
      "training\n",
      "current loss: 8.990283966064453\n",
      "training\n",
      "current loss: 7.901158332824707\n",
      "training\n",
      "current loss: 6.6834187507629395\n",
      "training\n",
      "current loss: 8.411155700683594\n",
      "training\n",
      "current loss: 6.092756271362305\n",
      "training\n",
      "current loss: 7.043998718261719\n",
      "training\n",
      "current loss: 6.174906253814697\n",
      "training\n",
      "current loss: 6.730069637298584\n",
      "training\n",
      "current loss: 10.124622344970703\n",
      "training\n",
      "current loss: 7.511048316955566\n",
      "training\n",
      "current loss: 10.53757095336914\n",
      "training\n",
      "current loss: 11.299674987792969\n",
      "training\n",
      "current loss: 8.963160514831543\n",
      "training\n",
      "current loss: 6.0134735107421875\n",
      "training\n",
      "current loss: 5.069342136383057\n",
      "training\n",
      "current loss: 9.642399787902832\n",
      "training\n",
      "current loss: 8.657086372375488\n",
      "training\n",
      "current loss: 8.870650291442871\n",
      "training\n",
      "current loss: 6.346217155456543\n",
      "training\n",
      "current loss: 8.117470741271973\n",
      "training\n",
      "current loss: 11.706523895263672\n",
      "training\n",
      "current loss: 6.041621208190918\n",
      "training\n",
      "current loss: 7.948967456817627\n",
      "training\n",
      "current loss: 9.267668724060059\n",
      "training\n",
      "current loss: 7.475637435913086\n",
      "training\n",
      "current loss: 8.99213981628418\n",
      "training\n",
      "current loss: 5.79962682723999\n",
      "training\n",
      "current loss: 7.294367790222168\n",
      "training\n",
      "current loss: 7.8735246658325195\n",
      "training\n",
      "current loss: 5.890494346618652\n",
      "training\n",
      "current loss: 5.996969223022461\n",
      "training\n",
      "current loss: 7.069406986236572\n",
      "training\n",
      "current loss: 6.27623987197876\n",
      "training\n",
      "current loss: 7.472475051879883\n",
      "training\n",
      "current loss: 7.139381408691406\n",
      "training\n",
      "current loss: 6.861523151397705\n",
      "training\n",
      "current loss: 7.015427589416504\n",
      "training\n",
      "current loss: 7.499876022338867\n",
      "training\n",
      "current loss: 6.2142486572265625\n",
      "training\n",
      "current loss: 9.047800064086914\n",
      "training\n",
      "current loss: 7.629411697387695\n",
      "training\n",
      "current loss: 7.955826282501221\n",
      "training\n",
      "current loss: 6.946836948394775\n",
      "training\n",
      "current loss: 6.670858383178711\n",
      "training\n",
      "current loss: 6.275552272796631\n",
      "training\n",
      "current loss: 5.297708988189697\n",
      "training\n",
      "current loss: 6.833568096160889\n",
      "training\n",
      "current loss: 8.009735107421875\n",
      "training\n",
      "current loss: 6.260592937469482\n",
      "training\n",
      "current loss: 5.079571723937988\n",
      "training\n",
      "current loss: 8.085155487060547\n",
      "training\n",
      "current loss: 6.836761474609375\n",
      "training\n",
      "current loss: 7.388944149017334\n",
      "training\n",
      "current loss: 6.771534442901611\n",
      "training\n",
      "current loss: 8.999603271484375\n",
      "training\n",
      "current loss: 8.386941909790039\n",
      "training\n",
      "current loss: 6.484002590179443\n",
      "training\n",
      "current loss: 5.403002738952637\n",
      "training\n",
      "current loss: 7.061025142669678\n",
      "training\n",
      "current loss: 7.671558856964111\n",
      "training\n",
      "current loss: 6.010325908660889\n",
      "training\n",
      "current loss: 6.746484756469727\n",
      "training\n",
      "current loss: 5.7053985595703125\n",
      "training\n",
      "current loss: 8.606061935424805\n",
      "training\n",
      "current loss: 5.985255241394043\n",
      "training\n",
      "current loss: 7.055121421813965\n",
      "training\n",
      "current loss: 5.01308012008667\n",
      "training\n",
      "current loss: 6.061024188995361\n",
      "training\n",
      "current loss: 7.013016223907471\n",
      "training\n",
      "current loss: 7.114880561828613\n",
      "training\n",
      "current loss: 8.001226425170898\n",
      "training\n",
      "current loss: 7.397575378417969\n",
      "training\n",
      "current loss: 7.005447864532471\n",
      "training\n",
      "current loss: 11.102182388305664\n",
      "training\n",
      "current loss: 7.001592636108398\n",
      "training\n",
      "current loss: 7.210377216339111\n",
      "training\n",
      "current loss: 5.959356307983398\n",
      "training\n",
      "current loss: 7.023094177246094\n",
      "training\n",
      "current loss: 7.013143539428711\n",
      "training\n",
      "current loss: 5.6894073486328125\n",
      "training\n",
      "current loss: 6.124589920043945\n",
      "training\n",
      "current loss: 8.502218246459961\n",
      "training\n",
      "current loss: 5.0349955558776855\n",
      "training\n",
      "current loss: 6.034686088562012\n",
      "training\n",
      "current loss: 6.979857444763184\n",
      "training\n",
      "current loss: 5.428754806518555\n",
      "training\n",
      "current loss: 6.18601131439209\n",
      "training\n",
      "current loss: 6.052590847015381\n",
      "training\n",
      "current loss: 6.2610578536987305\n",
      "training\n",
      "current loss: 10.927810668945312\n",
      "training\n",
      "current loss: 6.742053031921387\n",
      "training\n",
      "current loss: 8.985284805297852\n",
      "training\n",
      "current loss: 8.060792922973633\n",
      "training\n",
      "current loss: 5.925745010375977\n",
      "training\n",
      "current loss: 7.955509185791016\n",
      "training\n",
      "current loss: 5.993251800537109\n",
      "training\n",
      "current loss: 7.916417598724365\n",
      "training\n",
      "current loss: 11.326106071472168\n",
      "training\n",
      "current loss: 10.92630672454834\n",
      "training\n",
      "current loss: 6.591597557067871\n",
      "training\n",
      "current loss: 6.262057304382324\n",
      "training\n",
      "current loss: 6.609262943267822\n",
      "training\n",
      "current loss: 8.890326499938965\n",
      "training\n",
      "current loss: 6.263322353363037\n",
      "training\n",
      "current loss: 6.815254211425781\n",
      "training\n",
      "current loss: 8.156227111816406\n",
      "training\n",
      "current loss: 5.276090145111084\n",
      "training\n",
      "current loss: 7.513830184936523\n",
      "training\n",
      "current loss: 7.928999900817871\n",
      "training\n",
      "current loss: 5.809500217437744\n",
      "training\n",
      "current loss: 9.361376762390137\n",
      "training\n",
      "current loss: 8.480642318725586\n",
      "training\n",
      "current loss: 8.107778549194336\n",
      "training\n",
      "current loss: 9.73605728149414\n",
      "training\n",
      "current loss: 9.621832847595215\n",
      "training\n",
      "current loss: 6.608080863952637\n",
      "training\n",
      "current loss: 6.966569423675537\n",
      "training\n",
      "current loss: 5.043149948120117\n",
      "training\n",
      "current loss: 6.008939266204834\n",
      "training\n",
      "current loss: 5.063294887542725\n",
      "training\n",
      "current loss: 5.9727349281311035\n",
      "training\n",
      "current loss: 6.734365940093994\n",
      "training\n",
      "current loss: 8.636459350585938\n",
      "training\n",
      "current loss: 8.921619415283203\n",
      "training\n",
      "current loss: 7.059812545776367\n",
      "training\n",
      "current loss: 7.710183143615723\n",
      "training\n",
      "current loss: 6.106513500213623\n",
      "training\n",
      "current loss: 5.209681987762451\n",
      "training\n",
      "current loss: 5.255875587463379\n",
      "training\n",
      "current loss: 6.83540153503418\n",
      "training\n",
      "current loss: 5.0133771896362305\n",
      "training\n",
      "current loss: 9.477216720581055\n",
      "training\n",
      "current loss: 8.010931968688965\n",
      "training\n",
      "current loss: 6.977178573608398\n",
      "training\n",
      "current loss: 6.082142353057861\n",
      "training\n",
      "current loss: 5.016994953155518\n",
      "training\n",
      "current loss: 6.088438034057617\n",
      "training\n",
      "current loss: 7.887424945831299\n",
      "training\n",
      "current loss: 8.765917778015137\n",
      "training\n",
      "current loss: 7.852926731109619\n",
      "training\n",
      "current loss: 5.310939788818359\n",
      "training\n",
      "current loss: 6.7060065269470215\n",
      "training\n",
      "current loss: 6.636960506439209\n",
      "training\n",
      "current loss: 5.028696537017822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "current loss: 6.021851062774658\n",
      "training\n",
      "current loss: 6.1049370765686035\n",
      "training\n",
      "current loss: 5.993216514587402\n",
      "training\n",
      "current loss: 5.941198348999023\n",
      "training\n",
      "current loss: 8.575883865356445\n",
      "training\n",
      "current loss: 5.1056718826293945\n",
      "training\n",
      "current loss: 6.9967122077941895\n",
      "training\n",
      "current loss: 7.727817535400391\n",
      "training\n",
      "current loss: 6.344146251678467\n",
      "training\n",
      "current loss: 7.9867634773254395\n",
      "training\n",
      "current loss: 6.991996765136719\n",
      "training\n",
      "current loss: 6.850757598876953\n",
      "training\n",
      "current loss: 6.573969841003418\n",
      "training\n",
      "current loss: 7.0721354484558105\n",
      "training\n",
      "current loss: 5.026459217071533\n",
      "training\n",
      "current loss: 5.954272270202637\n",
      "training\n",
      "current loss: 7.41362190246582\n",
      "training\n",
      "current loss: 5.054441452026367\n",
      "training\n",
      "current loss: 6.092312335968018\n",
      "training\n",
      "current loss: 7.9289445877075195\n",
      "training\n",
      "current loss: 6.654376983642578\n",
      "training\n",
      "current loss: 6.069939613342285\n",
      "training\n",
      "current loss: 6.542503356933594\n",
      "training\n",
      "current loss: 8.075883865356445\n",
      "training\n",
      "current loss: 8.88260555267334\n",
      "training\n",
      "current loss: 10.779232025146484\n",
      "training\n",
      "current loss: 5.8184099197387695\n",
      "training\n",
      "current loss: 8.255338668823242\n",
      "training\n",
      "current loss: 6.847153186798096\n",
      "training\n",
      "current loss: 6.784701347351074\n",
      "training\n",
      "current loss: 8.79289436340332\n",
      "training\n",
      "current loss: 7.155839920043945\n",
      "training\n",
      "current loss: 5.4586029052734375\n",
      "training\n",
      "current loss: 5.405823707580566\n",
      "training\n",
      "current loss: 10.948448181152344\n",
      "training\n",
      "current loss: 8.94367504119873\n",
      "training\n",
      "current loss: 8.204751968383789\n",
      "training\n",
      "current loss: 11.010868072509766\n",
      "training\n",
      "current loss: 6.353424072265625\n",
      "training\n",
      "current loss: 10.23293685913086\n",
      "training\n",
      "current loss: 9.011414527893066\n",
      "training\n",
      "current loss: 9.210902214050293\n",
      "training\n",
      "current loss: 9.182209968566895\n",
      "training\n",
      "current loss: 9.01039981842041\n",
      "training\n",
      "current loss: 6.005343914031982\n",
      "training\n",
      "current loss: 5.131631851196289\n",
      "training\n",
      "current loss: 8.311111450195312\n",
      "training\n",
      "current loss: 5.210570812225342\n",
      "training\n",
      "current loss: 5.031588077545166\n",
      "training\n",
      "current loss: 7.980170249938965\n",
      "training\n",
      "current loss: 7.704870223999023\n",
      "training\n",
      "current loss: 6.938248634338379\n",
      "training\n",
      "current loss: 8.558731079101562\n",
      "training\n",
      "current loss: 7.0627641677856445\n",
      "training\n",
      "current loss: 6.509799003601074\n",
      "training\n",
      "current loss: 6.031707763671875\n",
      "training\n",
      "current loss: 6.942009449005127\n",
      "training\n",
      "current loss: 6.54836893081665\n",
      "training\n",
      "current loss: 7.042178630828857\n",
      "training\n",
      "current loss: 5.645768165588379\n",
      "training\n",
      "current loss: 6.470189094543457\n",
      "training\n",
      "current loss: 8.375265121459961\n",
      "training\n",
      "current loss: 6.017715930938721\n",
      "training\n",
      "current loss: 7.851596832275391\n",
      "training\n",
      "current loss: 6.180608749389648\n",
      "training\n",
      "current loss: 6.008535385131836\n",
      "training\n",
      "current loss: 9.255386352539062\n",
      "training\n",
      "current loss: 7.67470121383667\n",
      "training\n",
      "current loss: 10.772459983825684\n",
      "training\n",
      "current loss: 5.381409168243408\n",
      "training\n",
      "current loss: 8.77726936340332\n",
      "training\n",
      "current loss: 7.006156921386719\n",
      "training\n",
      "current loss: 6.013869285583496\n",
      "training\n",
      "current loss: 5.8753132820129395\n",
      "training\n",
      "current loss: 7.236769199371338\n",
      "training\n",
      "current loss: 6.127496719360352\n",
      "training\n",
      "current loss: 8.479732513427734\n",
      "training\n",
      "current loss: 7.924278259277344\n",
      "training\n",
      "current loss: 7.477308750152588\n",
      "training\n",
      "current loss: 8.04520034790039\n",
      "training\n",
      "current loss: 10.71749210357666\n",
      "training\n",
      "current loss: 8.979144096374512\n",
      "training\n",
      "current loss: 6.370229721069336\n",
      "training\n",
      "current loss: 5.937300205230713\n",
      "training\n",
      "current loss: 7.198107719421387\n",
      "training\n",
      "current loss: 7.084593772888184\n",
      "training\n",
      "current loss: 7.881309509277344\n",
      "training\n",
      "current loss: 5.066713333129883\n",
      "training\n",
      "current loss: 8.84465217590332\n",
      "training\n",
      "current loss: 7.03814172744751\n",
      "training\n",
      "current loss: 5.994117736816406\n",
      "training\n",
      "current loss: 6.008618354797363\n",
      "training\n",
      "current loss: 7.926702499389648\n",
      "training\n",
      "current loss: 5.3500494956970215\n",
      "training\n",
      "current loss: 5.226799488067627\n",
      "training\n",
      "current loss: 5.058771133422852\n",
      "training\n",
      "current loss: 7.739031791687012\n",
      "training\n",
      "current loss: 5.794960975646973\n",
      "training\n",
      "current loss: 6.738508701324463\n",
      "training\n",
      "current loss: 7.8603363037109375\n",
      "training\n",
      "current loss: 12.77726936340332\n",
      "training\n",
      "current loss: 6.635864734649658\n",
      "training\n",
      "current loss: 7.053287982940674\n",
      "training\n",
      "current loss: 9.715641975402832\n",
      "training\n",
      "current loss: 8.113140106201172\n",
      "training\n",
      "current loss: 5.421148300170898\n",
      "training\n",
      "current loss: 7.0136213302612305\n",
      "training\n",
      "current loss: 5.836780071258545\n",
      "training\n",
      "current loss: 6.548336505889893\n",
      "training\n",
      "current loss: 7.902896881103516\n",
      "training\n",
      "current loss: 5.615651607513428\n",
      "training\n",
      "current loss: 7.8778767585754395\n",
      "training\n",
      "current loss: 5.187995910644531\n",
      "training\n",
      "current loss: 10.323112487792969\n",
      "training\n",
      "current loss: 6.353867053985596\n",
      "training\n",
      "current loss: 7.216800689697266\n",
      "training\n",
      "current loss: 5.05866003036499\n",
      "training\n",
      "current loss: 6.330160140991211\n",
      "training\n",
      "current loss: 8.639644622802734\n",
      "training\n",
      "current loss: 7.358318328857422\n",
      "training\n",
      "current loss: 6.021781921386719\n",
      "training\n",
      "current loss: 7.608302116394043\n",
      "training\n",
      "current loss: 8.569526672363281\n",
      "training\n",
      "current loss: 6.708781719207764\n",
      "training\n",
      "current loss: 8.123125076293945\n",
      "training\n",
      "current loss: 6.519911766052246\n",
      "training\n",
      "current loss: 6.803629398345947\n",
      "training\n",
      "current loss: 5.084620475769043\n",
      "training\n",
      "current loss: 8.039774894714355\n",
      "training\n",
      "current loss: 6.830816268920898\n",
      "training\n",
      "current loss: 7.564671993255615\n",
      "training\n",
      "current loss: 7.294850826263428\n",
      "training\n",
      "current loss: 5.114672660827637\n",
      "training\n",
      "current loss: 5.522899627685547\n",
      "training\n",
      "current loss: 6.144861698150635\n",
      "training\n",
      "current loss: 6.421860218048096\n",
      "training\n",
      "current loss: 6.996173858642578\n",
      "training\n",
      "current loss: 5.407156944274902\n",
      "training\n",
      "current loss: 5.732717037200928\n",
      "training\n",
      "current loss: 7.099215507507324\n",
      "training\n",
      "current loss: 5.524643898010254\n",
      "training\n",
      "current loss: 8.23978328704834\n",
      "training\n",
      "current loss: 9.511860847473145\n",
      "training\n",
      "current loss: 6.854726314544678\n",
      "training\n",
      "current loss: 5.760979652404785\n",
      "training\n",
      "current loss: 5.187315940856934\n",
      "training\n",
      "current loss: 5.940357208251953\n",
      "training\n",
      "current loss: 8.025223731994629\n",
      "training\n",
      "current loss: 8.785847663879395\n",
      "training\n",
      "current loss: 6.03956937789917\n",
      "training\n",
      "current loss: 7.965868949890137\n",
      "training\n",
      "current loss: 6.337986469268799\n",
      "training\n",
      "current loss: 6.15947151184082\n",
      "training\n",
      "current loss: 5.398344039916992\n",
      "training\n",
      "current loss: 5.415358066558838\n",
      "training\n",
      "current loss: 6.8108086585998535\n",
      "training\n",
      "current loss: 8.473713874816895\n",
      "training\n",
      "current loss: 8.809036254882812\n",
      "training\n",
      "current loss: 5.0218095779418945\n",
      "training\n",
      "current loss: 7.096888065338135\n",
      "training\n",
      "current loss: 6.063568115234375\n",
      "training\n",
      "current loss: 8.106982231140137\n",
      "training\n",
      "current loss: 5.045711994171143\n",
      "training\n",
      "current loss: 6.015125274658203\n",
      "training\n",
      "current loss: 8.663446426391602\n",
      "training\n",
      "current loss: 6.751481056213379\n",
      "training\n",
      "current loss: 7.9753217697143555\n",
      "training\n",
      "current loss: 6.517217636108398\n",
      "training\n",
      "current loss: 7.14501953125\n",
      "training\n",
      "current loss: 7.464093208312988\n",
      "training\n",
      "current loss: 7.375085830688477\n",
      "training\n",
      "current loss: 6.302660942077637\n",
      "training\n",
      "current loss: 6.0178937911987305\n",
      "training\n",
      "current loss: 7.934744834899902\n",
      "training\n",
      "current loss: 5.0439324378967285\n",
      "training\n",
      "current loss: 5.955045700073242\n",
      "training\n",
      "current loss: 8.083669662475586\n",
      "training\n",
      "current loss: 6.382988929748535\n",
      "training\n",
      "current loss: 8.010964393615723\n",
      "training\n",
      "current loss: 6.040416717529297\n",
      "training\n",
      "current loss: 6.857285499572754\n",
      "training\n",
      "current loss: 6.011331081390381\n",
      "training\n",
      "current loss: 7.018982887268066\n",
      "training\n",
      "current loss: 9.285536766052246\n",
      "training\n",
      "current loss: 5.600903511047363\n",
      "training\n",
      "current loss: 6.645612716674805\n",
      "training\n",
      "current loss: 5.0149126052856445\n",
      "training\n",
      "current loss: 5.400141716003418\n",
      "training\n",
      "current loss: 7.439029216766357\n",
      "training\n",
      "current loss: 5.677581787109375\n",
      "training\n",
      "current loss: 7.150510787963867\n",
      "training\n",
      "current loss: 9.404428482055664\n",
      "training\n",
      "current loss: 5.220922946929932\n",
      "training\n",
      "current loss: 8.629382133483887\n",
      "training\n",
      "current loss: 8.75086498260498\n",
      "training\n",
      "current loss: 7.336015224456787\n",
      "training\n",
      "current loss: 6.384878158569336\n",
      "training\n",
      "current loss: 5.728023052215576\n",
      "training\n",
      "current loss: 9.262192726135254\n",
      "training\n",
      "current loss: 7.071168422698975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "current loss: 6.062479019165039\n",
      "training\n",
      "current loss: 7.85239839553833\n",
      "training\n",
      "current loss: 6.1123809814453125\n",
      "training\n",
      "current loss: 6.006380558013916\n",
      "training\n",
      "current loss: 7.663303375244141\n",
      "training\n",
      "current loss: 5.52585506439209\n",
      "training\n",
      "current loss: 6.859793663024902\n",
      "training\n",
      "current loss: 6.787890911102295\n",
      "training\n",
      "current loss: 5.651032447814941\n",
      "training\n",
      "current loss: 5.389668941497803\n",
      "training\n",
      "current loss: 7.83926248550415\n",
      "training\n",
      "current loss: 6.127013683319092\n",
      "training\n",
      "current loss: 5.399109363555908\n",
      "training\n",
      "current loss: 5.906515121459961\n",
      "training\n",
      "current loss: 6.757706642150879\n",
      "training\n",
      "current loss: 8.05049991607666\n",
      "training\n",
      "current loss: 7.874599456787109\n",
      "training\n",
      "current loss: 7.755462646484375\n",
      "training\n",
      "current loss: 8.980405807495117\n",
      "training\n",
      "current loss: 6.0217084884643555\n",
      "training\n",
      "current loss: 8.376322746276855\n",
      "training\n",
      "current loss: 8.128643035888672\n",
      "training\n",
      "current loss: 7.026290416717529\n",
      "training\n",
      "current loss: 6.992076873779297\n",
      "training\n",
      "current loss: 9.756786346435547\n",
      "training\n",
      "current loss: 7.018232345581055\n",
      "training\n",
      "current loss: 8.015162467956543\n",
      "training\n",
      "current loss: 6.608285903930664\n",
      "training\n",
      "current loss: 10.065218925476074\n",
      "training\n",
      "current loss: 6.020320892333984\n",
      "training\n",
      "current loss: 6.30844783782959\n",
      "training\n",
      "current loss: 5.052524566650391\n",
      "training\n",
      "current loss: 5.032096862792969\n",
      "training\n",
      "current loss: 5.805386543273926\n",
      "training\n",
      "current loss: 6.052896022796631\n",
      "training\n",
      "current loss: 7.984555244445801\n",
      "training\n",
      "current loss: 7.8551740646362305\n",
      "training\n",
      "current loss: 7.288264751434326\n",
      "training\n",
      "current loss: 7.558696746826172\n",
      "training\n",
      "current loss: 5.556057929992676\n",
      "training\n",
      "current loss: 5.417588233947754\n",
      "training\n",
      "current loss: 5.799827575683594\n",
      "training\n",
      "current loss: 5.013708114624023\n",
      "training\n",
      "current loss: 5.229875564575195\n",
      "training\n",
      "current loss: 5.066576957702637\n",
      "training\n",
      "current loss: 7.143829822540283\n",
      "training\n",
      "current loss: 8.863635063171387\n",
      "training\n",
      "current loss: 6.0770721435546875\n",
      "training\n",
      "current loss: 7.026436805725098\n",
      "training\n",
      "current loss: 7.2064995765686035\n",
      "training\n",
      "current loss: 6.9567484855651855\n",
      "training\n",
      "current loss: 5.9893670082092285\n",
      "training\n",
      "current loss: 7.005549907684326\n",
      "training\n",
      "current loss: 9.321995735168457\n",
      "training\n",
      "current loss: 7.294129371643066\n",
      "training\n",
      "current loss: 6.933983325958252\n",
      "training\n",
      "current loss: 6.0274505615234375\n",
      "training\n",
      "current loss: 6.014873504638672\n",
      "training\n",
      "current loss: 7.994596004486084\n",
      "training\n",
      "current loss: 6.886233806610107\n",
      "training\n",
      "current loss: 7.043574333190918\n",
      "training\n",
      "current loss: 6.340076446533203\n",
      "training\n",
      "current loss: 7.286471843719482\n",
      "training\n",
      "current loss: 8.076570510864258\n",
      "training\n",
      "current loss: 6.965437412261963\n",
      "training\n",
      "current loss: 6.795769691467285\n",
      "training\n",
      "current loss: 6.008634567260742\n",
      "training\n",
      "current loss: 7.041379451751709\n",
      "training\n",
      "current loss: 6.966197490692139\n",
      "training\n",
      "current loss: 9.164759635925293\n",
      "training\n",
      "current loss: 5.705325126647949\n",
      "training\n",
      "current loss: 8.364999771118164\n",
      "training\n",
      "current loss: 9.415022850036621\n",
      "training\n",
      "current loss: 7.921133041381836\n",
      "training\n",
      "current loss: 9.176877975463867\n",
      "training\n",
      "current loss: 6.020596027374268\n",
      "training\n",
      "current loss: 5.17337703704834\n",
      "training\n",
      "current loss: 6.248579025268555\n",
      "training\n",
      "current loss: 6.664736747741699\n",
      "training\n",
      "current loss: 6.273325443267822\n",
      "training\n",
      "current loss: 6.445184707641602\n",
      "training\n",
      "current loss: 8.119945526123047\n",
      "training\n",
      "current loss: 6.9364519119262695\n",
      "training\n",
      "current loss: 8.011676788330078\n",
      "training\n",
      "current loss: 7.015120983123779\n",
      "training\n",
      "current loss: 9.855059623718262\n",
      "training\n",
      "current loss: 8.143503189086914\n",
      "training\n",
      "current loss: 7.055051326751709\n",
      "training\n",
      "current loss: 10.278214454650879\n",
      "training\n",
      "current loss: 5.67415189743042\n",
      "training\n",
      "current loss: 6.019804954528809\n",
      "training\n",
      "current loss: 7.263998985290527\n",
      "training\n",
      "current loss: 7.326737403869629\n",
      "training\n",
      "current loss: 8.147615432739258\n",
      "training\n",
      "current loss: 6.012500286102295\n",
      "training\n",
      "current loss: 5.108325958251953\n",
      "training\n",
      "current loss: 6.017437934875488\n",
      "training\n",
      "current loss: 6.839402198791504\n",
      "training\n",
      "current loss: 6.915899276733398\n",
      "training\n",
      "current loss: 7.3127055168151855\n",
      "training\n",
      "current loss: 7.016324043273926\n",
      "training\n",
      "current loss: 5.243691444396973\n",
      "training\n",
      "current loss: 6.986508369445801\n",
      "training\n",
      "current loss: 6.20030403137207\n",
      "training\n",
      "current loss: 8.027978897094727\n",
      "training\n",
      "current loss: 7.957109451293945\n",
      "training\n",
      "current loss: 8.907674789428711\n",
      "training\n",
      "current loss: 7.011967658996582\n",
      "training\n",
      "current loss: 5.662586212158203\n",
      "training\n",
      "current loss: 5.106987476348877\n",
      "training\n",
      "current loss: 9.496814727783203\n",
      "training\n",
      "current loss: 6.995543003082275\n",
      "training\n",
      "current loss: 7.004305839538574\n",
      "training\n",
      "current loss: 6.1094183921813965\n",
      "training\n",
      "current loss: 9.136701583862305\n",
      "training\n",
      "current loss: 5.976940155029297\n",
      "training\n",
      "current loss: 5.986003398895264\n",
      "training\n",
      "current loss: 7.589555740356445\n",
      "training\n",
      "current loss: 5.016479015350342\n",
      "training\n",
      "current loss: 7.872511386871338\n",
      "training\n",
      "current loss: 6.012493133544922\n",
      "training\n",
      "current loss: 5.744833946228027\n",
      "training\n",
      "current loss: 5.0787811279296875\n",
      "training\n",
      "current loss: 6.058876991271973\n",
      "training\n",
      "current loss: 9.789190292358398\n",
      "training\n",
      "current loss: 7.018245697021484\n",
      "training\n",
      "current loss: 6.449835300445557\n",
      "training\n",
      "current loss: 7.296281814575195\n",
      "training\n",
      "current loss: 6.050287246704102\n",
      "training\n",
      "current loss: 6.826686382293701\n",
      "training\n",
      "current loss: 6.07833194732666\n",
      "training\n",
      "current loss: 8.033916473388672\n",
      "training\n",
      "current loss: 8.079482078552246\n",
      "training\n",
      "current loss: 7.025821685791016\n",
      "training\n",
      "current loss: 7.099066257476807\n",
      "training\n",
      "current loss: 8.785224914550781\n",
      "training\n",
      "current loss: 8.281902313232422\n",
      "training\n",
      "current loss: 5.020914554595947\n",
      "training\n",
      "current loss: 7.237380027770996\n",
      "training\n",
      "current loss: 8.000192642211914\n",
      "training\n",
      "current loss: 5.899980545043945\n",
      "training\n",
      "current loss: 5.378999710083008\n",
      "training\n",
      "current loss: 5.296954154968262\n",
      "training\n",
      "current loss: 7.668797016143799\n",
      "training\n",
      "current loss: 6.246589660644531\n",
      "training\n",
      "current loss: 8.004782676696777\n",
      "training\n",
      "current loss: 7.012928009033203\n",
      "training\n",
      "current loss: 5.11089563369751\n",
      "training\n",
      "current loss: 5.0124688148498535\n",
      "training\n",
      "current loss: 5.020610332489014\n",
      "training\n",
      "current loss: 7.013175964355469\n",
      "training\n",
      "current loss: 8.986115455627441\n",
      "training\n",
      "current loss: 5.433624744415283\n",
      "training\n",
      "current loss: 9.68091106414795\n",
      "training\n",
      "current loss: 5.01359224319458\n",
      "training\n",
      "current loss: 8.80130672454834\n",
      "training\n",
      "current loss: 7.012787818908691\n",
      "training\n",
      "current loss: 7.011740684509277\n",
      "training\n",
      "current loss: 7.0085296630859375\n",
      "training\n",
      "current loss: 7.097373008728027\n",
      "training\n",
      "current loss: 7.312469482421875\n",
      "training\n",
      "current loss: 6.821084499359131\n",
      "training\n",
      "current loss: 7.016867637634277\n",
      "training\n",
      "current loss: 6.909507751464844\n",
      "training\n",
      "current loss: 7.006610870361328\n",
      "training\n",
      "current loss: 9.882041931152344\n",
      "training\n",
      "current loss: 5.645308971405029\n",
      "training\n",
      "current loss: 5.047257423400879\n",
      "training\n",
      "current loss: 6.1210198402404785\n",
      "training\n",
      "current loss: 7.048753261566162\n",
      "training\n",
      "current loss: 6.737682342529297\n",
      "training\n",
      "current loss: 7.011257171630859\n",
      "training\n",
      "current loss: 6.975691795349121\n",
      "training\n",
      "current loss: 5.067896842956543\n",
      "training\n",
      "current loss: 8.106337547302246\n",
      "training\n",
      "current loss: 6.010948657989502\n",
      "training\n",
      "current loss: 6.917632579803467\n",
      "training\n",
      "current loss: 6.049726963043213\n",
      "training\n",
      "current loss: 9.628218650817871\n",
      "training\n",
      "current loss: 7.333907127380371\n",
      "training\n",
      "current loss: 5.015527248382568\n",
      "training\n",
      "current loss: 6.0202836990356445\n",
      "training\n",
      "current loss: 6.018171310424805\n",
      "training\n",
      "current loss: 8.77037525177002\n",
      "training\n",
      "current loss: 7.00493049621582\n",
      "training\n",
      "current loss: 6.360911846160889\n",
      "training\n",
      "current loss: 5.389829158782959\n",
      "training\n",
      "current loss: 7.019317626953125\n",
      "training\n",
      "current loss: 6.040804386138916\n",
      "training\n",
      "current loss: 7.145206451416016\n",
      "training\n",
      "current loss: 6.954807281494141\n",
      "training\n",
      "current loss: 8.886220932006836\n",
      "training\n",
      "current loss: 7.871618747711182\n",
      "training\n",
      "current loss: 7.106571197509766\n",
      "training\n",
      "current loss: 6.849968910217285\n",
      "training\n",
      "current loss: 8.021966934204102\n",
      "training\n",
      "current loss: 8.654385566711426\n",
      "training\n",
      "current loss: 8.008097648620605\n",
      "training\n",
      "current loss: 6.094775676727295\n",
      "training\n",
      "current loss: 5.014043807983398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "current loss: 6.983541011810303\n",
      "training\n",
      "current loss: 6.376364707946777\n",
      "training\n",
      "current loss: 7.6948981285095215\n",
      "training\n",
      "current loss: 10.006683349609375\n",
      "training\n",
      "current loss: 5.5137810707092285\n",
      "training\n",
      "current loss: 5.020934104919434\n",
      "training\n",
      "current loss: 7.0104804039001465\n",
      "training\n",
      "current loss: 6.014801502227783\n",
      "training\n",
      "current loss: 6.007933616638184\n",
      "training\n",
      "current loss: 7.023196220397949\n",
      "training\n",
      "current loss: 6.476802825927734\n",
      "training\n",
      "current loss: 6.0290727615356445\n",
      "training\n",
      "current loss: 8.033995628356934\n",
      "training\n",
      "current loss: 6.1546525955200195\n",
      "training\n",
      "current loss: 6.983323097229004\n",
      "training\n",
      "current loss: 8.099272727966309\n",
      "training\n",
      "current loss: 5.021736145019531\n",
      "training\n",
      "current loss: 8.052678108215332\n",
      "training\n",
      "current loss: 6.007551193237305\n",
      "training\n",
      "current loss: 6.936939239501953\n",
      "training\n",
      "current loss: 7.036518573760986\n",
      "training\n",
      "current loss: 5.012385845184326\n",
      "training\n",
      "current loss: 6.421536922454834\n",
      "training\n",
      "current loss: 7.9899582862854\n",
      "training\n",
      "current loss: 9.42939567565918\n",
      "training\n",
      "current loss: 8.012016296386719\n",
      "training\n",
      "current loss: 8.148429870605469\n",
      "training\n",
      "current loss: 8.253565788269043\n",
      "training\n",
      "current loss: 7.296082019805908\n",
      "training\n",
      "current loss: 5.0164289474487305\n",
      "training\n",
      "current loss: 7.937333106994629\n",
      "training\n",
      "current loss: 6.9984235763549805\n",
      "training\n",
      "current loss: 7.016402244567871\n",
      "training\n",
      "current loss: 6.026871204376221\n",
      "training\n",
      "current loss: 7.7800092697143555\n",
      "training\n",
      "current loss: 7.012721061706543\n",
      "training\n",
      "current loss: 6.854649543762207\n",
      "training\n",
      "current loss: 5.33499813079834\n",
      "training\n",
      "current loss: 6.036613941192627\n",
      "training\n",
      "current loss: 5.591545581817627\n",
      "training\n",
      "current loss: 7.738475322723389\n",
      "training\n",
      "current loss: 5.966962814331055\n",
      "training\n",
      "current loss: 7.9944658279418945\n",
      "training\n",
      "current loss: 7.84729528427124\n",
      "training\n",
      "current loss: 5.178386211395264\n",
      "training\n",
      "current loss: 8.056150436401367\n",
      "training\n",
      "current loss: 10.993793487548828\n",
      "training\n",
      "current loss: 7.075720310211182\n",
      "training\n",
      "current loss: 9.041191101074219\n",
      "training\n",
      "current loss: 8.999772071838379\n",
      "training\n",
      "current loss: 8.24233341217041\n",
      "training\n",
      "current loss: 10.452779769897461\n",
      "training\n",
      "current loss: 6.083641529083252\n",
      "training\n",
      "current loss: 9.006561279296875\n",
      "training\n",
      "current loss: 10.401424407958984\n",
      "training\n",
      "current loss: 8.998679161071777\n",
      "training\n",
      "current loss: 8.00102710723877\n",
      "training\n",
      "current loss: 6.107289791107178\n",
      "training\n",
      "current loss: 6.281230926513672\n",
      "training\n",
      "current loss: 5.258283615112305\n",
      "training\n",
      "current loss: 7.925754547119141\n",
      "training\n",
      "current loss: 5.016193389892578\n",
      "training\n",
      "current loss: 6.162168025970459\n",
      "training\n",
      "current loss: 7.0363264083862305\n",
      "training\n",
      "current loss: 9.080257415771484\n",
      "training\n",
      "current loss: 8.222884178161621\n",
      "training\n",
      "current loss: 10.966506004333496\n",
      "training\n",
      "current loss: 7.282970428466797\n",
      "training\n",
      "current loss: 6.330679416656494\n",
      "training\n",
      "current loss: 7.052875518798828\n",
      "training\n",
      "current loss: 6.937542915344238\n",
      "training\n",
      "current loss: 6.00860595703125\n",
      "training\n",
      "current loss: 6.080005168914795\n",
      "training\n",
      "current loss: 8.863388061523438\n",
      "training\n",
      "current loss: 7.017647743225098\n",
      "training\n",
      "current loss: 6.683666229248047\n",
      "training\n",
      "current loss: 5.901119232177734\n",
      "training\n",
      "current loss: 7.002448081970215\n",
      "training\n",
      "current loss: 6.012570381164551\n",
      "training\n",
      "current loss: 5.066967964172363\n",
      "training\n",
      "current loss: 6.045295238494873\n",
      "training\n",
      "current loss: 5.333094596862793\n",
      "training\n",
      "current loss: 6.026988983154297\n",
      "training\n",
      "current loss: 6.063762664794922\n",
      "training\n",
      "current loss: 7.040661811828613\n",
      "training\n",
      "current loss: 7.640295028686523\n",
      "training\n",
      "current loss: 7.838839530944824\n",
      "training\n",
      "current loss: 5.997908592224121\n",
      "training\n",
      "current loss: 5.603049278259277\n",
      "training\n",
      "current loss: 7.03238582611084\n",
      "training\n",
      "current loss: 8.057855606079102\n",
      "training\n",
      "current loss: 8.558149337768555\n",
      "training\n",
      "current loss: 6.0126471519470215\n",
      "training\n",
      "current loss: 6.726494789123535\n",
      "training\n",
      "current loss: 6.934189796447754\n",
      "training\n",
      "current loss: 5.016361713409424\n",
      "training\n",
      "current loss: 8.009565353393555\n",
      "training\n",
      "current loss: 6.029276371002197\n",
      "training\n",
      "current loss: 7.014691352844238\n",
      "training\n",
      "current loss: 6.022418022155762\n",
      "training\n",
      "current loss: 5.354827404022217\n",
      "training\n",
      "current loss: 7.1865129470825195\n",
      "training\n",
      "current loss: 8.940350532531738\n",
      "training\n",
      "current loss: 5.3562092781066895\n",
      "training\n",
      "current loss: 7.183666229248047\n",
      "training\n",
      "current loss: 6.680197715759277\n",
      "training\n",
      "current loss: 5.202968120574951\n",
      "training\n",
      "current loss: 6.98271369934082\n",
      "training\n",
      "current loss: 6.010067939758301\n",
      "training\n",
      "current loss: 6.8943939208984375\n",
      "training\n",
      "current loss: 6.047153472900391\n",
      "training\n",
      "current loss: 6.1962199211120605\n",
      "training\n",
      "current loss: 5.15139102935791\n",
      "training\n",
      "current loss: 7.760746479034424\n",
      "training\n",
      "current loss: 5.998079776763916\n",
      "training\n",
      "current loss: 6.9470038414001465\n",
      "training\n",
      "current loss: 6.0829243659973145\n",
      "training\n",
      "current loss: 5.022981643676758\n",
      "training\n",
      "current loss: 6.2476887702941895\n",
      "training\n",
      "current loss: 6.912965774536133\n",
      "training\n",
      "current loss: 7.078060150146484\n",
      "training\n",
      "current loss: 7.103878021240234\n",
      "training\n",
      "current loss: 8.371956825256348\n",
      "training\n",
      "current loss: 7.028867721557617\n",
      "training\n",
      "current loss: 6.274840831756592\n",
      "training\n",
      "current loss: 6.018283843994141\n",
      "training\n",
      "current loss: 5.331009387969971\n",
      "training\n",
      "current loss: 8.249561309814453\n",
      "training\n",
      "current loss: 7.091737270355225\n",
      "training\n",
      "current loss: 6.93904447555542\n",
      "training\n",
      "current loss: 5.88026762008667\n",
      "training\n",
      "current loss: 5.035894393920898\n",
      "training\n",
      "current loss: 5.535852432250977\n",
      "training\n",
      "current loss: 7.32029914855957\n",
      "training\n",
      "current loss: 6.501156806945801\n",
      "training\n",
      "current loss: 6.025215148925781\n",
      "training\n",
      "current loss: 5.791618347167969\n",
      "training\n",
      "current loss: 6.280117511749268\n",
      "training\n",
      "current loss: 6.91981315612793\n",
      "training\n",
      "current loss: 5.020313739776611\n",
      "training\n",
      "current loss: 5.046517372131348\n",
      "training\n",
      "current loss: 5.450939178466797\n",
      "training\n",
      "current loss: 7.441597938537598\n",
      "training\n",
      "current loss: 5.87961483001709\n",
      "training\n",
      "current loss: 6.829617977142334\n",
      "training\n",
      "current loss: 5.8970489501953125\n",
      "training\n",
      "current loss: 7.950957298278809\n",
      "training\n",
      "current loss: 7.0323591232299805\n",
      "training\n",
      "current loss: 7.944543361663818\n",
      "training\n",
      "current loss: 6.428167819976807\n",
      "training\n",
      "current loss: 5.155448913574219\n",
      "training\n",
      "current loss: 6.292420387268066\n",
      "training\n",
      "current loss: 5.204310417175293\n",
      "training\n",
      "current loss: 7.025776386260986\n",
      "training\n",
      "current loss: 6.8464436531066895\n",
      "training\n",
      "current loss: 5.448538303375244\n",
      "training\n",
      "current loss: 5.05001163482666\n",
      "training\n",
      "current loss: 5.606520652770996\n",
      "training\n",
      "current loss: 9.962345123291016\n",
      "training\n",
      "current loss: 5.150127410888672\n",
      "training\n",
      "current loss: 8.145530700683594\n",
      "training\n",
      "current loss: 8.623353004455566\n",
      "training\n",
      "current loss: 5.084108829498291\n",
      "training\n",
      "current loss: 5.774255752563477\n",
      "training\n",
      "current loss: 7.366118907928467\n",
      "training\n",
      "current loss: 7.355659484863281\n",
      "training\n",
      "current loss: 5.020745277404785\n",
      "training\n",
      "current loss: 6.001384735107422\n",
      "training\n",
      "current loss: 5.9933929443359375\n",
      "training\n",
      "current loss: 6.679318904876709\n",
      "training\n",
      "current loss: 6.890601634979248\n",
      "training\n",
      "current loss: 6.619632720947266\n",
      "training\n",
      "current loss: 5.098779678344727\n",
      "training\n",
      "current loss: 6.742752552032471\n",
      "training\n",
      "current loss: 5.2553300857543945\n",
      "training\n",
      "current loss: 6.052524566650391\n",
      "training\n",
      "current loss: 6.816050052642822\n",
      "training\n",
      "current loss: 6.857402324676514\n",
      "training\n",
      "current loss: 6.01967716217041\n",
      "training\n",
      "current loss: 8.356635093688965\n",
      "training\n",
      "current loss: 6.054049968719482\n",
      "training\n",
      "current loss: 7.952733516693115\n",
      "training\n",
      "current loss: 6.82991886138916\n",
      "training\n",
      "current loss: 6.893426418304443\n",
      "training\n",
      "current loss: 7.623840808868408\n",
      "training\n",
      "current loss: 6.005995750427246\n",
      "training\n",
      "current loss: 7.15618371963501\n",
      "training\n",
      "current loss: 7.030734062194824\n",
      "training\n",
      "current loss: 6.449228286743164\n",
      "training\n",
      "current loss: 6.429344654083252\n",
      "training\n",
      "current loss: 6.015143871307373\n",
      "training\n",
      "current loss: 6.818342208862305\n",
      "training\n",
      "current loss: 7.287215232849121\n",
      "training\n",
      "current loss: 7.94418478012085\n",
      "training\n",
      "current loss: 5.054232597351074\n",
      "training\n",
      "current loss: 5.023128032684326\n",
      "training\n",
      "current loss: 5.431689739227295\n",
      "training\n",
      "current loss: 7.000259876251221\n",
      "training\n",
      "current loss: 7.1556396484375\n",
      "training\n",
      "current loss: 6.2201409339904785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "current loss: 7.945786476135254\n",
      "training\n",
      "current loss: 8.398125648498535\n",
      "training\n",
      "current loss: 6.828274726867676\n",
      "training\n",
      "current loss: 6.920556545257568\n",
      "training\n",
      "current loss: 7.030367374420166\n",
      "training\n",
      "current loss: 7.514008522033691\n",
      "training\n",
      "current loss: 6.042201042175293\n",
      "training\n",
      "current loss: 7.33882474899292\n",
      "training\n",
      "current loss: 5.115379810333252\n",
      "training\n",
      "current loss: 9.752010345458984\n",
      "training\n",
      "current loss: 9.097580909729004\n",
      "training\n",
      "current loss: 9.009285926818848\n",
      "training\n",
      "current loss: 7.596100807189941\n",
      "training\n",
      "current loss: 6.020967960357666\n",
      "training\n",
      "current loss: 6.391189098358154\n",
      "training\n",
      "current loss: 7.814033508300781\n",
      "training\n",
      "current loss: 6.135636329650879\n",
      "training\n",
      "current loss: 8.184247970581055\n",
      "training\n",
      "current loss: 7.913206100463867\n",
      "training\n",
      "current loss: 5.988706111907959\n",
      "training\n",
      "current loss: 5.905585289001465\n",
      "training\n",
      "current loss: 6.698419570922852\n",
      "training\n",
      "current loss: 8.282532691955566\n",
      "training\n",
      "current loss: 6.94346284866333\n",
      "training\n",
      "current loss: 6.559636116027832\n",
      "training\n",
      "current loss: 9.731204986572266\n",
      "training\n",
      "current loss: 5.5509796142578125\n",
      "training\n",
      "current loss: 7.014835834503174\n",
      "training\n",
      "current loss: 5.201769828796387\n",
      "training\n",
      "current loss: 7.987460136413574\n",
      "training\n",
      "current loss: 5.042433738708496\n",
      "training\n",
      "current loss: 5.152499675750732\n",
      "training\n",
      "current loss: 7.621024131774902\n",
      "training\n",
      "current loss: 7.939668655395508\n",
      "training\n",
      "current loss: 6.151555061340332\n",
      "training\n",
      "current loss: 6.939220905303955\n",
      "training\n",
      "current loss: 5.911801338195801\n",
      "training\n",
      "current loss: 6.964386940002441\n",
      "training\n",
      "current loss: 6.013199806213379\n",
      "training\n",
      "current loss: 7.320648193359375\n",
      "training\n",
      "current loss: 7.053535461425781\n",
      "training\n",
      "current loss: 7.286385536193848\n",
      "training\n",
      "current loss: 7.891210079193115\n",
      "training\n",
      "current loss: 5.802453994750977\n",
      "training\n",
      "current loss: 7.084010601043701\n",
      "training\n",
      "current loss: 6.01662540435791\n",
      "training\n",
      "current loss: 8.666366577148438\n",
      "training\n",
      "current loss: 5.1815972328186035\n",
      "training\n",
      "current loss: 7.776771545410156\n",
      "training\n",
      "current loss: 7.955796241760254\n",
      "training\n",
      "current loss: 5.663059711456299\n",
      "training\n",
      "current loss: 7.879040718078613\n",
      "training\n",
      "current loss: 6.001103401184082\n",
      "training\n",
      "current loss: 6.165618896484375\n",
      "training\n",
      "current loss: 8.41213321685791\n",
      "training\n",
      "current loss: 5.304932594299316\n",
      "training\n",
      "current loss: 6.6051201820373535\n",
      "training\n",
      "current loss: 7.070792198181152\n",
      "training\n",
      "current loss: 6.305974960327148\n",
      "training\n",
      "current loss: 6.304009437561035\n",
      "training\n",
      "current loss: 9.138677597045898\n",
      "training\n",
      "current loss: 6.700636386871338\n",
      "training\n",
      "current loss: 5.05912446975708\n",
      "training\n",
      "current loss: 6.0693769454956055\n",
      "training\n",
      "current loss: 7.293895721435547\n",
      "training\n",
      "current loss: 6.086313247680664\n",
      "training\n",
      "current loss: 6.013406753540039\n",
      "training\n",
      "current loss: 7.22033166885376\n",
      "training\n",
      "current loss: 5.859933376312256\n",
      "training\n",
      "current loss: 7.296723365783691\n",
      "training\n",
      "current loss: 6.023072242736816\n",
      "training\n",
      "current loss: 7.901310920715332\n",
      "training\n",
      "current loss: 7.986103534698486\n",
      "training\n",
      "current loss: 6.865926265716553\n",
      "training\n",
      "current loss: 5.033300399780273\n",
      "training\n",
      "current loss: 7.909848213195801\n",
      "training\n",
      "current loss: 5.028951644897461\n",
      "training\n",
      "current loss: 6.234306812286377\n",
      "training\n",
      "current loss: 5.9993391036987305\n",
      "training\n",
      "current loss: 5.033117294311523\n",
      "training\n",
      "current loss: 5.879528045654297\n",
      "training\n",
      "current loss: 5.413164138793945\n",
      "training\n",
      "current loss: 7.015738487243652\n",
      "training\n",
      "current loss: 7.626574993133545\n",
      "training\n",
      "current loss: 5.716653823852539\n",
      "training\n",
      "current loss: 6.025931358337402\n",
      "training\n",
      "current loss: 6.617895603179932\n",
      "training\n",
      "current loss: 5.039978504180908\n",
      "training\n",
      "current loss: 6.044900894165039\n",
      "training\n",
      "current loss: 6.173224925994873\n",
      "training\n",
      "current loss: 6.032218933105469\n",
      "training\n",
      "current loss: 6.1310296058654785\n",
      "training\n",
      "current loss: 7.009395599365234\n",
      "training\n",
      "current loss: 5.640194416046143\n",
      "training\n",
      "current loss: 6.643324851989746\n",
      "training\n",
      "current loss: 7.024203300476074\n",
      "training\n",
      "current loss: 7.002989768981934\n",
      "training\n",
      "current loss: 6.945283889770508\n",
      "training\n",
      "current loss: 6.057883262634277\n",
      "training\n",
      "current loss: 9.522523880004883\n",
      "training\n",
      "current loss: 6.8351521492004395\n",
      "training\n",
      "current loss: 5.023029804229736\n",
      "training\n",
      "current loss: 6.020859718322754\n",
      "training\n",
      "current loss: 6.876296520233154\n",
      "training\n",
      "current loss: 6.043660640716553\n",
      "training\n",
      "current loss: 6.791131019592285\n",
      "training\n",
      "current loss: 5.560553550720215\n",
      "training\n",
      "current loss: 6.068778038024902\n",
      "training\n",
      "current loss: 7.9375081062316895\n",
      "training\n",
      "current loss: 7.035135269165039\n",
      "training\n",
      "current loss: 5.069621562957764\n",
      "training\n",
      "current loss: 5.7906494140625\n",
      "training\n",
      "current loss: 5.972573280334473\n",
      "training\n",
      "current loss: 5.035496711730957\n",
      "training\n",
      "current loss: 6.855728626251221\n",
      "training\n",
      "current loss: 6.900475978851318\n",
      "training\n",
      "current loss: 7.574782371520996\n",
      "training\n",
      "current loss: 7.010039806365967\n",
      "training\n",
      "current loss: 5.486012935638428\n",
      "training\n",
      "current loss: 7.018842697143555\n",
      "training\n",
      "current loss: 6.628105640411377\n",
      "training\n",
      "current loss: 6.023025989532471\n",
      "training\n",
      "current loss: 5.973309516906738\n",
      "training\n",
      "current loss: 7.988169193267822\n",
      "training\n",
      "current loss: 7.799039840698242\n",
      "training\n",
      "current loss: 6.026802062988281\n",
      "training\n",
      "current loss: 6.710197925567627\n",
      "training\n",
      "current loss: 7.477345943450928\n",
      "training\n",
      "current loss: 7.3671064376831055\n",
      "training\n",
      "current loss: 5.380562782287598\n",
      "training\n",
      "current loss: 8.794695854187012\n",
      "training\n",
      "current loss: 6.512989044189453\n",
      "training\n",
      "current loss: 5.601191520690918\n",
      "training\n",
      "current loss: 5.523824691772461\n",
      "training\n",
      "current loss: 6.079655647277832\n",
      "training\n",
      "current loss: 9.355812072753906\n",
      "training\n",
      "current loss: 7.001774787902832\n",
      "training\n",
      "current loss: 6.9631876945495605\n",
      "training\n",
      "current loss: 5.9716796875\n",
      "training\n",
      "current loss: 5.01356840133667\n",
      "training\n",
      "current loss: 7.906386852264404\n",
      "training\n",
      "current loss: 6.052271842956543\n",
      "training\n",
      "current loss: 6.973435878753662\n",
      "training\n",
      "current loss: 6.422558307647705\n",
      "training\n",
      "current loss: 5.157657623291016\n",
      "training\n",
      "current loss: 7.553516387939453\n",
      "training\n",
      "current loss: 8.556055068969727\n",
      "training\n",
      "current loss: 5.0152907371521\n",
      "training\n",
      "current loss: 10.638691902160645\n",
      "training\n",
      "current loss: 6.026314735412598\n",
      "training\n",
      "current loss: 7.398532867431641\n",
      "training\n",
      "current loss: 8.039493560791016\n",
      "training\n",
      "current loss: 6.1179656982421875\n",
      "training\n",
      "current loss: 6.092426300048828\n",
      "training\n",
      "current loss: 5.062187194824219\n",
      "training\n",
      "current loss: 8.57466983795166\n",
      "training\n",
      "current loss: 5.723027229309082\n",
      "training\n",
      "current loss: 5.0325517654418945\n",
      "training\n",
      "current loss: 7.044121265411377\n",
      "training\n",
      "current loss: 6.049919128417969\n",
      "training\n",
      "current loss: 6.190717697143555\n",
      "training\n",
      "current loss: 5.025813579559326\n",
      "training\n",
      "current loss: 6.645520210266113\n",
      "training\n",
      "current loss: 7.3264055252075195\n",
      "training\n",
      "current loss: 6.54920768737793\n",
      "training\n",
      "current loss: 8.97945785522461\n",
      "training\n",
      "current loss: 6.843788146972656\n",
      "training\n",
      "current loss: 7.508546352386475\n",
      "training\n",
      "current loss: 5.013851165771484\n",
      "training\n",
      "current loss: 6.830134391784668\n",
      "training\n",
      "current loss: 5.298646926879883\n",
      "training\n",
      "current loss: 6.001191139221191\n",
      "training\n",
      "current loss: 6.198799133300781\n",
      "training\n",
      "current loss: 8.391032218933105\n",
      "training\n",
      "current loss: 6.025344371795654\n",
      "training\n",
      "current loss: 8.56665325164795\n",
      "training\n",
      "current loss: 7.124553203582764\n",
      "training\n",
      "current loss: 6.031737804412842\n",
      "training\n",
      "current loss: 5.570055961608887\n",
      "training\n",
      "current loss: 5.327728748321533\n",
      "training\n",
      "current loss: 8.201528549194336\n",
      "training\n",
      "current loss: 5.079602241516113\n",
      "training\n",
      "current loss: 6.452596187591553\n",
      "training\n",
      "current loss: 5.989124774932861\n",
      "training\n",
      "current loss: 6.051854610443115\n",
      "training\n",
      "current loss: 6.487250328063965\n",
      "training\n",
      "current loss: 5.0636982917785645\n",
      "training\n",
      "current loss: 6.051836013793945\n",
      "training\n",
      "current loss: 6.034460544586182\n",
      "training\n",
      "current loss: 5.014461517333984\n",
      "training\n",
      "current loss: 7.140308380126953\n",
      "training\n",
      "current loss: 7.233077049255371\n",
      "training\n",
      "current loss: 6.942757606506348\n",
      "training\n",
      "current loss: 6.017179012298584\n",
      "training\n",
      "current loss: 7.438999176025391\n",
      "training\n",
      "current loss: 7.177181243896484\n",
      "training\n",
      "current loss: 7.75054931640625\n",
      "training\n",
      "current loss: 6.022602081298828\n",
      "training\n",
      "current loss: 5.481098175048828\n",
      "training\n",
      "current loss: 6.302320957183838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "current loss: 6.304295063018799\n",
      "training\n",
      "current loss: 6.350862503051758\n",
      "training\n",
      "current loss: 6.230442047119141\n",
      "training\n",
      "current loss: 5.299745559692383\n",
      "training\n",
      "current loss: 5.015066146850586\n",
      "training\n",
      "current loss: 6.0950927734375\n",
      "training\n",
      "current loss: 5.049982070922852\n",
      "training\n",
      "current loss: 6.256412506103516\n",
      "training\n",
      "current loss: 6.020949840545654\n",
      "training\n",
      "current loss: 6.05374813079834\n",
      "training\n",
      "current loss: 5.654176712036133\n",
      "training\n",
      "current loss: 8.06679630279541\n",
      "training\n",
      "current loss: 8.366926193237305\n",
      "training\n",
      "current loss: 6.502330780029297\n",
      "training\n",
      "current loss: 8.024633407592773\n",
      "training\n",
      "current loss: 6.255819320678711\n",
      "training\n",
      "current loss: 6.838858127593994\n",
      "training\n",
      "current loss: 6.6610846519470215\n",
      "training\n",
      "current loss: 9.285467147827148\n",
      "training\n",
      "current loss: 6.0224504470825195\n",
      "training\n",
      "current loss: 10.230299949645996\n",
      "training\n",
      "current loss: 9.713037490844727\n",
      "training\n",
      "current loss: 7.0232720375061035\n",
      "training\n",
      "current loss: 7.402527332305908\n",
      "training\n",
      "current loss: 5.047630786895752\n",
      "training\n",
      "current loss: 7.300140857696533\n",
      "training\n",
      "current loss: 8.173307418823242\n",
      "training\n",
      "current loss: 8.749890327453613\n",
      "training\n",
      "current loss: 6.811662197113037\n",
      "training\n",
      "current loss: 5.138607978820801\n",
      "training\n",
      "current loss: 7.922608375549316\n",
      "training\n",
      "current loss: 6.470546245574951\n",
      "training\n",
      "current loss: 7.031040191650391\n",
      "training\n",
      "current loss: 9.854427337646484\n",
      "training\n",
      "current loss: 5.619873523712158\n",
      "training\n",
      "current loss: 5.653688430786133\n",
      "training\n",
      "current loss: 6.152450084686279\n",
      "training\n",
      "current loss: 6.112752914428711\n",
      "training\n",
      "current loss: 7.300965309143066\n",
      "training\n",
      "current loss: 7.050037384033203\n",
      "training\n",
      "current loss: 6.1917724609375\n",
      "training\n",
      "current loss: 6.779600620269775\n",
      "training\n",
      "current loss: 7.927972316741943\n",
      "training\n",
      "current loss: 6.023180961608887\n",
      "training\n",
      "current loss: 8.806370735168457\n",
      "training\n",
      "current loss: 6.953211784362793\n",
      "training\n",
      "current loss: 9.18317699432373\n",
      "training\n",
      "current loss: 7.007125377655029\n",
      "training\n",
      "current loss: 6.0358405113220215\n",
      "training\n",
      "current loss: 6.011015892028809\n",
      "training\n",
      "current loss: 8.44751262664795\n",
      "training\n",
      "current loss: 7.014416217803955\n",
      "training\n",
      "current loss: 7.988188743591309\n",
      "training\n",
      "current loss: 6.018675327301025\n",
      "training\n",
      "current loss: 8.55687141418457\n",
      "training\n",
      "current loss: 8.025710105895996\n",
      "training\n",
      "current loss: 8.022317886352539\n",
      "training\n",
      "current loss: 6.7564239501953125\n",
      "training\n",
      "current loss: 7.0262908935546875\n",
      "training\n",
      "current loss: 7.0187668800354\n",
      "training\n",
      "current loss: 7.190282344818115\n",
      "training\n",
      "current loss: 6.931486129760742\n",
      "training\n",
      "current loss: 7.6637420654296875\n",
      "training\n",
      "current loss: 5.02474308013916\n",
      "training\n",
      "current loss: 9.07585334777832\n",
      "training\n",
      "current loss: 7.455475807189941\n",
      "training\n",
      "current loss: 7.045876502990723\n",
      "training\n",
      "current loss: 6.4183173179626465\n",
      "training\n",
      "current loss: 5.993613243103027\n",
      "training\n",
      "current loss: 8.737934112548828\n",
      "training\n",
      "current loss: 6.898398399353027\n",
      "training\n",
      "current loss: 6.162371635437012\n",
      "training\n",
      "current loss: 7.237612724304199\n",
      "training\n",
      "current loss: 7.034526348114014\n",
      "training\n",
      "current loss: 5.0303544998168945\n",
      "training\n",
      "current loss: 5.438024044036865\n",
      "training\n",
      "current loss: 7.234297275543213\n",
      "training\n",
      "current loss: 5.714672088623047\n",
      "training\n",
      "current loss: 6.053348064422607\n",
      "training\n",
      "current loss: 7.577857494354248\n",
      "training\n",
      "current loss: 7.043622970581055\n",
      "training\n",
      "current loss: 8.498549461364746\n",
      "training\n",
      "current loss: 7.961689472198486\n",
      "training\n",
      "current loss: 8.321743965148926\n",
      "training\n",
      "current loss: 5.938009262084961\n",
      "training\n",
      "current loss: 7.290103912353516\n",
      "training\n",
      "current loss: 6.018314838409424\n",
      "training\n",
      "current loss: 7.011876583099365\n",
      "training\n",
      "current loss: 6.011661529541016\n",
      "training\n",
      "current loss: 7.505478858947754\n",
      "training\n",
      "current loss: 5.980677127838135\n",
      "training\n",
      "current loss: 7.365382194519043\n",
      "training\n",
      "current loss: 9.099802017211914\n",
      "training\n",
      "current loss: 6.7520833015441895\n",
      "training\n",
      "current loss: 7.0697174072265625\n",
      "training\n",
      "current loss: 8.219221115112305\n",
      "training\n",
      "current loss: 6.136237144470215\n",
      "training\n",
      "current loss: 6.952008247375488\n",
      "training\n",
      "current loss: 8.368646621704102\n",
      "training\n",
      "current loss: 5.474654674530029\n",
      "training\n",
      "current loss: 7.590971946716309\n",
      "training\n",
      "current loss: 6.323676586151123\n",
      "training\n",
      "current loss: 8.160737991333008\n",
      "training\n",
      "current loss: 7.74727725982666\n",
      "training\n",
      "current loss: 6.28093147277832\n",
      "training\n",
      "current loss: 6.3039021492004395\n",
      "training\n",
      "current loss: 5.390624523162842\n",
      "training\n",
      "current loss: 7.501466274261475\n",
      "training\n",
      "current loss: 6.9825520515441895\n",
      "training\n",
      "current loss: 6.993429660797119\n",
      "training\n",
      "current loss: 6.990001201629639\n",
      "training\n",
      "current loss: 7.008692264556885\n",
      "training\n",
      "current loss: 6.446494102478027\n",
      "training\n",
      "current loss: 7.323181629180908\n",
      "training\n",
      "current loss: 7.01838493347168\n",
      "training\n",
      "current loss: 7.712469577789307\n",
      "training\n",
      "current loss: 6.14546537399292\n",
      "training\n",
      "current loss: 9.70088005065918\n",
      "training\n",
      "current loss: 7.098474025726318\n",
      "training\n",
      "current loss: 5.016346454620361\n",
      "training\n",
      "current loss: 7.13918924331665\n",
      "training\n",
      "current loss: 8.011713027954102\n",
      "training\n",
      "current loss: 6.585824489593506\n",
      "training\n",
      "current loss: 5.054128646850586\n",
      "training\n",
      "current loss: 6.004227638244629\n",
      "training\n",
      "current loss: 5.600469589233398\n",
      "training\n",
      "current loss: 6.011961936950684\n",
      "training\n",
      "current loss: 6.311432361602783\n",
      "training\n",
      "current loss: 6.0147223472595215\n",
      "training\n",
      "current loss: 7.390107154846191\n",
      "training\n",
      "current loss: 5.015798568725586\n",
      "training\n",
      "current loss: 6.715902328491211\n",
      "training\n",
      "current loss: 5.11477518081665\n",
      "training\n",
      "current loss: 9.114604949951172\n",
      "training\n",
      "current loss: 5.194467544555664\n",
      "training\n",
      "current loss: 8.039013862609863\n",
      "training\n",
      "current loss: 8.976487159729004\n",
      "training\n",
      "current loss: 5.0662760734558105\n",
      "training\n",
      "current loss: 6.492519378662109\n",
      "training\n",
      "current loss: 6.085146427154541\n",
      "training\n",
      "current loss: 7.815040588378906\n",
      "training\n",
      "current loss: 6.02119779586792\n",
      "training\n",
      "current loss: 6.783921718597412\n",
      "training\n",
      "current loss: 7.392777442932129\n",
      "training\n",
      "current loss: 6.015103340148926\n",
      "training\n",
      "current loss: 6.016618728637695\n",
      "training\n",
      "current loss: 6.032053470611572\n",
      "training\n",
      "current loss: 7.745789051055908\n",
      "training\n",
      "current loss: 8.897500991821289\n",
      "training\n",
      "current loss: 6.008027076721191\n",
      "training\n",
      "current loss: 9.617572784423828\n",
      "training\n",
      "current loss: 7.024919033050537\n",
      "training\n",
      "current loss: 5.965679168701172\n",
      "training\n",
      "current loss: 6.617960453033447\n",
      "training\n",
      "current loss: 8.073739051818848\n",
      "training\n",
      "current loss: 5.01411247253418\n",
      "training\n",
      "current loss: 8.320406913757324\n",
      "training\n",
      "current loss: 5.164587497711182\n",
      "training\n",
      "current loss: 6.038290023803711\n",
      "training\n",
      "current loss: 7.3236541748046875\n",
      "training\n",
      "current loss: 6.926563262939453\n",
      "training\n",
      "current loss: 8.461577415466309\n",
      "training\n",
      "current loss: 7.646097660064697\n",
      "training\n",
      "current loss: 7.042670249938965\n",
      "training\n",
      "current loss: 6.200802326202393\n",
      "training\n",
      "current loss: 8.019023895263672\n",
      "training\n",
      "current loss: 7.698897361755371\n",
      "training\n",
      "current loss: 6.70386266708374\n",
      "training\n",
      "current loss: 7.562734127044678\n",
      "training\n",
      "current loss: 7.317394256591797\n",
      "training\n",
      "current loss: 6.0216827392578125\n",
      "training\n",
      "current loss: 6.249697685241699\n",
      "training\n",
      "current loss: 5.983175277709961\n",
      "training\n",
      "current loss: 5.70513916015625\n",
      "training\n",
      "current loss: 9.428499221801758\n",
      "training\n",
      "current loss: 5.063681125640869\n",
      "training\n",
      "current loss: 8.468344688415527\n",
      "training\n",
      "current loss: 5.0400614738464355\n",
      "training\n",
      "current loss: 5.973773002624512\n",
      "training\n",
      "current loss: 6.474956035614014\n",
      "training\n",
      "current loss: 9.073704719543457\n",
      "training\n",
      "current loss: 7.0624680519104\n",
      "training\n",
      "current loss: 5.2796525955200195\n",
      "training\n",
      "current loss: 6.580448150634766\n",
      "training\n",
      "current loss: 6.420506954193115\n",
      "training\n",
      "current loss: 7.744057655334473\n",
      "training\n",
      "current loss: 7.934265613555908\n",
      "training\n",
      "current loss: 6.237246036529541\n",
      "training\n",
      "current loss: 5.202203273773193\n",
      "training\n",
      "current loss: 6.302595615386963\n",
      "training\n",
      "current loss: 7.0368733406066895\n",
      "training\n",
      "current loss: 8.015104293823242\n",
      "training\n",
      "current loss: 7.939575672149658\n",
      "training\n",
      "current loss: 6.116765022277832\n",
      "training\n",
      "current loss: 6.858714580535889\n",
      "training\n",
      "current loss: 5.313835144042969\n",
      "training\n",
      "current loss: 10.412677764892578\n",
      "training\n",
      "current loss: 7.132366180419922\n",
      "training\n",
      "current loss: 7.996583938598633\n",
      "training\n",
      "current loss: 6.257498741149902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "current loss: 8.016578674316406\n",
      "training\n",
      "current loss: 9.493766784667969\n",
      "training\n",
      "current loss: 9.773865699768066\n",
      "training\n",
      "current loss: 6.02055549621582\n",
      "training\n",
      "current loss: 7.102031707763672\n",
      "training\n",
      "current loss: 6.067966938018799\n",
      "training\n",
      "current loss: 5.825465202331543\n",
      "training\n",
      "current loss: 9.135316848754883\n",
      "training\n",
      "current loss: 7.54616641998291\n",
      "training\n",
      "current loss: 6.826194763183594\n",
      "training\n",
      "current loss: 6.463395118713379\n",
      "training\n",
      "current loss: 7.576942443847656\n",
      "training\n",
      "current loss: 6.655559539794922\n",
      "training\n",
      "current loss: 6.1859893798828125\n",
      "training\n",
      "current loss: 6.04295539855957\n",
      "training\n",
      "current loss: 5.886505603790283\n",
      "training\n",
      "current loss: 6.235899448394775\n",
      "training\n",
      "current loss: 6.850043296813965\n",
      "training\n",
      "current loss: 5.33441686630249\n",
      "training\n",
      "current loss: 6.3492913246154785\n",
      "training\n",
      "current loss: 6.2727837562561035\n",
      "training\n",
      "current loss: 7.675508975982666\n",
      "training\n",
      "current loss: 5.065889835357666\n",
      "training\n",
      "current loss: 7.931989669799805\n",
      "training\n",
      "current loss: 7.124360084533691\n",
      "training\n",
      "current loss: 8.702656745910645\n",
      "training\n",
      "current loss: 7.434861183166504\n",
      "training\n",
      "current loss: 6.665950298309326\n",
      "training\n",
      "current loss: 6.97155237197876\n",
      "training\n",
      "current loss: 6.026487827301025\n",
      "training\n",
      "current loss: 8.24199104309082\n",
      "training\n",
      "current loss: 6.727355003356934\n",
      "training\n",
      "current loss: 5.018803596496582\n",
      "training\n",
      "current loss: 7.508630275726318\n",
      "training\n",
      "current loss: 6.661036491394043\n",
      "training\n",
      "current loss: 6.59554386138916\n",
      "training\n",
      "current loss: 5.989869594573975\n",
      "training\n",
      "current loss: 6.084359169006348\n",
      "training\n",
      "current loss: 5.4126057624816895\n",
      "training\n",
      "current loss: 6.393867015838623\n",
      "training\n",
      "current loss: 8.31675910949707\n",
      "training\n",
      "current loss: 6.075769424438477\n",
      "training\n",
      "current loss: 10.734801292419434\n",
      "training\n",
      "current loss: 6.349941253662109\n",
      "training\n",
      "current loss: 6.043309688568115\n",
      "training\n",
      "current loss: 6.2840166091918945\n",
      "training\n",
      "current loss: 7.022516250610352\n",
      "training\n",
      "current loss: 6.326006889343262\n",
      "training\n",
      "current loss: 6.782949447631836\n",
      "training\n",
      "current loss: 6.440074920654297\n",
      "training\n",
      "current loss: 7.166191577911377\n",
      "training\n",
      "current loss: 6.955955505371094\n",
      "training\n",
      "current loss: 9.797957420349121\n",
      "training\n",
      "current loss: 6.647424221038818\n",
      "training\n",
      "current loss: 6.090604782104492\n",
      "training\n",
      "current loss: 6.387747287750244\n",
      "training\n",
      "current loss: 9.209897994995117\n",
      "training\n",
      "current loss: 7.2932281494140625\n",
      "training\n",
      "current loss: 5.236319065093994\n",
      "training\n",
      "current loss: 7.888844013214111\n",
      "training\n",
      "current loss: 5.265445709228516\n",
      "training\n",
      "current loss: 6.910447597503662\n",
      "training\n",
      "current loss: 5.100556373596191\n",
      "training\n",
      "current loss: 7.540254592895508\n",
      "training\n",
      "current loss: 7.078045845031738\n",
      "training\n",
      "current loss: 6.065858364105225\n",
      "training\n",
      "current loss: 8.238463401794434\n",
      "training\n",
      "current loss: 7.481199264526367\n",
      "training\n",
      "current loss: 5.059887409210205\n",
      "training\n",
      "current loss: 6.229672908782959\n",
      "training\n",
      "current loss: 6.440504550933838\n",
      "training\n",
      "current loss: 7.938872814178467\n",
      "training\n",
      "current loss: 6.805174827575684\n",
      "training\n",
      "current loss: 5.137706279754639\n",
      "training\n",
      "current loss: 6.066126823425293\n",
      "training\n",
      "current loss: 6.62315034866333\n",
      "training\n",
      "current loss: 6.997202396392822\n",
      "training\n",
      "current loss: 5.4191741943359375\n",
      "training\n",
      "current loss: 7.959265232086182\n",
      "training\n",
      "current loss: 8.648735046386719\n",
      "training\n",
      "current loss: 6.993901252746582\n",
      "training\n",
      "current loss: 5.098051071166992\n",
      "training\n",
      "current loss: 8.668689727783203\n",
      "training\n",
      "current loss: 6.032400131225586\n",
      "training\n",
      "current loss: 7.472925186157227\n",
      "training\n",
      "current loss: 5.596307754516602\n",
      "training\n",
      "current loss: 5.015033721923828\n",
      "training\n",
      "current loss: 7.749156475067139\n",
      "training\n",
      "current loss: 6.0731587409973145\n",
      "training\n",
      "current loss: 8.094087600708008\n",
      "training\n",
      "current loss: 7.9892754554748535\n",
      "training\n",
      "current loss: 6.025694847106934\n",
      "training\n",
      "current loss: 6.488381385803223\n",
      "training\n",
      "current loss: 6.7442169189453125\n",
      "training\n",
      "current loss: 6.474420547485352\n",
      "training\n",
      "current loss: 7.699565410614014\n",
      "training\n",
      "current loss: 6.889545440673828\n",
      "training\n",
      "current loss: 5.219601154327393\n",
      "training\n",
      "current loss: 6.550196170806885\n",
      "training\n",
      "current loss: 7.403436660766602\n",
      "training\n",
      "current loss: 6.02744722366333\n",
      "training\n",
      "current loss: 6.624110221862793\n",
      "training\n",
      "current loss: 5.039007186889648\n",
      "training\n",
      "current loss: 5.202390670776367\n",
      "training\n",
      "current loss: 5.8594794273376465\n",
      "training\n",
      "current loss: 5.966214656829834\n",
      "training\n",
      "current loss: 6.014065742492676\n",
      "training\n",
      "current loss: 6.965121746063232\n",
      "training\n",
      "current loss: 8.861282348632812\n",
      "training\n",
      "current loss: 7.498347282409668\n",
      "training\n",
      "current loss: 5.131134033203125\n",
      "training\n",
      "current loss: 5.92043399810791\n",
      "training\n",
      "current loss: 8.9171781539917\n",
      "training\n",
      "current loss: 6.1293816566467285\n",
      "training\n",
      "current loss: 8.546415328979492\n",
      "training\n",
      "current loss: 7.230928421020508\n",
      "training\n",
      "current loss: 6.015743732452393\n",
      "training\n",
      "current loss: 5.444673538208008\n",
      "training\n",
      "current loss: 8.472760200500488\n",
      "training\n",
      "current loss: 7.761647701263428\n",
      "training\n",
      "current loss: 5.929225444793701\n",
      "training\n",
      "current loss: 5.017978668212891\n",
      "training\n",
      "current loss: 9.907509803771973\n",
      "training\n",
      "current loss: 6.4631500244140625\n",
      "training\n",
      "current loss: 5.036730766296387\n",
      "training\n",
      "current loss: 9.531996726989746\n",
      "training\n",
      "current loss: 7.138063430786133\n",
      "training\n",
      "current loss: 5.303205966949463\n",
      "training\n",
      "current loss: 6.503556251525879\n",
      "training\n",
      "current loss: 6.128659725189209\n",
      "training\n",
      "current loss: 5.030153274536133\n",
      "training\n",
      "current loss: 8.0413818359375\n",
      "training\n",
      "current loss: 7.639663219451904\n",
      "training\n",
      "current loss: 5.234933853149414\n",
      "training\n",
      "current loss: 7.129507541656494\n",
      "training\n",
      "current loss: 6.41196346282959\n",
      "training\n",
      "current loss: 5.485570430755615\n",
      "training\n",
      "current loss: 5.014774322509766\n",
      "training\n",
      "current loss: 6.01491117477417\n",
      "training\n",
      "current loss: 6.942105293273926\n",
      "training\n",
      "current loss: 5.749330997467041\n",
      "training\n",
      "current loss: 6.85795783996582\n",
      "training\n",
      "current loss: 5.594327926635742\n",
      "training\n",
      "current loss: 6.138027667999268\n",
      "training\n",
      "current loss: 6.172327995300293\n",
      "training\n",
      "current loss: 7.830965995788574\n",
      "training\n",
      "current loss: 5.019017696380615\n",
      "training\n",
      "current loss: 7.784950256347656\n",
      "training\n",
      "current loss: 5.279781818389893\n",
      "training\n",
      "current loss: 6.561827659606934\n",
      "training\n",
      "current loss: 5.033212661743164\n",
      "training\n",
      "current loss: 7.038885593414307\n",
      "training\n",
      "current loss: 5.778433799743652\n",
      "training\n",
      "current loss: 6.685445785522461\n",
      "training\n",
      "current loss: 7.4994988441467285\n",
      "training\n",
      "current loss: 7.636563301086426\n",
      "training\n",
      "current loss: 6.140020370483398\n",
      "training\n",
      "current loss: 6.23253059387207\n",
      "training\n",
      "current loss: 6.7870073318481445\n",
      "training\n",
      "current loss: 7.4122633934021\n",
      "training\n",
      "current loss: 6.629301071166992\n",
      "training\n",
      "current loss: 7.606037616729736\n",
      "training\n",
      "current loss: 7.443507194519043\n",
      "training\n",
      "current loss: 6.044469833374023\n",
      "training\n",
      "current loss: 7.935631275177002\n",
      "training\n",
      "current loss: 5.100591659545898\n",
      "training\n",
      "current loss: 6.504254341125488\n",
      "training\n",
      "current loss: 7.9964470863342285\n",
      "training\n",
      "current loss: 5.895712375640869\n",
      "training\n",
      "current loss: 8.106928825378418\n",
      "training\n",
      "current loss: 5.097521781921387\n",
      "training\n",
      "current loss: 6.944149971008301\n",
      "training\n",
      "current loss: 6.069759845733643\n",
      "training\n",
      "current loss: 5.980329513549805\n",
      "training\n",
      "current loss: 7.007309913635254\n",
      "training\n",
      "current loss: 7.018441677093506\n",
      "training\n",
      "current loss: 5.353038787841797\n",
      "training\n",
      "current loss: 6.655313491821289\n",
      "training\n",
      "current loss: 5.903112411499023\n",
      "training\n",
      "current loss: 6.085664749145508\n",
      "training\n",
      "current loss: 7.156698226928711\n",
      "training\n",
      "current loss: 6.02465295791626\n",
      "training\n",
      "current loss: 7.943824291229248\n",
      "training\n",
      "current loss: 5.0176520347595215\n",
      "training\n",
      "current loss: 7.020459175109863\n",
      "training\n",
      "current loss: 7.016381740570068\n",
      "training\n",
      "current loss: 6.056669235229492\n",
      "training\n",
      "current loss: 6.5099406242370605\n",
      "training\n",
      "current loss: 6.039636135101318\n",
      "training\n",
      "current loss: 8.839905738830566\n",
      "training\n",
      "current loss: 8.083263397216797\n",
      "training\n",
      "current loss: 5.842289447784424\n",
      "training\n",
      "current loss: 5.054819107055664\n",
      "training\n",
      "current loss: 6.234946250915527\n",
      "training\n",
      "current loss: 6.018397331237793\n",
      "training\n",
      "current loss: 6.485924243927002\n",
      "training\n",
      "current loss: 8.088970184326172\n",
      "training\n",
      "current loss: 7.264728546142578\n",
      "training\n",
      "current loss: 7.942455768585205\n",
      "training\n",
      "current loss: 7.48883056640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "current loss: 5.198891639709473\n",
      "training\n",
      "current loss: 5.100594997406006\n",
      "training\n",
      "current loss: 7.293290138244629\n",
      "training\n",
      "current loss: 6.222903728485107\n",
      "training\n",
      "current loss: 5.0182271003723145\n",
      "training\n",
      "current loss: 7.191897869110107\n",
      "training\n",
      "current loss: 7.027085304260254\n",
      "training\n",
      "current loss: 8.06906509399414\n",
      "training\n",
      "current loss: 5.02786922454834\n",
      "training\n",
      "current loss: 7.058807373046875\n",
      "training\n",
      "current loss: 8.010089874267578\n",
      "training\n",
      "current loss: 5.328155040740967\n",
      "training\n",
      "current loss: 5.384399890899658\n",
      "training\n",
      "current loss: 6.117415428161621\n",
      "training\n",
      "current loss: 8.058947563171387\n",
      "training\n",
      "current loss: 7.745092391967773\n",
      "training\n",
      "current loss: 6.248088836669922\n",
      "training\n",
      "current loss: 6.020420551300049\n",
      "training\n",
      "current loss: 5.0232930183410645\n",
      "training\n",
      "current loss: 7.01708459854126\n",
      "training\n",
      "current loss: 6.014987945556641\n",
      "training\n",
      "current loss: 6.3214335441589355\n",
      "training\n",
      "current loss: 8.405779838562012\n",
      "training\n",
      "current loss: 5.014734745025635\n",
      "training\n",
      "current loss: 7.003001689910889\n",
      "training\n",
      "current loss: 6.091188907623291\n",
      "training\n",
      "current loss: 6.949813365936279\n",
      "training\n",
      "current loss: 6.293259620666504\n",
      "training\n",
      "current loss: 7.100552558898926\n",
      "training\n",
      "current loss: 6.491018295288086\n",
      "training\n",
      "current loss: 5.013713836669922\n",
      "training\n",
      "current loss: 7.011745452880859\n",
      "training\n",
      "current loss: 8.005844116210938\n",
      "training\n",
      "current loss: 6.097486972808838\n",
      "training\n",
      "current loss: 6.604342937469482\n",
      "training\n",
      "current loss: 7.0263872146606445\n",
      "training\n",
      "current loss: 6.761590480804443\n",
      "training\n",
      "current loss: 6.465972900390625\n",
      "training\n",
      "current loss: 7.298759460449219\n",
      "training\n",
      "current loss: 7.017117500305176\n",
      "training\n",
      "current loss: 10.126943588256836\n",
      "training\n",
      "current loss: 5.788924217224121\n",
      "training\n",
      "current loss: 7.966968536376953\n",
      "training\n",
      "current loss: 7.989743709564209\n",
      "training\n",
      "current loss: 6.745364189147949\n",
      "training\n",
      "current loss: 5.986664295196533\n",
      "training\n",
      "current loss: 6.79996919631958\n",
      "training\n",
      "current loss: 5.013065814971924\n",
      "training\n",
      "current loss: 7.997328281402588\n",
      "training\n",
      "current loss: 6.030858039855957\n",
      "training\n",
      "current loss: 6.049652576446533\n",
      "training\n",
      "current loss: 6.068077087402344\n",
      "training\n",
      "current loss: 6.16422700881958\n",
      "training\n",
      "current loss: 7.504209995269775\n",
      "training\n",
      "current loss: 7.273639678955078\n",
      "training\n",
      "current loss: 6.095089912414551\n",
      "training\n",
      "current loss: 6.012351036071777\n",
      "training\n",
      "current loss: 6.473374366760254\n",
      "training\n",
      "current loss: 7.01736307144165\n",
      "training\n",
      "current loss: 6.0143303871154785\n",
      "training\n",
      "current loss: 6.645575523376465\n",
      "training\n",
      "current loss: 5.0646281242370605\n",
      "training\n",
      "current loss: 8.396371841430664\n",
      "training\n",
      "current loss: 7.590873718261719\n",
      "training\n",
      "current loss: 5.024301528930664\n",
      "training\n",
      "current loss: 5.843175888061523\n",
      "training\n",
      "current loss: 7.399807929992676\n",
      "training\n",
      "current loss: 6.963932037353516\n",
      "training\n",
      "current loss: 7.478390693664551\n",
      "training\n",
      "current loss: 7.183904647827148\n",
      "training\n",
      "current loss: 6.047973155975342\n",
      "training\n",
      "current loss: 10.039148330688477\n",
      "training\n",
      "current loss: 7.011861801147461\n",
      "training\n",
      "current loss: 5.039144515991211\n",
      "training\n",
      "current loss: 6.898376941680908\n",
      "training\n",
      "current loss: 8.297277450561523\n",
      "training\n",
      "current loss: 6.682732105255127\n",
      "training\n",
      "current loss: 7.03857421875\n",
      "training\n",
      "current loss: 8.11868953704834\n",
      "training\n",
      "current loss: 7.011531829833984\n",
      "training\n",
      "current loss: 5.015215873718262\n",
      "training\n",
      "current loss: 6.479889392852783\n",
      "training\n",
      "current loss: 6.540564060211182\n",
      "training\n",
      "current loss: 7.000874042510986\n",
      "training\n",
      "current loss: 7.189806938171387\n",
      "training\n",
      "current loss: 8.012022972106934\n",
      "training\n",
      "current loss: 7.294772624969482\n",
      "training\n",
      "current loss: 7.009759426116943\n",
      "training\n",
      "current loss: 6.029823303222656\n",
      "training\n",
      "current loss: 6.438530921936035\n",
      "training\n",
      "current loss: 5.67481803894043\n",
      "training\n",
      "current loss: 5.414502143859863\n",
      "training\n",
      "current loss: 6.159171104431152\n",
      "training\n",
      "current loss: 6.900005340576172\n",
      "training\n",
      "current loss: 7.003966808319092\n",
      "training\n",
      "current loss: 6.9284539222717285\n",
      "training\n",
      "current loss: 8.821025848388672\n",
      "training\n",
      "current loss: 5.748569488525391\n",
      "training\n",
      "current loss: 6.013560771942139\n",
      "training\n",
      "current loss: 6.015678405761719\n",
      "training\n",
      "current loss: 6.580746650695801\n",
      "training\n",
      "current loss: 7.609543323516846\n",
      "training\n",
      "current loss: 7.217138767242432\n",
      "training\n",
      "current loss: 7.197018146514893\n",
      "training\n",
      "current loss: 5.609260559082031\n",
      "training\n",
      "current loss: 9.06234359741211\n",
      "training\n",
      "current loss: 5.895177364349365\n",
      "training\n",
      "current loss: 7.8970723152160645\n",
      "training\n",
      "current loss: 6.745785236358643\n",
      "training\n",
      "current loss: 5.055117130279541\n",
      "training\n",
      "current loss: 8.024864196777344\n",
      "training\n",
      "current loss: 7.28845739364624\n",
      "training\n",
      "current loss: 5.489477634429932\n",
      "training\n",
      "current loss: 7.00839900970459\n",
      "training\n",
      "current loss: 8.140974998474121\n",
      "training\n",
      "current loss: 6.013558387756348\n",
      "training\n",
      "current loss: 7.635292053222656\n",
      "training\n",
      "current loss: 5.079526901245117\n",
      "training\n",
      "current loss: 5.999429225921631\n",
      "training\n",
      "current loss: 7.812003135681152\n",
      "training\n",
      "current loss: 6.412008762359619\n",
      "training\n",
      "current loss: 6.356469631195068\n",
      "training\n",
      "current loss: 9.002845764160156\n",
      "training\n",
      "current loss: 6.603338718414307\n",
      "training\n",
      "current loss: 5.95109224319458\n",
      "training\n",
      "current loss: 9.140475273132324\n",
      "training\n",
      "current loss: 5.99534273147583\n",
      "training\n",
      "current loss: 5.949972629547119\n",
      "training\n",
      "current loss: 5.970992088317871\n",
      "training\n",
      "current loss: 7.910645008087158\n",
      "training\n",
      "current loss: 8.012285232543945\n",
      "training\n",
      "current loss: 8.848499298095703\n",
      "training\n",
      "current loss: 6.383752822875977\n",
      "training\n",
      "current loss: 5.449437141418457\n",
      "training\n",
      "current loss: 7.432420253753662\n",
      "training\n",
      "current loss: 8.41273307800293\n",
      "training\n",
      "current loss: 6.753480911254883\n",
      "training\n",
      "current loss: 7.684484004974365\n",
      "training\n",
      "current loss: 6.017851829528809\n",
      "training\n",
      "current loss: 6.013885021209717\n",
      "training\n",
      "current loss: 8.788132667541504\n",
      "training\n",
      "current loss: 5.015805244445801\n",
      "training\n",
      "current loss: 6.071105480194092\n",
      "training\n",
      "current loss: 6.968258380889893\n",
      "training\n",
      "current loss: 6.1412153244018555\n",
      "training\n",
      "current loss: 7.5041069984436035\n",
      "training\n",
      "current loss: 6.011966705322266\n",
      "training\n",
      "current loss: 6.317766189575195\n",
      "training\n",
      "current loss: 7.495684623718262\n",
      "training\n",
      "current loss: 7.938050270080566\n",
      "training\n",
      "current loss: 6.855978488922119\n",
      "training\n",
      "current loss: 7.015453815460205\n",
      "training\n",
      "current loss: 7.133804798126221\n",
      "training\n",
      "current loss: 6.022459506988525\n",
      "training\n",
      "current loss: 5.937158584594727\n",
      "training\n",
      "current loss: 8.117837905883789\n",
      "training\n",
      "current loss: 6.627066135406494\n",
      "training\n",
      "current loss: 7.307136058807373\n",
      "training\n",
      "current loss: 7.499116897583008\n",
      "training\n",
      "current loss: 7.157502174377441\n",
      "training\n",
      "current loss: 5.029349327087402\n",
      "training\n",
      "current loss: 6.020591735839844\n",
      "training\n",
      "current loss: 6.839323043823242\n",
      "training\n",
      "current loss: 5.9551897048950195\n",
      "training\n",
      "current loss: 6.02360725402832\n",
      "training\n",
      "current loss: 5.072483539581299\n",
      "training\n",
      "current loss: 7.405555725097656\n",
      "training\n",
      "current loss: 6.586101531982422\n",
      "training\n",
      "current loss: 6.047987937927246\n",
      "training\n",
      "current loss: 5.7273054122924805\n",
      "training\n",
      "current loss: 6.319764614105225\n",
      "training\n",
      "current loss: 5.25905704498291\n",
      "training\n",
      "current loss: 7.642422676086426\n",
      "training\n",
      "current loss: 6.64693021774292\n",
      "training\n",
      "current loss: 7.093220233917236\n",
      "training\n",
      "current loss: 6.323972702026367\n",
      "training\n",
      "current loss: 5.259141445159912\n",
      "training\n",
      "current loss: 6.748149394989014\n",
      "training\n",
      "current loss: 8.308209419250488\n",
      "training\n",
      "current loss: 6.6810526847839355\n",
      "training\n",
      "current loss: 7.056166648864746\n",
      "training\n",
      "current loss: 8.088224411010742\n",
      "training\n",
      "current loss: 6.012938499450684\n",
      "training\n",
      "current loss: 6.025493621826172\n",
      "training\n",
      "current loss: 7.013100624084473\n",
      "training\n",
      "current loss: 6.187705993652344\n",
      "training\n",
      "current loss: 6.860051155090332\n",
      "training\n",
      "current loss: 6.01713752746582\n",
      "training\n",
      "current loss: 5.039029598236084\n",
      "training\n",
      "current loss: 7.028758525848389\n",
      "training\n",
      "current loss: 7.930002212524414\n",
      "training\n",
      "current loss: 6.886531829833984\n",
      "training\n",
      "current loss: 5.228850364685059\n",
      "training\n",
      "current loss: 8.162516593933105\n",
      "training\n",
      "current loss: 7.826216697692871\n",
      "training\n",
      "current loss: 8.34524917602539\n",
      "training\n",
      "current loss: 5.079501152038574\n",
      "training\n",
      "current loss: 7.987999439239502\n",
      "training\n",
      "current loss: 6.044970512390137\n",
      "training\n",
      "current loss: 6.073265075683594\n",
      "training\n",
      "current loss: 8.601322174072266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "current loss: 5.550053119659424\n",
      "training\n",
      "current loss: 7.9049530029296875\n",
      "training\n",
      "current loss: 7.193953037261963\n",
      "training\n",
      "current loss: 7.6959967613220215\n",
      "training\n",
      "current loss: 5.0179595947265625\n",
      "training\n",
      "current loss: 6.656864643096924\n",
      "training\n",
      "current loss: 5.018057823181152\n",
      "training\n",
      "current loss: 6.011343955993652\n",
      "training\n",
      "current loss: 7.009825706481934\n",
      "training\n",
      "current loss: 6.125548839569092\n",
      "training\n",
      "current loss: 6.584684371948242\n",
      "training\n",
      "current loss: 5.115257740020752\n",
      "training\n",
      "current loss: 8.788599014282227\n",
      "training\n",
      "current loss: 6.005918979644775\n",
      "training\n",
      "current loss: 6.214993953704834\n",
      "training\n",
      "current loss: 5.087972640991211\n",
      "training\n",
      "current loss: 6.673803329467773\n",
      "training\n",
      "current loss: 5.03446626663208\n",
      "training\n",
      "current loss: 7.40028715133667\n",
      "training\n",
      "current loss: 6.305365562438965\n",
      "training\n",
      "current loss: 7.5226545333862305\n",
      "training\n",
      "current loss: 5.241548538208008\n",
      "training\n",
      "current loss: 5.087096691131592\n",
      "training\n",
      "current loss: 7.261316299438477\n",
      "training\n",
      "current loss: 6.025557518005371\n",
      "training\n",
      "current loss: 6.599998474121094\n",
      "training\n",
      "current loss: 7.016596794128418\n",
      "training\n",
      "current loss: 8.1022310256958\n",
      "training\n",
      "current loss: 8.246476173400879\n",
      "training\n",
      "current loss: 6.915074825286865\n",
      "training\n",
      "current loss: 6.089045524597168\n",
      "training\n",
      "current loss: 7.042375564575195\n",
      "training\n",
      "current loss: 6.104536056518555\n",
      "training\n",
      "current loss: 9.168349266052246\n",
      "training\n",
      "current loss: 5.019477844238281\n",
      "training\n",
      "current loss: 5.325590133666992\n",
      "training\n",
      "current loss: 5.020707130432129\n",
      "training\n",
      "current loss: 6.882917404174805\n",
      "training\n",
      "current loss: 6.027466297149658\n",
      "training\n",
      "current loss: 8.092751502990723\n",
      "training\n",
      "current loss: 5.334145545959473\n",
      "training\n",
      "current loss: 5.290970802307129\n",
      "training\n",
      "current loss: 5.797240257263184\n",
      "training\n",
      "current loss: 9.558990478515625\n",
      "training\n",
      "current loss: 6.356785774230957\n",
      "training\n",
      "current loss: 5.138974189758301\n",
      "training\n",
      "current loss: 6.273740768432617\n",
      "training\n",
      "current loss: 6.059333324432373\n",
      "training\n",
      "current loss: 5.653223037719727\n",
      "training\n",
      "current loss: 7.330634117126465\n",
      "training\n",
      "current loss: 6.8227219581604\n",
      "training\n",
      "current loss: 8.331490516662598\n",
      "training\n",
      "current loss: 6.9695353507995605\n",
      "training\n",
      "current loss: 7.767199516296387\n",
      "training\n",
      "current loss: 7.609757423400879\n",
      "training\n",
      "current loss: 5.040931701660156\n",
      "training\n",
      "current loss: 6.01019811630249\n",
      "training\n",
      "current loss: 6.509990692138672\n",
      "training\n",
      "current loss: 6.011083602905273\n",
      "training\n",
      "current loss: 5.014852046966553\n",
      "training\n",
      "current loss: 7.657990455627441\n",
      "training\n",
      "current loss: 6.058718681335449\n",
      "training\n",
      "current loss: 6.995948314666748\n",
      "training\n",
      "current loss: 6.046499729156494\n",
      "training\n",
      "current loss: 8.321365356445312\n",
      "training\n",
      "current loss: 5.1038947105407715\n",
      "training\n",
      "current loss: 7.009615898132324\n",
      "training\n",
      "current loss: 5.218894958496094\n",
      "training\n",
      "current loss: 7.0059428215026855\n",
      "training\n",
      "current loss: 6.095254421234131\n",
      "training\n",
      "current loss: 7.016987323760986\n",
      "training\n",
      "current loss: 7.759031772613525\n",
      "training\n",
      "current loss: 6.8692169189453125\n",
      "training\n",
      "current loss: 6.36315393447876\n",
      "training\n",
      "current loss: 6.0140156745910645\n",
      "training\n",
      "current loss: 5.026381015777588\n",
      "training\n",
      "current loss: 7.491957664489746\n",
      "training\n",
      "current loss: 6.339685440063477\n",
      "training\n",
      "current loss: 7.884851932525635\n",
      "training\n",
      "current loss: 6.876993179321289\n",
      "training\n",
      "current loss: 7.279143333435059\n",
      "training\n",
      "current loss: 7.032066345214844\n",
      "training\n",
      "current loss: 7.804866313934326\n",
      "training\n",
      "current loss: 5.051602363586426\n",
      "training\n",
      "current loss: 8.985933303833008\n",
      "training\n",
      "current loss: 7.894439220428467\n",
      "training\n",
      "current loss: 6.197002410888672\n",
      "training\n",
      "current loss: 5.046425819396973\n",
      "training\n",
      "current loss: 6.625812530517578\n",
      "training\n",
      "current loss: 5.329938888549805\n",
      "training\n",
      "current loss: 6.048409461975098\n",
      "training\n",
      "current loss: 7.050553321838379\n",
      "training\n",
      "current loss: 5.972043991088867\n",
      "training\n",
      "current loss: 5.963318824768066\n",
      "training\n",
      "current loss: 6.018807888031006\n",
      "training\n",
      "current loss: 6.973730564117432\n",
      "training\n",
      "current loss: 6.9395856857299805\n",
      "training\n",
      "current loss: 7.020408630371094\n",
      "training\n",
      "current loss: 5.377188205718994\n",
      "training\n",
      "current loss: 6.763912200927734\n",
      "training\n",
      "current loss: 7.210090637207031\n",
      "training\n",
      "current loss: 5.213404655456543\n",
      "training\n",
      "current loss: 5.202754497528076\n",
      "training\n",
      "current loss: 6.2051191329956055\n",
      "training\n",
      "current loss: 6.186158180236816\n",
      "training\n",
      "current loss: 5.109267711639404\n",
      "training\n",
      "current loss: 6.669820785522461\n",
      "training\n",
      "current loss: 6.552199840545654\n",
      "training\n",
      "current loss: 8.382999420166016\n",
      "training\n",
      "current loss: 6.05728816986084\n",
      "training\n",
      "current loss: 6.419646263122559\n",
      "training\n",
      "current loss: 5.462575912475586\n",
      "training\n",
      "current loss: 8.885431289672852\n",
      "training\n",
      "current loss: 5.020554542541504\n",
      "training\n",
      "current loss: 6.54269552230835\n",
      "training\n",
      "current loss: 6.84425163269043\n",
      "training\n",
      "current loss: 6.352238655090332\n",
      "training\n",
      "current loss: 6.428160667419434\n",
      "training\n",
      "current loss: 5.033299446105957\n",
      "training\n",
      "current loss: 7.593295097351074\n",
      "training\n",
      "current loss: 7.884541988372803\n",
      "training\n",
      "current loss: 8.872941017150879\n",
      "training\n",
      "current loss: 5.013759613037109\n",
      "training\n",
      "current loss: 8.399982452392578\n",
      "training\n",
      "current loss: 7.984672546386719\n",
      "training\n",
      "current loss: 5.188605785369873\n",
      "training\n",
      "current loss: 7.4985551834106445\n",
      "training\n",
      "current loss: 9.12042236328125\n",
      "training\n",
      "current loss: 8.520700454711914\n",
      "training\n",
      "current loss: 6.909024238586426\n",
      "training\n",
      "current loss: 6.916318416595459\n",
      "training\n",
      "current loss: 7.691802978515625\n",
      "training\n",
      "current loss: 5.403896808624268\n",
      "training\n",
      "current loss: 6.4466376304626465\n",
      "training\n",
      "current loss: 6.906877517700195\n",
      "training\n",
      "current loss: 6.014360427856445\n",
      "training\n",
      "current loss: 7.065831184387207\n",
      "training\n",
      "current loss: 7.027093887329102\n",
      "training\n",
      "current loss: 8.09015941619873\n",
      "training\n",
      "current loss: 6.575420379638672\n",
      "training\n",
      "current loss: 5.025117874145508\n",
      "training\n",
      "current loss: 5.041250705718994\n",
      "training\n",
      "current loss: 6.726853847503662\n",
      "training\n",
      "current loss: 6.9795145988464355\n",
      "training\n",
      "current loss: 7.470019340515137\n",
      "training\n",
      "current loss: 7.516270637512207\n",
      "training\n",
      "current loss: 5.24759578704834\n",
      "training\n",
      "current loss: 6.731529235839844\n",
      "training\n",
      "current loss: 9.239660263061523\n",
      "training\n",
      "current loss: 5.069220542907715\n",
      "training\n",
      "current loss: 7.449580192565918\n",
      "training\n",
      "current loss: 5.012604236602783\n",
      "training\n",
      "current loss: 6.862152576446533\n",
      "training\n",
      "current loss: 6.1386871337890625\n",
      "training\n",
      "current loss: 6.000338077545166\n",
      "training\n",
      "current loss: 5.251154899597168\n",
      "training\n",
      "current loss: 7.973649978637695\n",
      "training\n",
      "current loss: 6.40872049331665\n",
      "training\n",
      "current loss: 5.9881768226623535\n",
      "training\n",
      "current loss: 5.012455940246582\n",
      "training\n",
      "current loss: 9.530109405517578\n",
      "training\n",
      "current loss: 7.02951717376709\n",
      "training\n",
      "current loss: 6.251194000244141\n",
      "training\n",
      "current loss: 5.0513105392456055\n",
      "training\n",
      "current loss: 8.00052261352539\n",
      "training\n",
      "current loss: 7.993124485015869\n",
      "training\n",
      "current loss: 8.930343627929688\n",
      "training\n",
      "current loss: 6.013218879699707\n",
      "training\n",
      "current loss: 7.099697113037109\n",
      "training\n",
      "current loss: 7.914545059204102\n",
      "training\n",
      "current loss: 7.089778900146484\n",
      "training\n",
      "current loss: 7.619047164916992\n",
      "training\n",
      "current loss: 7.012594223022461\n",
      "training\n",
      "current loss: 6.5876784324646\n"
     ]
    }
   ],
   "source": [
    "model_ft1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
