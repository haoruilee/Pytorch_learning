{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张奇MAX_Torch网课-第一课"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ZeweiChu/PyTorch-Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.bilibili.com/video/av62138405?from=search&seid=16955465206699397063"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy手写两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36071895.64961438\n",
      "1 33638759.43235971\n",
      "2 35663974.95950992\n",
      "3 35094102.63645306\n",
      "4 28165208.538973965\n",
      "5 17821640.264875457\n",
      "6 9325658.212858435\n",
      "7 4663590.31084634\n",
      "8 2549626.23570395\n",
      "9 1627217.923861003\n",
      "10 1186369.5912678437\n",
      "11 939901.8748079204\n",
      "12 778742.4710398583\n",
      "13 660105.9973346619\n",
      "14 566875.0914717915\n",
      "15 490954.80545251956\n",
      "16 427803.9054238959\n",
      "17 374691.7380388045\n",
      "18 329719.33728048153\n",
      "19 291323.7068340034\n",
      "20 258327.8096893634\n",
      "21 229891.3525010703\n",
      "22 205201.74545663627\n",
      "23 183702.6947988956\n",
      "24 164915.60772851206\n",
      "25 148416.5142053944\n",
      "26 133899.59046679683\n",
      "27 121090.58471173608\n",
      "28 109746.45930007647\n",
      "29 99651.7384607984\n",
      "30 90649.14121212665\n",
      "31 82594.6070077754\n",
      "32 75373.09953629156\n",
      "33 68900.3206996572\n",
      "34 63075.52356349074\n",
      "35 57818.532985178346\n",
      "36 53073.08982010561\n",
      "37 48776.5270472227\n",
      "38 44881.67054135527\n",
      "39 41347.44106886851\n",
      "40 38129.99186870442\n",
      "41 35197.60559835291\n",
      "42 32522.3518298298\n",
      "43 30076.517859011503\n",
      "44 27837.32211097776\n",
      "45 25785.138097858413\n",
      "46 23902.803717507803\n",
      "47 22173.461988254065\n",
      "48 20582.92984712709\n",
      "49 19118.813719886464\n",
      "50 17770.37778485036\n",
      "51 16526.233404826547\n",
      "52 15377.627673088064\n",
      "53 14316.866940263437\n",
      "54 13336.295053234122\n",
      "55 12429.370264403566\n",
      "56 11589.677690889195\n",
      "57 10812.042133718754\n",
      "58 10090.818017711788\n",
      "59 9421.504462790037\n",
      "60 8799.921854988288\n",
      "61 8222.76731919188\n",
      "62 7686.614797894003\n",
      "63 7191.885958402481\n",
      "64 6738.1402447768705\n",
      "65 6315.400273339411\n",
      "66 5921.395951646009\n",
      "67 5554.014427342032\n",
      "68 5211.161899411171\n",
      "69 4891.058543730186\n",
      "70 4592.3334708669045\n",
      "71 4313.119601624054\n",
      "72 4052.0882126745923\n",
      "73 3807.9760834645563\n",
      "74 3579.6024044193205\n",
      "75 3365.796793498853\n",
      "76 3165.6300906528622\n",
      "77 2978.2643944244037\n",
      "78 2802.6895477228045\n",
      "79 2638.1155319793106\n",
      "80 2483.7221911531706\n",
      "81 2338.9335560747154\n",
      "82 2203.1325145972896\n",
      "83 2075.7136798594993\n",
      "84 1956.0343783538083\n",
      "85 1843.6223949416903\n",
      "86 1738.0202033515905\n",
      "87 1638.7933001096324\n",
      "88 1545.5921715552274\n",
      "89 1457.9368040830122\n",
      "90 1375.5145974370344\n",
      "91 1298.0151271695015\n",
      "92 1225.0754977478612\n",
      "93 1156.4858019904036\n",
      "94 1091.9377659490792\n",
      "95 1031.146939701054\n",
      "96 973.7781388745597\n",
      "97 919.7791210788758\n",
      "98 868.8962872691899\n",
      "99 820.9877736649701\n",
      "100 775.8175431338649\n",
      "101 733.258181144961\n",
      "102 693.1337872261786\n",
      "103 655.3186646278231\n",
      "104 619.6711793800607\n",
      "105 586.0284667767149\n",
      "106 554.2901547345834\n",
      "107 524.3600096773696\n",
      "108 496.099435399783\n",
      "109 469.43596129772703\n",
      "110 444.26612827713745\n",
      "111 420.49668660287267\n",
      "112 398.05101870089004\n",
      "113 376.8522698220347\n",
      "114 356.8399910950061\n",
      "115 337.9288062588861\n",
      "116 320.0534150626467\n",
      "117 303.1672886307741\n",
      "118 287.2191764407142\n",
      "119 272.1468626315352\n",
      "120 257.908056546986\n",
      "121 244.4505244354454\n",
      "122 231.7238215135523\n",
      "123 219.68723484003715\n",
      "124 208.29945263423954\n",
      "125 197.53729620964103\n",
      "126 187.34080592080414\n",
      "127 177.69075992566096\n",
      "128 168.55551667002695\n",
      "129 159.90846304058044\n",
      "130 151.72320401344902\n",
      "131 143.9778920339634\n",
      "132 136.6370046973058\n",
      "133 129.6837804000981\n",
      "134 123.09597037235179\n",
      "135 116.85662926576907\n",
      "136 110.94718837544261\n",
      "137 105.34575677674015\n",
      "138 100.03621254307082\n",
      "139 95.00346905612082\n",
      "140 90.23284151794189\n",
      "141 85.71081193660594\n",
      "142 81.42264450917952\n",
      "143 77.35785184790056\n",
      "144 73.50046669161932\n",
      "145 69.8417984934123\n",
      "146 66.37362711529195\n",
      "147 63.08412363837363\n",
      "148 59.96015175244166\n",
      "149 56.99534544783344\n",
      "150 54.18181956551548\n",
      "151 51.51141200991523\n",
      "152 48.97767480631936\n",
      "153 46.57221372630199\n",
      "154 44.28825948882342\n",
      "155 42.12066590111414\n",
      "156 40.061728005279306\n",
      "157 38.106480299748355\n",
      "158 36.250160333896424\n",
      "159 34.485752814927906\n",
      "160 32.80990029572233\n",
      "161 31.217982576062482\n",
      "162 29.705501476081444\n",
      "163 28.269193558308228\n",
      "164 26.903410669200774\n",
      "165 25.60541480318699\n",
      "166 24.37187254729219\n",
      "167 23.199990286077913\n",
      "168 22.08608903480588\n",
      "169 21.027566368731314\n",
      "170 20.020223293100315\n",
      "171 19.062594083904735\n",
      "172 18.151885815795126\n",
      "173 17.285652205273333\n",
      "174 16.462260066044156\n",
      "175 15.678750559822161\n",
      "176 14.933511662602537\n",
      "177 14.22461621518123\n",
      "178 13.550158083782785\n",
      "179 12.908714649965393\n",
      "180 12.298487966324771\n",
      "181 11.7176172334168\n",
      "182 11.16500459070183\n",
      "183 10.63910660966165\n",
      "184 10.13836703583862\n",
      "185 9.661829779390295\n",
      "186 9.208191623403676\n",
      "187 8.77638991460653\n",
      "188 8.365559019368098\n",
      "189 7.974101533874267\n",
      "190 7.601323769925137\n",
      "191 7.2465392421512504\n",
      "192 6.9085584810704725\n",
      "193 6.586790234223608\n",
      "194 6.280281790874165\n",
      "195 5.988253232901216\n",
      "196 5.710302010924988\n",
      "197 5.445539702652422\n",
      "198 5.193150062931777\n",
      "199 4.952772098487923\n",
      "200 4.723692117467284\n",
      "201 4.505395090342947\n",
      "202 4.297450193554091\n",
      "203 4.099313079215705\n",
      "204 3.9104401873201398\n",
      "205 3.7305431224705363\n",
      "206 3.559069460774893\n",
      "207 3.3955437281798764\n",
      "208 3.239724733883858\n",
      "209 3.091206930685109\n",
      "210 2.94964653122302\n",
      "211 2.8147144563387223\n",
      "212 2.685964580386491\n",
      "213 2.563214620742734\n",
      "214 2.4462927862275707\n",
      "215 2.334698673819289\n",
      "216 2.22829600931985\n",
      "217 2.126811030045979\n",
      "218 2.0300508167990072\n",
      "219 1.9377642572494111\n",
      "220 1.8497683006226973\n",
      "221 1.7658162116399179\n",
      "222 1.685740570576265\n",
      "223 1.609374639095186\n",
      "224 1.5365146751236585\n",
      "225 1.4669905053620096\n",
      "226 1.400738453061618\n",
      "227 1.3374484561316806\n",
      "228 1.2770591979667882\n",
      "229 1.219452634420378\n",
      "230 1.164474251083667\n",
      "231 1.112063107024216\n",
      "232 1.0620490329667303\n",
      "233 1.0142735510100265\n",
      "234 0.9686765055523311\n",
      "235 0.9251594049911056\n",
      "236 0.8836340614908396\n",
      "237 0.8439970486181922\n",
      "238 0.8061702404837556\n",
      "239 0.7700479755141637\n",
      "240 0.7355833374579328\n",
      "241 0.7026900730436558\n",
      "242 0.6712867022693471\n",
      "243 0.6412899391635198\n",
      "244 0.6126702939349304\n",
      "245 0.5853265633150886\n",
      "246 0.5592200401774747\n",
      "247 0.5342923151906598\n",
      "248 0.5104911393346938\n",
      "249 0.487759554114333\n",
      "250 0.4660648056198684\n",
      "251 0.44533914093887544\n",
      "252 0.42555288414800807\n",
      "253 0.40664749153742463\n",
      "254 0.3886058629406206\n",
      "255 0.3713629606529829\n",
      "256 0.35490308791689384\n",
      "257 0.33918791387628794\n",
      "258 0.3241661247505302\n",
      "259 0.30981376002304656\n",
      "260 0.2961024051787057\n",
      "261 0.28300704648858294\n",
      "262 0.27050370807076585\n",
      "263 0.25855062658404737\n",
      "264 0.24713183213935996\n",
      "265 0.23622143364902937\n",
      "266 0.2258420844703373\n",
      "267 0.21592399103288998\n",
      "268 0.20645033524186407\n",
      "269 0.19739427299984308\n",
      "270 0.18874228125275058\n",
      "271 0.180473148733362\n",
      "272 0.1725732158237726\n",
      "273 0.1650203837986863\n",
      "274 0.15780833575644404\n",
      "275 0.1509083984422741\n",
      "276 0.1443143688660129\n",
      "277 0.13801313255520586\n",
      "278 0.1319855762004451\n",
      "279 0.12622413532847582\n",
      "280 0.12071822673163654\n",
      "281 0.11545535074025834\n",
      "282 0.11042362929384383\n",
      "283 0.10561310813895981\n",
      "284 0.10101410337464795\n",
      "285 0.09661678701412887\n",
      "286 0.09241323901948129\n",
      "287 0.08839458448956214\n",
      "288 0.0845521629519928\n",
      "289 0.08087803708142326\n",
      "290 0.07736435502555591\n",
      "291 0.07400732963959906\n",
      "292 0.07079472992400965\n",
      "293 0.06772493970064805\n",
      "294 0.06478677658299231\n",
      "295 0.06197713659056825\n",
      "296 0.05929049553366832\n",
      "297 0.05672140342597298\n",
      "298 0.05426488763538971\n",
      "299 0.051916130785040294\n",
      "300 0.0496702715063668\n",
      "301 0.04752120501344331\n",
      "302 0.045465112955211405\n",
      "303 0.0434988673114116\n",
      "304 0.041619093214071576\n",
      "305 0.03982159493755204\n",
      "306 0.038101576260300495\n",
      "307 0.03645595441844899\n",
      "308 0.034881858734253715\n",
      "309 0.03337734275084071\n",
      "310 0.031937150729562894\n",
      "311 0.030559503587282003\n",
      "312 0.029242485462968922\n",
      "313 0.027981857723371174\n",
      "314 0.02677608742031626\n",
      "315 0.02562289820942949\n",
      "316 0.02451951480136096\n",
      "317 0.023463678506000736\n",
      "318 0.022453788453371103\n",
      "319 0.021487665172345488\n",
      "320 0.020563504388668087\n",
      "321 0.01967889373707752\n",
      "322 0.01883263011054506\n",
      "323 0.018023128413624658\n",
      "324 0.017249209814913177\n",
      "325 0.01650812550346847\n",
      "326 0.015799101042202886\n",
      "327 0.015120907448037785\n",
      "328 0.014472016718492929\n",
      "329 0.013850778569692364\n",
      "330 0.013256321414586634\n",
      "331 0.012687778521001124\n",
      "332 0.01214349876113479\n",
      "333 0.011622777420126879\n",
      "334 0.01112445973999633\n",
      "335 0.010647837683896669\n",
      "336 0.010191496522955226\n",
      "337 0.009754880963765806\n",
      "338 0.00933713485642041\n",
      "339 0.008937264628807787\n",
      "340 0.008554689523528575\n",
      "341 0.008188536456080957\n",
      "342 0.00783808882402617\n",
      "343 0.007502771355892407\n",
      "344 0.007181935708856769\n",
      "345 0.006874694934232864\n",
      "346 0.006580915139530753\n",
      "347 0.006299558997284968\n",
      "348 0.006030521023155216\n",
      "349 0.005772786933288658\n",
      "350 0.0055261673706020155\n",
      "351 0.005290147750274206\n",
      "352 0.005064169757623357\n",
      "353 0.004847955082448828\n",
      "354 0.004640921202701244\n",
      "355 0.004442830897869677\n",
      "356 0.0042532408302987995\n",
      "357 0.004071823065713583\n",
      "358 0.003898068645784291\n",
      "359 0.0037317492532110514\n",
      "360 0.003572585202599456\n",
      "361 0.003420223776242752\n",
      "362 0.0032744057611087305\n",
      "363 0.0031348202370248463\n",
      "364 0.0030012225171608107\n",
      "365 0.0028734066728182813\n",
      "366 0.002751033733089053\n",
      "367 0.0026338285885955837\n",
      "368 0.002521621012592314\n",
      "369 0.0024142220500935783\n",
      "370 0.0023114707869024646\n",
      "371 0.0022130477382473436\n",
      "372 0.002118924764205263\n",
      "373 0.002028744038388755\n",
      "374 0.0019424113085287917\n",
      "375 0.0018597629236054543\n",
      "376 0.001780634341251125\n",
      "377 0.0017049028077652435\n",
      "378 0.0016323942332553336\n",
      "379 0.0015629921911567886\n",
      "380 0.001496533204765567\n",
      "381 0.001432912570681467\n",
      "382 0.0013719978568195604\n",
      "383 0.0013136910819169947\n",
      "384 0.00125787045064844\n",
      "385 0.0012044628175549139\n",
      "386 0.001153279385724643\n",
      "387 0.0011042838103324567\n",
      "388 0.001057381458060366\n",
      "389 0.0010124697876410455\n",
      "390 0.0009694826687581757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391 0.0009283182537392663\n",
      "392 0.0008889104720361067\n",
      "393 0.0008511765623786647\n",
      "394 0.0008150420990183751\n",
      "395 0.0007804478594071024\n",
      "396 0.0007473357014225928\n",
      "397 0.0007156592917825516\n",
      "398 0.0006852950443700167\n",
      "399 0.0006562237056133958\n",
      "400 0.0006283924310743344\n",
      "401 0.0006017596233728001\n",
      "402 0.0005762408516137095\n",
      "403 0.0005518059760942481\n",
      "404 0.0005284123786673645\n",
      "405 0.0005060163244986986\n",
      "406 0.0004845788583545074\n",
      "407 0.00046403738038984434\n",
      "408 0.00044437158949981717\n",
      "409 0.00042553819454335245\n",
      "410 0.00040751117341418595\n",
      "411 0.00039024647936973305\n",
      "412 0.00037371456632321535\n",
      "413 0.0003578814171573563\n",
      "414 0.0003427263059387145\n",
      "415 0.0003282139254620486\n",
      "416 0.0003143133740704447\n",
      "417 0.00030100642192998654\n",
      "418 0.00028826149054410615\n",
      "419 0.0002760567993689525\n",
      "420 0.00026437090149745155\n",
      "421 0.00025317936002369507\n",
      "422 0.00024246804454138894\n",
      "423 0.00023221187137305093\n",
      "424 0.00022238754589068484\n",
      "425 0.00021297602422825057\n",
      "426 0.0002039665194757021\n",
      "427 0.0001953433760222954\n",
      "428 0.0001870808896110273\n",
      "429 0.0001791676004606429\n",
      "430 0.00017159179216482223\n",
      "431 0.00016433569149289953\n",
      "432 0.00015738532053148602\n",
      "433 0.0001507286733996318\n",
      "434 0.00014435658129129103\n",
      "435 0.00013825269389199964\n",
      "436 0.00013240747957194278\n",
      "437 0.00012681039785518582\n",
      "438 0.0001214497462555174\n",
      "439 0.00011631617297870894\n",
      "440 0.00011140022328473534\n",
      "441 0.00010669216608222359\n",
      "442 0.00010218256678845511\n",
      "443 9.786450951982687e-05\n",
      "444 9.37309162798081e-05\n",
      "445 8.977142699234047e-05\n",
      "446 8.597950718825163e-05\n",
      "447 8.234751533858434e-05\n",
      "448 7.887412449673256e-05\n",
      "449 7.55427523289545e-05\n",
      "450 7.235176872930391e-05\n",
      "451 6.929676625706379e-05\n",
      "452 6.637019908210965e-05\n",
      "453 6.356761058907913e-05\n",
      "454 6.0883378479158244e-05\n",
      "455 5.831212537164226e-05\n",
      "456 5.584971247583915e-05\n",
      "457 5.3491935836210974e-05\n",
      "458 5.123494725429665e-05\n",
      "459 4.907244006710535e-05\n",
      "460 4.7000863009088316e-05\n",
      "461 4.501704103794548e-05\n",
      "462 4.311729109104712e-05\n",
      "463 4.129787783404271e-05\n",
      "464 3.955534186121057e-05\n",
      "465 3.788628477164937e-05\n",
      "466 3.628750953294588e-05\n",
      "467 3.475631537772814e-05\n",
      "468 3.328957601212195e-05\n",
      "469 3.188548923114638e-05\n",
      "470 3.05409435125527e-05\n",
      "471 2.9252690201507517e-05\n",
      "472 2.8018897927463817e-05\n",
      "473 2.6837040208319683e-05\n",
      "474 2.5705733791806728e-05\n",
      "475 2.462179018138598e-05\n",
      "476 2.3583675042577206e-05\n",
      "477 2.258921887104127e-05\n",
      "478 2.1636705707605428e-05\n",
      "479 2.0724408357760455e-05\n",
      "480 1.9850498479833882e-05\n",
      "481 1.9013353092391963e-05\n",
      "482 1.8211623751305393e-05\n",
      "483 1.7443730911710607e-05\n",
      "484 1.670829324611097e-05\n",
      "485 1.600393338137389e-05\n",
      "486 1.532949896660771e-05\n",
      "487 1.4683289862626118e-05\n",
      "488 1.4064491724014892e-05\n",
      "489 1.347164363613e-05\n",
      "490 1.2903764384858368e-05\n",
      "491 1.2360024220719577e-05\n",
      "492 1.1839936328454603e-05\n",
      "493 1.1341039074529332e-05\n",
      "494 1.0863022075968047e-05\n",
      "495 1.040521355674092e-05\n",
      "496 9.966717311880747e-06\n",
      "497 9.546749256753134e-06\n",
      "498 9.14444593816391e-06\n",
      "499 8.759238044896258e-06\n",
      "运行时间 0.5397589206695557\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "time1=time.time()\n",
    "\n",
    "N,D_in,H,D_out=64,1000,100,10\n",
    "#64个输入，输入为1000维，输出为10维，HIdden_state为100维\n",
    "#随机创建训练数据\n",
    "#一般矩阵大写X，向量小写x\n",
    "x=np.random.randn(N,D_in)#N个输入，每个D_in维\n",
    "y=np.random.randn(N,D_out)\n",
    "\n",
    "#权重层\n",
    "w1=np.random.randn(D_in,H)\n",
    "w2=np.random.randn(H,D_out)\n",
    "\n",
    "learning_rate=1e-6\n",
    "for it in range(500):\n",
    "    #forward pass前向传播\n",
    "    #即为模型\n",
    "    \n",
    "    h=x.dot(w1)#维度变为N*H\n",
    "    h_relu=np.maximum(h,0)#relu函数\n",
    "    y_pred=h_relu.dot(w2)#预测\n",
    "    \n",
    "    #定义loss\n",
    "    #MSE loss，均方误差\n",
    "    #MSE是非凸的，所以不一定会找到全局最优，梯度下降过程中也可能先下后上再下\n",
    "    loss=np.square(y_pred - y).sum()#都是64*10\n",
    "    print(it,loss)#it 为iteration\n",
    "    \n",
    "    #backward pass反向传播\n",
    "    #compute gradient\n",
    "    \n",
    "    # loss = (y_pred - y) ** 2\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    # \n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    \n",
    "time2=time.time()\n",
    "print('运行时间',(time2-time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.34080434e-01, -9.93933852e-01,  6.70757243e-01,\n",
       "        -6.78360040e-01, -1.69774101e-01, -1.65769790e+00,\n",
       "         2.39723384e+00,  8.88519643e-01, -8.19984232e-01,\n",
       "        -5.09232236e-01],\n",
       "       [ 5.71812552e-01, -1.23765707e+00, -7.95386421e-02,\n",
       "         4.02179248e-01,  1.08136898e+00, -5.49461053e-01,\n",
       "        -3.16270234e-01, -5.08490704e-01, -6.76561700e-01,\n",
       "        -7.18108343e-01],\n",
       "       [-1.78820842e-01, -3.61958783e-01,  1.10837716e+00,\n",
       "         7.03418554e-01,  1.43938008e+00, -5.61541891e-01,\n",
       "         6.09285568e-01, -9.76458994e-01, -2.36103845e-01,\n",
       "        -1.03568281e+00],\n",
       "       [ 1.48571975e-01, -2.55360577e-01, -1.85083647e-01,\n",
       "        -1.24029190e+00, -1.66643307e-01, -4.49683109e-01,\n",
       "         1.13827528e+00, -2.97677696e-01,  8.66912538e-01,\n",
       "        -1.53002856e+00],\n",
       "       [-5.48854387e-01, -1.01009699e+00,  4.98664020e-01,\n",
       "        -9.21089901e-01, -4.51930600e-02,  4.16216913e-03,\n",
       "         1.40174470e-01,  3.97924089e-01,  1.05626332e+00,\n",
       "        -1.14059365e+00],\n",
       "       [-3.75976677e-01,  7.80053286e-01,  1.14179135e+00,\n",
       "        -5.61686535e-01, -2.10878319e+00,  6.31892478e-01,\n",
       "        -1.07716053e+00, -1.60577527e+00, -8.11680516e-01,\n",
       "        -1.53209353e+00],\n",
       "       [-2.35533886e+00, -1.88532607e+00,  1.84388908e+00,\n",
       "        -1.62761125e+00,  4.62833852e-01,  3.59892278e-01,\n",
       "        -8.09370717e-01, -1.63304339e+00,  3.87938434e-01,\n",
       "         1.53198522e+00],\n",
       "       [ 3.96753364e-01,  7.78500474e-03, -2.25270986e-01,\n",
       "        -3.71546292e-01,  3.89560299e-01, -1.99035095e-01,\n",
       "         7.51858818e-01,  3.17051996e-01,  1.44825713e+00,\n",
       "        -1.25896138e+00],\n",
       "       [ 1.76138321e-01,  8.00483532e-01,  8.30126251e-01,\n",
       "         9.54975808e-01, -9.63535636e-02,  3.41244216e-01,\n",
       "        -3.56995428e-02, -5.86638202e-01, -2.01828851e-01,\n",
       "        -1.95666539e-01],\n",
       "       [ 9.34463037e-01,  2.50228672e+00, -8.53700785e-01,\n",
       "         7.73783630e-01, -1.77294321e-01,  1.11946907e+00,\n",
       "         2.73796371e-01,  4.58531473e-01, -8.71495254e-01,\n",
       "        -2.85371025e-01],\n",
       "       [ 3.35729493e-01,  6.20961054e-01,  1.04630623e+00,\n",
       "         7.61765623e-01,  5.78707919e-01,  4.31237011e-01,\n",
       "        -2.28914075e+00,  6.26469644e-01,  9.64903728e-01,\n",
       "         1.42490107e+00],\n",
       "       [-3.32888578e-01,  1.62372153e+00, -1.55919814e+00,\n",
       "         4.99115568e-01, -6.80338487e-01, -2.48790893e-01,\n",
       "         3.13624213e-01, -1.33408082e-01,  2.16618277e-01,\n",
       "        -3.45816278e-01],\n",
       "       [ 4.85712171e-01,  2.02881068e-02, -4.46396268e-01,\n",
       "         6.90452622e-01, -1.40785631e+00, -7.60811975e-01,\n",
       "         7.97513786e-01,  4.88254640e-02,  1.61240725e-01,\n",
       "         4.83762125e-01],\n",
       "       [-1.22915175e+00,  1.65995868e+00, -1.08163147e+00,\n",
       "         9.29957594e-01, -1.07297541e-01,  7.98407952e-02,\n",
       "         5.00207662e-01,  4.17036508e-01,  1.46097230e-01,\n",
       "         1.44801556e-01],\n",
       "       [-1.37749671e+00,  3.23915864e-01,  9.00489690e-01,\n",
       "         1.45536832e+00, -2.65338569e+00,  1.52783974e-01,\n",
       "         1.30724092e+00,  1.87387275e-01, -3.48181043e-01,\n",
       "         6.78535336e-02],\n",
       "       [ 9.89064669e-01, -2.60833538e-02, -1.66032498e-01,\n",
       "         1.32509914e+00, -2.69457404e-01, -3.42528463e-01,\n",
       "        -9.24642058e-01, -7.04060481e-01, -9.91188820e-01,\n",
       "         1.31233443e+00],\n",
       "       [ 1.90491880e-01,  8.01647720e-01, -1.18122705e+00,\n",
       "         5.68974311e-01,  8.88828643e-01, -1.18127811e+00,\n",
       "        -5.00048815e-01,  9.24710767e-01, -1.35023124e+00,\n",
       "        -1.63684557e+00],\n",
       "       [ 1.14513469e+00, -8.82174433e-01,  4.06196426e-02,\n",
       "         1.93068476e+00,  7.44287575e-01, -4.74066834e-01,\n",
       "        -1.76629205e-01,  5.53550188e-02, -1.02718375e+00,\n",
       "        -1.65194281e-01],\n",
       "       [ 1.38935971e+00, -6.33212078e-01, -4.03016331e-01,\n",
       "        -9.44794025e-01,  1.02301832e+00, -7.93994624e-01,\n",
       "        -1.86757920e+00, -3.56775388e-01, -1.21817501e-01,\n",
       "         3.58479635e-01],\n",
       "       [-5.90036060e-01, -3.25944192e-01, -8.76701775e-01,\n",
       "         1.32352489e+00,  6.92519494e-01,  4.05945641e-02,\n",
       "        -9.31301695e-01,  9.51713376e-01, -1.42714384e-01,\n",
       "         1.16769782e+00],\n",
       "       [-9.14845160e-02,  3.33873336e-01,  1.99678302e+00,\n",
       "         1.42672994e+00, -1.20002374e+00,  4.39692845e-01,\n",
       "        -9.65397847e-01, -1.12661097e+00,  1.85588081e-01,\n",
       "        -8.83441433e-01],\n",
       "       [ 1.60324906e+00,  1.27234910e+00,  3.87872065e-01,\n",
       "         9.54465476e-01, -7.20653903e-01,  2.06641516e+00,\n",
       "         4.92427897e-01,  1.77884551e+00, -2.52973820e-01,\n",
       "         4.42455094e-01],\n",
       "       [ 1.42615525e+00, -2.11523021e-01, -5.54390303e-02,\n",
       "        -1.04524529e+00,  6.59041944e-01,  9.37336306e-01,\n",
       "        -1.59730355e-01, -7.45503484e-01,  2.84550622e-01,\n",
       "        -1.25848183e+00],\n",
       "       [-1.91964553e-01, -1.75212700e+00, -5.93828661e-01,\n",
       "         8.47431783e-01, -2.76273718e-01, -3.73052524e-01,\n",
       "         1.82374796e+00,  1.14061954e+00, -2.55872505e-01,\n",
       "         2.18959674e+00],\n",
       "       [-2.40728207e-01,  8.51450094e-02, -9.54032134e-01,\n",
       "        -1.24007383e-01, -1.02793614e+00, -1.46495172e+00,\n",
       "        -1.36231740e-01, -3.49172881e-01, -2.36413920e+00,\n",
       "        -3.65863583e-01],\n",
       "       [ 6.81536618e-01,  3.24899759e-01, -9.52097197e-01,\n",
       "         4.72954848e-01, -3.62283640e-01,  1.42762955e-01,\n",
       "        -3.03089432e-01,  1.77159176e+00,  7.93737742e-01,\n",
       "         1.26315020e+00],\n",
       "       [-6.83697752e-01,  7.91963676e-01, -7.71033902e-01,\n",
       "         9.29576030e-01,  9.87837415e-01, -1.36009539e+00,\n",
       "        -2.10322282e-01,  5.48525334e-01,  1.01722886e+00,\n",
       "        -9.00821864e-01],\n",
       "       [-6.73506474e-01,  1.60487976e+00,  3.06886582e-02,\n",
       "        -9.32690332e-01,  3.07527436e-01,  3.21800406e-01,\n",
       "        -1.31444739e+00,  2.34208025e-01, -5.46528591e-01,\n",
       "        -1.55364464e+00],\n",
       "       [ 1.94361748e-01,  4.59369341e-01, -1.25293472e+00,\n",
       "        -3.08674523e-01, -1.32853719e+00, -1.69968377e+00,\n",
       "        -1.24193915e+00,  1.26668659e+00, -9.73096233e-01,\n",
       "         6.74603529e-01],\n",
       "       [-6.64205012e-01, -2.01233731e-01,  3.32147810e-01,\n",
       "        -1.63013829e+00,  1.11551366e+00, -6.41035815e-01,\n",
       "         3.82118519e-01,  2.29198797e+00,  2.73261618e-01,\n",
       "         4.60513134e-01],\n",
       "       [-3.45976569e-01, -2.21905245e+00, -9.90193687e-01,\n",
       "         9.60501633e-01, -7.09326950e-01,  1.72640190e-01,\n",
       "         3.15148059e-01, -1.83682644e-01, -1.00263986e+00,\n",
       "        -7.48723739e-01],\n",
       "       [ 1.34228977e+00, -7.80005682e-01, -3.94460622e-01,\n",
       "        -1.21797816e+00, -5.53577562e-01, -8.48292183e-01,\n",
       "        -4.34544258e-01,  7.50973872e-01, -1.38360854e+00,\n",
       "         6.08660978e-01],\n",
       "       [-6.02621995e-01, -8.02365770e-02, -6.70036478e-01,\n",
       "         4.62378008e-01, -5.99473227e-01,  5.55603224e-01,\n",
       "         7.98565913e-01, -1.75234423e-01, -1.95035285e-01,\n",
       "         3.08668846e+00],\n",
       "       [-3.41137625e-02, -9.47452550e-01, -6.84445865e-02,\n",
       "         7.83274105e-01, -1.46637904e+00, -2.10189332e+00,\n",
       "         4.66299545e-01, -8.14337128e-01,  1.42650862e+00,\n",
       "         4.48133105e-01],\n",
       "       [-1.86503982e+00, -1.00036959e+00,  5.97248438e-01,\n",
       "        -4.17207409e-01, -5.53213921e-01, -9.64814916e-01,\n",
       "        -1.61102997e-01,  8.14063083e-01, -1.32614674e+00,\n",
       "         4.65289785e-01],\n",
       "       [-6.68098976e-01,  1.36664788e+00, -2.11658954e-01,\n",
       "         2.19937888e-01, -4.74024123e-01, -1.77638675e-01,\n",
       "         1.57162631e-01, -4.80383245e-01, -3.16133585e-01,\n",
       "        -7.21398012e-01],\n",
       "       [ 2.95266869e-01,  8.15675127e-01, -2.13774168e+00,\n",
       "         1.64817583e-01, -8.81154396e-01,  4.60604467e-01,\n",
       "        -1.29218258e+00,  1.03991372e+00,  1.57790454e+00,\n",
       "         2.33681388e+00],\n",
       "       [ 1.37943908e+00, -1.08671384e+00,  9.90236853e-01,\n",
       "        -1.09267797e-01,  5.86411278e-01,  1.28494165e-01,\n",
       "        -1.06604215e-01, -1.09125471e+00, -3.39011413e-01,\n",
       "        -1.58522296e-01],\n",
       "       [-1.28748062e+00, -6.80676387e-01,  1.17093391e+00,\n",
       "         2.61128731e-01, -4.52823355e-01, -3.96045467e-02,\n",
       "        -2.48476840e-03, -7.37175412e-01,  3.49428835e-01,\n",
       "        -2.07453837e-01],\n",
       "       [-3.16424387e-01,  1.08994186e-01, -4.75490048e-01,\n",
       "         1.14583323e+00,  2.84568121e-01,  4.96928967e-01,\n",
       "        -2.95294144e-01, -3.05927577e-01, -3.33094981e-01,\n",
       "        -5.10713587e-01],\n",
       "       [ 6.89278960e-02, -1.17242798e+00,  5.96129719e-01,\n",
       "        -5.47592915e-02, -3.64413659e-02,  2.26052424e-01,\n",
       "        -1.14696537e+00, -5.12569945e-01, -3.19340512e-01,\n",
       "        -6.57904938e-01],\n",
       "       [ 9.72033408e-01, -1.62292188e+00, -1.20154730e+00,\n",
       "        -1.40093294e+00,  1.20660314e+00,  1.52251428e-02,\n",
       "        -2.38261678e+00,  1.01189931e+00,  1.36742485e-01,\n",
       "         1.23764174e+00],\n",
       "       [ 1.37872807e+00,  4.65171645e-01, -3.61483829e-01,\n",
       "        -6.92721857e-01,  2.18453254e+00,  1.49691711e+00,\n",
       "         6.19828743e-02, -1.85094690e+00, -4.53985939e-01,\n",
       "         2.05179114e-01],\n",
       "       [-2.01323742e-01, -8.30879610e-01, -1.76612332e+00,\n",
       "         1.22020940e+00,  2.10956699e-01, -4.95877417e-01,\n",
       "         1.04958127e-01,  8.06934363e-01, -4.17212216e-01,\n",
       "         1.10595789e+00],\n",
       "       [ 9.87201228e-01,  1.02044670e-01, -8.26022695e-01,\n",
       "        -6.99361267e-01,  7.78871380e-01,  5.56614014e-01,\n",
       "        -5.11608511e-01,  2.85779982e-01,  1.20000632e+00,\n",
       "        -1.46866732e+00],\n",
       "       [-5.50086166e-01, -4.92919977e-01, -3.78982814e-01,\n",
       "         9.78422139e-01,  7.86306441e-01, -8.21034695e-01,\n",
       "        -7.35957693e-01,  5.87105019e-02, -1.24791490e+00,\n",
       "        -1.75543676e+00],\n",
       "       [ 1.31489371e+00, -1.23505476e+00, -1.31780491e+00,\n",
       "        -1.33303266e+00,  2.13578476e-01,  5.35272356e-01,\n",
       "         1.45406172e+00,  9.33490906e-01,  5.32422247e-01,\n",
       "        -4.42038690e-01],\n",
       "       [ 8.97182760e-01,  1.29554325e+00, -2.27673497e-01,\n",
       "        -9.90061805e-01,  4.09283719e-01,  7.90330707e-02,\n",
       "        -5.60410689e-02,  7.00111926e-01, -9.62631995e-01,\n",
       "        -1.79948291e-01],\n",
       "       [ 9.01247331e-01, -2.24716456e-01,  1.53279579e+00,\n",
       "        -1.78880214e-01, -1.61804579e+00, -1.72482124e+00,\n",
       "         2.82989885e-01, -1.25883770e+00,  3.62704419e-01,\n",
       "        -1.31320184e+00],\n",
       "       [-1.78988469e-01,  4.87965099e-01, -4.99008924e-01,\n",
       "         1.24801549e+00, -2.13653208e+00,  1.46028088e+00,\n",
       "         1.15255342e-01, -1.00772949e+00, -1.43579305e+00,\n",
       "        -2.88324096e-01],\n",
       "       [ 7.53877481e-03,  8.12446621e-01,  1.11737737e+00,\n",
       "        -1.47659675e+00, -1.12444322e+00,  1.53222362e-01,\n",
       "        -1.36548241e+00, -6.64612904e-01, -5.05857579e-01,\n",
       "         1.10669159e+00],\n",
       "       [-6.54100039e-01, -4.19033458e-01,  2.60031009e+00,\n",
       "        -4.83793199e-01, -3.20063483e-01,  7.50916577e-01,\n",
       "        -1.86155784e-01, -2.05896021e+00, -4.41791714e-01,\n",
       "         6.47518645e-01],\n",
       "       [ 1.04478608e+00, -2.52357637e-01,  7.28344230e-01,\n",
       "         1.87489679e+00,  7.55725603e-02, -2.54316177e+00,\n",
       "        -7.11219438e-02,  1.93095430e+00, -2.07667767e+00,\n",
       "         1.26241269e-01],\n",
       "       [-1.02351467e+00, -3.15246907e-01,  2.04023911e+00,\n",
       "         1.81936616e-01, -2.52745637e+00, -1.41467684e+00,\n",
       "        -6.16093046e-01,  1.66406233e+00,  4.09266243e-01,\n",
       "         5.13694952e-01],\n",
       "       [-5.99894020e-01,  5.03065493e-01,  5.39954341e-01,\n",
       "         1.86789844e-01, -1.56685859e-02, -1.31569647e+00,\n",
       "        -8.61012188e-01, -8.81752829e-01,  7.73032506e-01,\n",
       "        -1.95624889e+00],\n",
       "       [-8.32438415e-01,  1.47589785e+00, -1.23962762e+00,\n",
       "        -1.49980342e-01, -6.20628492e-02, -9.74265345e-01,\n",
       "         1.48539102e+00, -1.44175920e+00, -1.34763246e+00,\n",
       "         1.29313625e+00],\n",
       "       [ 1.67406184e+00,  1.81829781e-01, -3.93348134e-01,\n",
       "         5.94132717e-01,  9.36134121e-01, -1.33688247e+00,\n",
       "        -2.33120588e+00, -3.88258148e-01,  6.19159440e-01,\n",
       "         1.32976782e+00],\n",
       "       [ 4.82755536e-01, -2.78050520e+00,  8.71118773e-01,\n",
       "         3.94141391e-01, -1.31037069e+00,  9.63580653e-02,\n",
       "         1.09998614e+00,  3.46321235e-01,  3.97820336e-01,\n",
       "         5.22013855e-01],\n",
       "       [-6.10340482e-01,  8.20498855e-01, -6.31067291e-02,\n",
       "         3.81182440e-01, -4.52172246e-02, -3.42068563e-01,\n",
       "        -1.07384071e+00, -2.82132802e-01,  9.04094908e-01,\n",
       "         8.36476934e-01],\n",
       "       [ 2.18142150e-01,  7.11189235e-02,  5.14717152e-01,\n",
       "        -2.19137045e-01,  1.52952979e-01,  9.52739176e-01,\n",
       "         1.09908317e+00, -1.87960692e+00, -1.46191074e-01,\n",
       "         1.31216814e+00],\n",
       "       [ 3.09221604e-01, -3.07235126e-01,  6.15675647e-01,\n",
       "        -1.35259251e+00, -7.38079108e-01, -1.80213833e+00,\n",
       "        -1.70622576e+00,  5.29914556e-01,  7.63437493e-01,\n",
       "         1.21539289e+00],\n",
       "       [-1.45186918e+00,  1.25389403e+00, -4.71605497e-01,\n",
       "         7.86977475e-02,  2.89926092e-02, -2.75660147e-01,\n",
       "        -1.09532914e+00,  9.38179699e-01, -7.31128978e-01,\n",
       "        -7.67380044e-01],\n",
       "       [ 1.03964634e+00,  8.81599042e-01, -5.13736356e-01,\n",
       "         4.91544612e-03, -1.12112704e-01,  6.61876059e-01,\n",
       "         1.10667747e+00, -5.14623500e-01, -4.67891581e-01,\n",
       "         8.36565738e-01],\n",
       "       [ 1.06424662e-01,  1.08062335e+00, -1.07344492e+00,\n",
       "        -5.20531124e-01,  1.47159681e+00, -2.99742512e-01,\n",
       "         1.05507095e+00,  2.49587878e-01,  1.29612733e+00,\n",
       "        -1.10135815e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#跑一遍模型,用x预测y\n",
    "h=x.dot(w1)#维度变为N*H\n",
    "h_relu=np.maximum(h,0)#relu函数\n",
    "y_pred=h_relu.dot(w2)#预测\n",
    "y_pred#y_pred应该和y非常相似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.34130261e-01, -9.93940431e-01,  6.70797383e-01,\n",
       "        -6.78345884e-01, -1.69768535e-01, -1.65765098e+00,\n",
       "         2.39720312e+00,  8.88504090e-01, -8.19961562e-01,\n",
       "        -5.09339515e-01],\n",
       "       [ 5.71712610e-01, -1.23765530e+00, -7.96316777e-02,\n",
       "         4.02139166e-01,  1.08135942e+00, -5.49524205e-01,\n",
       "        -3.16219273e-01, -5.08445772e-01, -6.76584722e-01,\n",
       "        -7.17968149e-01],\n",
       "       [-1.78827045e-01, -3.61969084e-01,  1.10843076e+00,\n",
       "         7.03413406e-01,  1.43938644e+00, -5.61518829e-01,\n",
       "         6.09314130e-01, -9.76397187e-01, -2.36108515e-01,\n",
       "        -1.03566996e+00],\n",
       "       [ 1.48576673e-01, -2.55357680e-01, -1.85101930e-01,\n",
       "        -1.24029769e+00, -1.66646110e-01, -4.49686025e-01,\n",
       "         1.13827844e+00, -2.97682789e-01,  8.66905151e-01,\n",
       "        -1.53001264e+00],\n",
       "       [-5.48857068e-01, -1.01008939e+00,  4.98694490e-01,\n",
       "        -9.21106322e-01, -4.52033466e-02,  4.17067805e-03,\n",
       "         1.40201014e-01,  3.97939623e-01,  1.05625408e+00,\n",
       "        -1.14055696e+00],\n",
       "       [-3.75905300e-01,  7.80066189e-01,  1.14183547e+00,\n",
       "        -5.61661591e-01, -2.10878059e+00,  6.31916932e-01,\n",
       "        -1.07722659e+00, -1.60578506e+00, -8.11674680e-01,\n",
       "        -1.53221996e+00],\n",
       "       [-2.35526176e+00, -1.88531355e+00,  1.84394975e+00,\n",
       "        -1.62760887e+00,  4.62827529e-01,  3.59863885e-01,\n",
       "        -8.09374951e-01, -1.63309916e+00,  3.87953295e-01,\n",
       "         1.53193428e+00],\n",
       "       [ 3.96841323e-01,  7.79824754e-03, -2.25188905e-01,\n",
       "        -3.71544076e-01,  3.89552493e-01, -1.99005221e-01,\n",
       "         7.51864813e-01,  3.17084665e-01,  1.44825476e+00,\n",
       "        -1.25897107e+00],\n",
       "       [ 1.76129298e-01,  8.00515837e-01,  8.30145671e-01,\n",
       "         9.54956740e-01, -9.63885554e-02,  3.41220354e-01,\n",
       "        -3.56653065e-02, -5.86624689e-01, -2.01845744e-01,\n",
       "        -1.95601411e-01],\n",
       "       [ 9.34581567e-01,  2.50228458e+00, -8.53643285e-01,\n",
       "         7.73830886e-01, -1.77325898e-01,  1.11948137e+00,\n",
       "         2.73753151e-01,  4.58467189e-01, -8.71471980e-01,\n",
       "        -2.85418881e-01],\n",
       "       [ 3.35743566e-01,  6.20973564e-01,  1.04634455e+00,\n",
       "         7.61753684e-01,  5.78688277e-01,  4.31242442e-01,\n",
       "        -2.28913738e+00,  6.26450314e-01,  9.64907390e-01,\n",
       "         1.42490271e+00],\n",
       "       [-3.32883536e-01,  1.62372543e+00, -1.55918945e+00,\n",
       "         4.99131744e-01, -6.80326896e-01, -2.48761032e-01,\n",
       "         3.13608649e-01, -1.33389600e-01,  2.16624262e-01,\n",
       "        -3.45848353e-01],\n",
       "       [ 4.85785816e-01,  2.02871288e-02, -4.46333417e-01,\n",
       "         6.90487873e-01, -1.40786764e+00, -7.60786098e-01,\n",
       "         7.97462241e-01,  4.87994174e-02,  1.61260509e-01,\n",
       "         4.83688566e-01],\n",
       "       [-1.22943065e+00,  1.65989189e+00, -1.08188892e+00,\n",
       "         9.29876847e-01, -1.07285414e-01,  7.97736620e-02,\n",
       "         5.00305713e-01,  4.17194111e-01,  1.46052971e-01,\n",
       "         1.45115722e-01],\n",
       "       [-1.37745287e+00,  3.23931281e-01,  9.00563007e-01,\n",
       "         1.45537116e+00, -2.65340334e+00,  1.52823682e-01,\n",
       "         1.30722844e+00,  1.87392289e-01, -3.48170703e-01,\n",
       "         6.78141748e-02],\n",
       "       [ 9.89085458e-01, -2.60910617e-02, -1.65992926e-01,\n",
       "         1.32510398e+00, -2.69465437e-01, -3.42506342e-01,\n",
       "        -9.24648786e-01, -7.04072269e-01, -9.91174167e-01,\n",
       "         1.31231977e+00],\n",
       "       [ 1.90411528e-01,  8.01637684e-01, -1.18130283e+00,\n",
       "         5.68930974e-01,  8.88840844e-01, -1.18133157e+00,\n",
       "        -5.00006376e-01,  9.24746187e-01, -1.35025965e+00,\n",
       "        -1.63670586e+00],\n",
       "       [ 1.14514718e+00, -8.82186909e-01,  4.06629071e-02,\n",
       "         1.93070449e+00,  7.44271454e-01, -4.74045832e-01,\n",
       "        -1.76660554e-01,  5.53055002e-02, -1.02714935e+00,\n",
       "        -1.65258282e-01],\n",
       "       [ 1.38920187e+00, -6.33237203e-01, -4.03145113e-01,\n",
       "        -9.44839443e-01,  1.02305960e+00, -7.94082570e-01,\n",
       "        -1.86748799e+00, -3.56721865e-01, -1.21837229e-01,\n",
       "         3.58679866e-01],\n",
       "       [-5.89669190e-01, -3.25826862e-01, -8.76360422e-01,\n",
       "         1.32368212e+00,  6.92457772e-01,  4.07711779e-02,\n",
       "        -9.31471815e-01,  9.51701546e-01, -1.42638750e-01,\n",
       "         1.16713356e+00],\n",
       "       [-9.16596821e-02,  3.33860071e-01,  1.99659129e+00,\n",
       "         1.42666304e+00, -1.20001022e+00,  4.39539165e-01,\n",
       "        -9.65294084e-01, -1.12653376e+00,  1.85544359e-01,\n",
       "        -8.83160228e-01],\n",
       "       [ 1.60333779e+00,  1.27234479e+00,  3.88013472e-01,\n",
       "         9.54514611e-01, -7.20663886e-01,  2.06644920e+00,\n",
       "         4.92379256e-01,  1.77881070e+00, -2.52949826e-01,\n",
       "         4.42334950e-01],\n",
       "       [ 1.42607098e+00, -2.11526036e-01, -5.54788940e-02,\n",
       "        -1.04528934e+00,  6.59035530e-01,  9.37326610e-01,\n",
       "        -1.59702094e-01, -7.45475721e-01,  2.84535541e-01,\n",
       "        -1.25840303e+00],\n",
       "       [-1.92060320e-01, -1.75216132e+00, -5.93913315e-01,\n",
       "         8.47389270e-01, -2.76263627e-01, -3.73100071e-01,\n",
       "         1.82377399e+00,  1.14064859e+00, -2.55867478e-01,\n",
       "         2.18969239e+00],\n",
       "       [-2.40723819e-01,  8.51212450e-02, -9.54064806e-01,\n",
       "        -1.23999301e-01, -1.02790073e+00, -1.46495900e+00,\n",
       "        -1.36225450e-01, -3.49182893e-01, -2.36416846e+00,\n",
       "        -3.65847103e-01],\n",
       "       [ 6.81510854e-01,  3.24894117e-01, -9.52115735e-01,\n",
       "         4.72951817e-01, -3.62276457e-01,  1.42772656e-01,\n",
       "        -3.03081634e-01,  1.77162824e+00,  7.93725508e-01,\n",
       "         1.26317301e+00],\n",
       "       [-6.83560936e-01,  7.92011288e-01, -7.70844515e-01,\n",
       "         9.29616597e-01,  9.87778235e-01, -1.36006494e+00,\n",
       "        -2.10360058e-01,  5.48503537e-01,  1.01727996e+00,\n",
       "        -9.00993506e-01],\n",
       "       [-6.73532109e-01,  1.60487093e+00,  3.06810235e-02,\n",
       "        -9.32695619e-01,  3.07533788e-01,  3.21803448e-01,\n",
       "        -1.31443733e+00,  2.34232746e-01, -5.46526547e-01,\n",
       "        -1.55362071e+00],\n",
       "       [ 1.94054476e-01,  4.59369128e-01, -1.25333268e+00,\n",
       "        -3.08866452e-01, -1.32855763e+00, -1.69990368e+00,\n",
       "        -1.24177086e+00,  1.26679738e+00, -9.73186606e-01,\n",
       "         6.75068791e-01],\n",
       "       [-6.64261642e-01, -2.01240484e-01,  3.32103753e-01,\n",
       "        -1.63015193e+00,  1.11552167e+00, -6.41046665e-01,\n",
       "         3.82135716e-01,  2.29198327e+00,  2.73251533e-01,\n",
       "         4.60570716e-01],\n",
       "       [-3.45920431e-01, -2.21906441e+00, -9.90129894e-01,\n",
       "         9.60546856e-01, -7.09332795e-01,  1.72670153e-01,\n",
       "         3.15101324e-01, -1.83719765e-01, -1.00262335e+00,\n",
       "        -7.48798154e-01],\n",
       "       [ 1.34233035e+00, -7.79957485e-01, -3.94337770e-01,\n",
       "        -1.21798127e+00, -5.53645442e-01, -8.48234856e-01,\n",
       "        -4.34547300e-01,  7.50997544e-01, -1.38360111e+00,\n",
       "         6.08574339e-01],\n",
       "       [-6.02794947e-01, -8.02699659e-02, -6.70203676e-01,\n",
       "         4.62341728e-01, -5.99472038e-01,  5.55587109e-01,\n",
       "         7.98680250e-01, -1.75155647e-01, -1.95059579e-01,\n",
       "         3.08692423e+00],\n",
       "       [-3.44581079e-02, -9.47474016e-01, -6.87921315e-02,\n",
       "         7.83131420e-01, -1.46640158e+00, -2.10191646e+00,\n",
       "         4.66375849e-01, -8.14115213e-01,  1.42642342e+00,\n",
       "         4.48444387e-01],\n",
       "       [-1.86507722e+00, -1.00037114e+00,  5.97239092e-01,\n",
       "        -4.17229067e-01, -5.53225628e-01, -9.64806619e-01,\n",
       "        -1.61062573e-01,  8.14092125e-01, -1.32615918e+00,\n",
       "         4.65344663e-01],\n",
       "       [-6.68033834e-01,  1.36664996e+00, -2.11573616e-01,\n",
       "         2.19960619e-01, -4.74028910e-01, -1.77578654e-01,\n",
       "         1.57151669e-01, -4.80384951e-01, -3.16133644e-01,\n",
       "        -7.21442978e-01],\n",
       "       [ 2.95220692e-01,  8.15660455e-01, -2.13780441e+00,\n",
       "         1.64811591e-01, -8.81184537e-01,  4.60557114e-01,\n",
       "        -1.29217949e+00,  1.03991636e+00,  1.57789678e+00,\n",
       "         2.33690205e+00],\n",
       "       [ 1.37938091e+00, -1.08668659e+00,  9.90208330e-01,\n",
       "        -1.09285916e-01,  5.86383072e-01,  1.28476840e-01,\n",
       "        -1.06577973e-01, -1.09121043e+00, -3.39011199e-01,\n",
       "        -1.58468339e-01],\n",
       "       [-1.28751924e+00, -6.80707998e-01,  1.17086537e+00,\n",
       "         2.61144605e-01, -4.52812807e-01, -3.96287842e-02,\n",
       "        -2.49953250e-03, -7.37179872e-01,  3.49426159e-01,\n",
       "        -2.07406619e-01],\n",
       "       [-3.16455662e-01,  1.08969988e-01, -4.75550159e-01,\n",
       "         1.14583072e+00,  2.84591310e-01,  4.96911772e-01,\n",
       "        -2.95284454e-01, -3.05899746e-01, -3.33102189e-01,\n",
       "        -5.10671737e-01],\n",
       "       [ 6.89840911e-02, -1.17242291e+00,  5.96190970e-01,\n",
       "        -5.47503004e-02, -3.64526705e-02,  2.26070478e-01,\n",
       "        -1.14697151e+00, -5.12581904e-01, -3.19330001e-01,\n",
       "        -6.57972172e-01],\n",
       "       [ 9.72017358e-01, -1.62291835e+00, -1.20156453e+00,\n",
       "        -1.40093356e+00,  1.20661465e+00,  1.52174908e-02,\n",
       "        -2.38260544e+00,  1.01188835e+00,  1.36740719e-01,\n",
       "         1.23765429e+00],\n",
       "       [ 1.37849408e+00,  4.65186863e-01, -3.61698937e-01,\n",
       "        -6.92827278e-01,  2.18457346e+00,  1.49678816e+00,\n",
       "         6.21234749e-02, -1.85084212e+00, -4.54059915e-01,\n",
       "         2.05398055e-01],\n",
       "       [-2.01304906e-01, -8.30854226e-01, -1.76610417e+00,\n",
       "         1.22021308e+00,  2.10914306e-01, -4.95890172e-01,\n",
       "         1.04946420e-01,  8.06917356e-01, -4.17217939e-01,\n",
       "         1.10595743e+00],\n",
       "       [ 9.87441880e-01,  1.02139091e-01, -8.25793515e-01,\n",
       "        -6.99291772e-01,  7.78876241e-01,  5.56647946e-01,\n",
       "        -5.11685206e-01,  2.85645506e-01,  1.20006905e+00,\n",
       "        -1.46892942e+00],\n",
       "       [-5.49798568e-01, -4.92880531e-01, -3.78681305e-01,\n",
       "         9.78573769e-01,  7.86239781e-01, -8.20899752e-01,\n",
       "        -7.36111863e-01,  5.86051246e-02, -1.24781687e+00,\n",
       "        -1.75589230e+00],\n",
       "       [ 1.31508159e+00, -1.23506838e+00, -1.31762641e+00,\n",
       "        -1.33300772e+00,  2.13606594e-01,  5.35335843e-01,\n",
       "         1.45397980e+00,  9.33384483e-01,  5.32437870e-01,\n",
       "        -4.42257878e-01],\n",
       "       [ 8.97250280e-01,  1.29553655e+00, -2.27603324e-01,\n",
       "        -9.90040089e-01,  4.09282838e-01,  7.90465174e-02,\n",
       "        -5.60685275e-02,  7.00084162e-01, -9.62614802e-01,\n",
       "        -1.80012324e-01],\n",
       "       [ 9.01065004e-01, -2.24722198e-01,  1.53263505e+00,\n",
       "        -1.78987832e-01, -1.61803766e+00, -1.72485648e+00,\n",
       "         2.83108094e-01, -1.25877490e+00,  3.62668956e-01,\n",
       "        -1.31299027e+00],\n",
       "       [-1.78929193e-01,  4.87949846e-01, -4.98956352e-01,\n",
       "         1.24804956e+00, -2.13653710e+00,  1.46029762e+00,\n",
       "         1.15203676e-01, -1.00777029e+00, -1.43578181e+00,\n",
       "        -2.88390647e-01],\n",
       "       [ 7.66781568e-03,  8.12501832e-01,  1.11752334e+00,\n",
       "        -1.47655623e+00, -1.12445711e+00,  1.53270000e-01,\n",
       "        -1.36556088e+00, -6.64651228e-01, -5.05838498e-01,\n",
       "         1.10652763e+00],\n",
       "       [-6.54116870e-01, -4.19018979e-01,  2.60031191e+00,\n",
       "        -4.83795999e-01, -3.20061232e-01,  7.50928561e-01,\n",
       "        -1.86152672e-01, -2.05895144e+00, -4.41805002e-01,\n",
       "         6.47493502e-01],\n",
       "       [ 1.04428106e+00, -2.52453077e-01,  7.27630669e-01,\n",
       "         1.87469922e+00,  7.58186934e-02, -2.54335723e+00,\n",
       "        -7.08795361e-02,  1.93110710e+00, -2.07684774e+00,\n",
       "         1.26867048e-01],\n",
       "       [-1.02361137e+00, -3.15279644e-01,  2.04014509e+00,\n",
       "         1.81920948e-01, -2.52741538e+00, -1.41471845e+00,\n",
       "        -6.16058567e-01,  1.66408209e+00,  4.09266863e-01,\n",
       "         5.13809773e-01],\n",
       "       [-5.99778881e-01,  5.03092103e-01,  5.40068626e-01,\n",
       "         1.86846665e-01, -1.56965791e-02, -1.31566403e+00,\n",
       "        -8.61071049e-01, -8.81787663e-01,  7.73047564e-01,\n",
       "        -1.95634802e+00],\n",
       "       [-8.32426208e-01,  1.47589443e+00, -1.23961443e+00,\n",
       "        -1.49975551e-01, -6.20744931e-02, -9.74256388e-01,\n",
       "         1.48538057e+00, -1.44176505e+00, -1.34762546e+00,\n",
       "         1.29310976e+00],\n",
       "       [ 1.67426061e+00,  1.81844987e-01, -3.93119008e-01,\n",
       "         5.94235757e-01,  9.36159558e-01, -1.33678977e+00,\n",
       "        -2.33130409e+00, -3.88340490e-01,  6.19242480e-01,\n",
       "         1.32947105e+00],\n",
       "       [ 4.82821215e-01, -2.78051006e+00,  8.71176917e-01,\n",
       "         3.94130717e-01, -1.31033490e+00,  9.63629449e-02,\n",
       "         1.10001078e+00,  3.46250326e-01,  3.97798177e-01,\n",
       "         5.21997789e-01],\n",
       "       [-6.10751568e-01,  8.20405873e-01, -6.38551240e-02,\n",
       "         3.81206966e-01, -4.50527944e-02, -3.42409981e-01,\n",
       "        -1.07389329e+00, -2.82263455e-01,  9.04074126e-01,\n",
       "         8.36837016e-01],\n",
       "       [ 2.18533149e-01,  7.11206671e-02,  5.15094039e-01,\n",
       "        -2.19018316e-01,  1.52948380e-01,  9.53020217e-01,\n",
       "         1.09891345e+00, -1.87969460e+00, -1.46119691e-01,\n",
       "         1.31168952e+00],\n",
       "       [ 3.09459027e-01, -3.07219104e-01,  6.15924804e-01,\n",
       "        -1.35252788e+00, -7.38096252e-01, -1.80203441e+00,\n",
       "        -1.70633749e+00,  5.29722309e-01,  7.63507155e-01,\n",
       "         1.21507162e+00],\n",
       "       [-1.45175255e+00,  1.25392189e+00, -4.71445012e-01,\n",
       "         7.87085460e-02,  2.89243694e-02, -2.75633040e-01,\n",
       "        -1.09541445e+00,  9.38081767e-01, -7.31091285e-01,\n",
       "        -7.67616964e-01],\n",
       "       [ 1.03983413e+00,  8.81588287e-01, -5.13552512e-01,\n",
       "         5.01430997e-03, -1.12133561e-01,  6.61943208e-01,\n",
       "         1.10659532e+00, -5.14715144e-01, -4.67869747e-01,\n",
       "         8.36389487e-01],\n",
       "       [ 1.06315285e-01,  1.08061649e+00, -1.07356313e+00,\n",
       "        -5.20570599e-01,  1.47161625e+00, -2.99792493e-01,\n",
       "         1.05512235e+00,  2.49633638e-01,  1.29609830e+00,\n",
       "        -1.10124881e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.98274095e-05,  6.57843516e-06, -4.01399622e-05,\n",
       "        -1.41559208e-05, -5.56624573e-06, -4.69207271e-05,\n",
       "         3.07288506e-05,  1.55532059e-05, -2.26697016e-05,\n",
       "         1.07279372e-04],\n",
       "       [ 9.99424018e-05, -1.77560148e-06,  9.30355197e-05,\n",
       "         4.00827092e-05,  9.55650916e-06,  6.31515520e-05,\n",
       "        -5.09611006e-05, -4.49318120e-05,  2.30213978e-05,\n",
       "        -1.40193855e-04],\n",
       "       [ 6.20272937e-06,  1.03007458e-05, -5.36001616e-05,\n",
       "         5.14814818e-06, -6.35330494e-06, -2.30615795e-05,\n",
       "        -2.85621487e-05, -6.18074758e-05,  4.67009325e-06,\n",
       "        -1.28563525e-05],\n",
       "       [-4.69783622e-06, -2.89764421e-06,  1.82825621e-05,\n",
       "         5.79372362e-06,  2.80263763e-06,  2.91667027e-06,\n",
       "        -3.15925059e-06,  5.09243665e-06,  7.38708846e-06,\n",
       "        -1.59164717e-05],\n",
       "       [ 2.68086043e-06, -7.60691548e-06, -3.04694704e-05,\n",
       "         1.64211668e-05,  1.02865403e-05, -8.50892292e-06,\n",
       "        -2.65443362e-05, -1.55338128e-05,  9.23850228e-06,\n",
       "        -3.66938276e-05],\n",
       "       [-7.13770774e-05, -1.29021760e-05, -4.41201350e-05,\n",
       "        -2.49439755e-05, -2.59756875e-06, -2.44546741e-05,\n",
       "         6.60557932e-05,  9.78862085e-06, -5.83571477e-06,\n",
       "         1.26430617e-04],\n",
       "       [-7.70945870e-05, -1.25131912e-05, -6.06737946e-05,\n",
       "        -2.37657200e-06,  6.32351304e-06,  2.83935956e-05,\n",
       "         4.23432676e-06,  5.57673530e-05, -1.48611868e-05,\n",
       "         5.09339871e-05],\n",
       "       [-8.79584316e-05, -1.32427977e-05, -8.20814195e-05,\n",
       "        -2.21607980e-06,  7.80614574e-06, -2.98740243e-05,\n",
       "        -5.99470219e-06, -3.26685807e-05,  2.36930532e-06,\n",
       "         9.68707318e-06],\n",
       "       [ 9.02263400e-06, -3.23051559e-05, -1.94197025e-05,\n",
       "         1.90684161e-05,  3.49917316e-05,  2.38621569e-05,\n",
       "        -3.42362412e-05, -1.35122603e-05,  1.68933446e-05,\n",
       "        -6.51281883e-05],\n",
       "       [-1.18529955e-04,  2.14123471e-06, -5.74995585e-05,\n",
       "        -4.72559227e-05,  3.15767764e-05, -1.22984032e-05,\n",
       "         4.32206764e-05,  6.42840339e-05, -2.32738712e-05,\n",
       "         4.78562932e-05],\n",
       "       [-1.40730455e-05, -1.25103880e-05, -3.83181360e-05,\n",
       "         1.19397138e-05,  1.96415456e-05, -5.43035844e-06,\n",
       "        -3.36231352e-06,  1.93307838e-05, -3.66223525e-06,\n",
       "        -1.64044722e-06],\n",
       "       [-5.04249620e-06, -3.89423083e-06, -8.69079527e-06,\n",
       "        -1.61752833e-05, -1.15914579e-05, -2.98608595e-05,\n",
       "         1.55633070e-05, -1.84821246e-05, -5.98450828e-06,\n",
       "         3.20749205e-05],\n",
       "       [-7.36451627e-05,  9.78016927e-07, -6.28510205e-05,\n",
       "        -3.52508465e-05,  1.13282062e-05, -2.58768792e-05,\n",
       "         5.15449045e-05,  2.60465266e-05, -1.97836623e-05,\n",
       "         7.35587586e-05],\n",
       "       [ 2.78896531e-04,  6.67979131e-05,  2.57450379e-04,\n",
       "         8.07475005e-05, -1.21275504e-05,  6.71331733e-05,\n",
       "        -9.80508484e-05, -1.57602812e-04,  4.42588288e-05,\n",
       "        -3.14165189e-04],\n",
       "       [-4.38346069e-05, -1.54164911e-05, -7.33170428e-05,\n",
       "        -2.83925743e-06,  1.76438235e-05, -3.97075685e-05,\n",
       "         1.24824480e-05, -5.01342672e-06, -1.03401963e-05,\n",
       "         3.93588103e-05],\n",
       "       [-2.07885909e-05,  7.70792183e-06, -3.95718773e-05,\n",
       "        -4.84602251e-06,  8.03327147e-06, -2.21208341e-05,\n",
       "         6.72799629e-06,  1.17881145e-05, -1.46521116e-05,\n",
       "         1.46649996e-05],\n",
       "       [ 8.03523741e-05,  1.00364765e-05,  7.57848376e-05,\n",
       "         4.33376851e-05, -1.22007589e-05,  5.34547899e-05,\n",
       "        -4.24394556e-05, -3.54201954e-05,  2.84059348e-05,\n",
       "        -1.39703379e-04],\n",
       "       [-1.24857406e-05,  1.24765719e-05, -4.32645040e-05,\n",
       "        -1.97293263e-05,  1.61208546e-05, -2.10021882e-05,\n",
       "         3.13485591e-05,  4.95186168e-05, -3.43991258e-05,\n",
       "         6.40007774e-05],\n",
       "       [ 1.57844534e-04,  2.51248510e-05,  1.28782066e-04,\n",
       "         4.54179901e-05, -4.12853610e-05,  8.79457832e-05,\n",
       "        -9.12039864e-05, -5.35222260e-05,  1.97279527e-05,\n",
       "        -2.00230526e-04],\n",
       "       [-3.66870413e-04, -1.17330374e-04, -3.41353600e-04,\n",
       "        -1.57231580e-04,  6.17213938e-05, -1.76613832e-04,\n",
       "         1.70120149e-04,  1.18297094e-05, -7.56335885e-05,\n",
       "         5.64256279e-04],\n",
       "       [ 1.75166047e-04,  1.32656917e-05,  1.91727912e-04,\n",
       "         6.69009535e-05, -1.35166955e-05,  1.53680433e-04,\n",
       "        -1.03763026e-04, -7.72055555e-05,  4.37226552e-05,\n",
       "        -2.81205147e-04],\n",
       "       [-8.87290471e-05,  4.31234990e-06, -1.41407484e-04,\n",
       "        -4.91351054e-05,  9.98277459e-06, -3.40414826e-05,\n",
       "         4.86408462e-05,  3.48173416e-05, -2.39935429e-05,\n",
       "         1.20144550e-04],\n",
       "       [ 8.42648058e-05,  3.01508855e-06,  3.98637217e-05,\n",
       "         4.40421133e-05,  6.41450125e-06,  9.69687941e-06,\n",
       "        -2.82609835e-05, -2.77629808e-05,  1.50806660e-05,\n",
       "        -7.88057840e-05],\n",
       "       [ 9.57673698e-05,  3.43280810e-05,  8.46545597e-05,\n",
       "         4.25125604e-05, -1.00911720e-05,  4.75466641e-05,\n",
       "        -2.60352043e-05, -2.90446950e-05, -5.02713142e-06,\n",
       "        -9.56473554e-05],\n",
       "       [-4.38760368e-06,  2.37644229e-05,  3.26723369e-05,\n",
       "        -8.08171135e-06, -3.54129381e-05,  7.28269065e-06,\n",
       "        -6.29011929e-06,  1.00117170e-05,  2.92564321e-05,\n",
       "        -1.64796690e-05],\n",
       "       [ 2.57642446e-05,  5.64175480e-06,  1.85376199e-05,\n",
       "         3.03103891e-06, -7.18271264e-06, -9.70033859e-06,\n",
       "        -7.79746282e-06, -3.64809648e-05,  1.22344892e-05,\n",
       "        -2.28102484e-05],\n",
       "       [-1.36816291e-04, -4.76123677e-05, -1.89386788e-04,\n",
       "        -4.05669584e-05,  5.91800723e-05, -3.04542979e-05,\n",
       "         3.77759494e-05,  2.17968762e-05, -5.10979048e-05,\n",
       "         1.71642797e-04],\n",
       "       [ 2.56342730e-05,  8.82755481e-06,  7.63468846e-06,\n",
       "         5.28700406e-06, -6.35164054e-06, -3.04241394e-06,\n",
       "        -1.00680574e-05, -2.47212906e-05, -2.04436580e-06,\n",
       "        -2.39254026e-05],\n",
       "       [ 3.07272727e-04,  2.13028790e-07,  3.97963119e-04,\n",
       "         1.91929195e-04,  2.04401260e-05,  2.19910809e-04,\n",
       "        -1.68290970e-04, -1.10781791e-04,  9.03735383e-05,\n",
       "        -4.65262536e-04],\n",
       "       [ 5.66298048e-05,  6.75350132e-06,  4.40564456e-05,\n",
       "         1.36366693e-05, -8.00603129e-06,  1.08505354e-05,\n",
       "        -1.71967097e-05,  4.69919666e-06,  1.00854105e-05,\n",
       "        -5.75822908e-05],\n",
       "       [-5.61382240e-05,  1.19614875e-05, -6.37930125e-05,\n",
       "        -4.52223396e-05,  5.84547719e-06, -2.99622388e-05,\n",
       "         4.67350760e-05,  3.71218107e-05, -1.65091297e-05,\n",
       "         7.44149423e-05],\n",
       "       [-4.05872351e-05, -4.81976293e-05, -1.22851637e-04,\n",
       "         3.10619422e-06,  6.78794253e-05, -5.73274597e-05,\n",
       "         3.04185044e-06, -2.36714599e-05, -7.42698864e-06,\n",
       "         8.66381873e-05],\n",
       "       [ 1.72951185e-04,  3.33889389e-05,  1.67198628e-04,\n",
       "         3.62796763e-05, -1.18903966e-06,  1.61148502e-05,\n",
       "        -1.14337061e-04, -7.87757461e-05,  2.42940715e-05,\n",
       "        -2.35771341e-04],\n",
       "       [ 3.44345426e-04,  2.14659747e-05,  3.47545020e-04,\n",
       "         1.42684729e-04,  2.25430429e-05,  2.31445580e-05,\n",
       "        -7.63039057e-05, -2.21914919e-04,  8.51975538e-05,\n",
       "        -3.11281823e-04],\n",
       "       [ 3.73938571e-05,  1.55352529e-06,  9.34600636e-06,\n",
       "         2.16577540e-05,  1.17062648e-05, -8.29685383e-06,\n",
       "        -4.04239230e-05, -2.90423098e-05,  1.24430413e-05,\n",
       "        -5.48781089e-05],\n",
       "       [-6.51415017e-05, -2.07924923e-06, -8.53372766e-05,\n",
       "        -2.27311703e-05,  4.78666524e-06, -6.00212641e-05,\n",
       "         1.09614139e-05,  1.70532362e-06,  5.82990881e-08,\n",
       "         4.49657157e-05],\n",
       "       [ 4.61761756e-05,  1.46721671e-05,  6.27233010e-05,\n",
       "         5.99226686e-06,  3.01414790e-05,  4.73536491e-05,\n",
       "        -3.08378026e-06, -2.63694061e-06,  7.76084113e-06,\n",
       "        -8.81661332e-05],\n",
       "       [ 5.81727786e-05, -2.72549085e-05,  2.85231480e-05,\n",
       "         1.81190155e-05,  2.82054837e-05,  1.73255171e-05,\n",
       "        -2.62420547e-05, -4.42747577e-05, -2.14810522e-07,\n",
       "        -5.39573392e-05],\n",
       "       [ 3.86212474e-05,  3.16117286e-05,  6.85368842e-05,\n",
       "        -1.58738999e-05, -1.05483957e-05,  2.42375624e-05,\n",
       "         1.47641024e-05,  4.45967302e-06,  2.67609869e-06,\n",
       "        -4.72181587e-05],\n",
       "       [ 3.12750429e-05,  2.41974598e-05,  6.01113558e-05,\n",
       "         2.51045118e-06, -2.31889641e-05,  1.71948381e-05,\n",
       "        -9.68920392e-06, -2.78309220e-05,  7.20787587e-06,\n",
       "        -4.18507214e-05],\n",
       "       [-5.61951488e-05, -5.07525278e-06, -6.12501739e-05,\n",
       "        -8.99114978e-06,  1.13046167e-05, -1.80545057e-05,\n",
       "         6.14321795e-06,  1.19590294e-05, -1.05104985e-05,\n",
       "         6.72343430e-05],\n",
       "       [ 1.60505448e-05, -3.52582118e-06,  1.72271347e-05,\n",
       "         6.14230407e-07, -1.15053465e-05,  7.65194643e-06,\n",
       "        -1.13394633e-05,  1.09569490e-05,  1.76573231e-06,\n",
       "        -1.25515722e-05],\n",
       "       [ 2.33991273e-04, -1.52182507e-05,  2.15108006e-04,\n",
       "         1.05420859e-04, -4.09175612e-05,  1.28951744e-04,\n",
       "        -1.40600590e-04, -1.04772641e-04,  7.39760321e-05,\n",
       "        -2.18940791e-04],\n",
       "       [-1.88357792e-05, -2.53842269e-05, -1.91523824e-05,\n",
       "        -3.68666829e-06,  4.23930558e-05,  1.27549248e-05,\n",
       "         1.17073726e-05,  1.70065759e-05,  5.72264245e-06,\n",
       "         4.65293634e-07],\n",
       "       [-2.40652582e-04, -9.44215460e-05, -2.29180366e-04,\n",
       "        -6.94949988e-05, -4.86107745e-06, -3.39320113e-05,\n",
       "         7.66955675e-05,  1.34475729e-04, -6.27288251e-05,\n",
       "         2.62100108e-04],\n",
       "       [-2.87597607e-04, -3.94467807e-05, -3.01509790e-04,\n",
       "        -1.51630394e-04,  6.66599128e-05, -1.34942604e-04,\n",
       "         1.54170375e-04,  1.05377261e-04, -9.80310840e-05,\n",
       "         4.55543881e-04],\n",
       "       [-1.87879466e-04,  1.36217281e-05, -1.78496006e-04,\n",
       "        -2.49411775e-05, -2.81186973e-05, -6.34873170e-05,\n",
       "         8.19193199e-05,  1.06422409e-04, -1.56228796e-05,\n",
       "         2.19188366e-04],\n",
       "       [-6.75195620e-05,  6.70008650e-06, -7.01728142e-05,\n",
       "        -2.17160014e-05,  8.81586001e-07, -1.34466819e-05,\n",
       "         2.74585814e-05,  2.77640475e-05, -1.71924497e-05,\n",
       "         6.40329193e-05],\n",
       "       [ 1.82327114e-04,  5.74203725e-06,  1.60738838e-04,\n",
       "         1.07618280e-04, -8.13117213e-06,  3.52458940e-05,\n",
       "        -1.18209479e-04, -6.27981239e-05,  3.54625674e-05,\n",
       "        -2.11571655e-04],\n",
       "       [-5.92755340e-05,  1.52530976e-05, -5.25719026e-05,\n",
       "        -3.40686537e-05,  5.02402965e-06, -1.67446721e-05,\n",
       "         5.16660334e-05,  4.07982164e-05, -1.12411406e-05,\n",
       "         6.65512109e-05],\n",
       "       [-1.29040877e-04, -5.52118694e-05, -1.45969146e-04,\n",
       "        -4.05217036e-05,  1.38903443e-05, -4.76377015e-05,\n",
       "         7.84747326e-05,  3.83243378e-05, -1.90802189e-05,\n",
       "         1.63953836e-04],\n",
       "       [ 1.68311559e-05, -1.44790978e-05, -1.82659443e-06,\n",
       "         2.80023249e-06, -2.25064047e-06, -1.19841749e-05,\n",
       "        -3.11265765e-06, -8.76707341e-06,  1.32880178e-05,\n",
       "         2.51438009e-05],\n",
       "       [ 5.05019909e-04,  9.54403642e-05,  7.13560459e-04,\n",
       "         1.97570000e-04, -2.46133119e-04,  1.95457902e-04,\n",
       "        -2.42407709e-04, -1.52796176e-04,  1.70069627e-04,\n",
       "        -6.25779041e-04],\n",
       "       [ 9.67070598e-05,  3.27364735e-05,  9.40203440e-05,\n",
       "         1.56680687e-05, -4.09931977e-05,  4.16098388e-05,\n",
       "        -3.44787049e-05, -1.97638500e-05, -6.20281359e-07,\n",
       "        -1.14821229e-04],\n",
       "       [-1.15139310e-04, -2.66096049e-05, -1.14284936e-04,\n",
       "        -5.68208525e-05,  2.79931845e-05, -3.24431066e-05,\n",
       "         5.88608841e-05,  3.48339055e-05, -1.50585425e-05,\n",
       "         9.91214679e-05],\n",
       "       [-1.22067326e-05,  3.41945529e-06, -1.31871215e-05,\n",
       "        -4.79154404e-06,  1.16439051e-05, -8.95742423e-06,\n",
       "         1.04467102e-05,  5.85296579e-06, -7.00054426e-06,\n",
       "         2.64868375e-05],\n",
       "       [-1.98767678e-04, -1.52066383e-05, -2.29126701e-04,\n",
       "        -1.03040760e-04, -2.54370305e-05, -9.26966131e-05,\n",
       "         9.82036059e-05,  8.23417437e-05, -8.30397785e-05,\n",
       "         2.96777684e-04],\n",
       "       [-6.56782726e-05,  4.85934313e-06, -5.81440484e-05,\n",
       "         1.06742065e-05, -3.57961155e-05, -4.87967597e-06,\n",
       "        -2.46471562e-05,  7.09085957e-05,  2.21597946e-05,\n",
       "         1.60663940e-05],\n",
       "       [ 4.11085866e-04,  9.29819776e-05,  7.48394894e-04,\n",
       "        -2.45262759e-05, -1.64430167e-04,  3.41417720e-04,\n",
       "         5.25844664e-05,  1.30652368e-04,  2.07829134e-05,\n",
       "        -3.60082114e-04],\n",
       "       [-3.90998653e-04, -1.74363539e-06, -3.76887464e-04,\n",
       "        -1.18729176e-04,  4.59961230e-06, -2.81040888e-04,\n",
       "         1.69727362e-04,  8.76817024e-05, -7.13826028e-05,\n",
       "         4.78618395e-04],\n",
       "       [-2.37423172e-04, -1.60218741e-05, -2.49157120e-04,\n",
       "        -6.46334916e-05,  1.71445175e-05, -1.03915622e-04,\n",
       "         1.11722422e-04,  1.92247562e-04, -6.96621970e-05,\n",
       "         3.21276432e-04],\n",
       "       [-1.16635435e-04, -2.78609515e-05, -1.60484495e-04,\n",
       "        -1.07984483e-05,  6.82397954e-05, -2.71073598e-05,\n",
       "         8.53171548e-05,  9.79325611e-05, -3.76931184e-05,\n",
       "         2.36919676e-04],\n",
       "       [-1.87795365e-04,  1.07554472e-05, -1.83844213e-04,\n",
       "        -9.88638447e-05,  2.08568697e-05, -6.71488207e-05,\n",
       "         8.21455517e-05,  9.16437316e-05, -2.18339512e-05,\n",
       "         1.76251013e-04],\n",
       "       [ 1.09377392e-04,  6.86288328e-06,  1.18206545e-04,\n",
       "         3.94756647e-05, -1.94431827e-05,  4.99804621e-05,\n",
       "        -5.14034123e-05, -4.57596176e-05,  2.90341587e-05,\n",
       "        -1.09343921e-04]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred-y#数字很小，证明会预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Torch的Tensor写神经网络\n",
    "\n",
    "先全复制上面代码\n",
    "\n",
    "把np.random改成torch\n",
    "\n",
    "把.dot()改成mm，即matrix Multiplication\n",
    "\n",
    "把max改成Clamp\n",
    "\n",
    "把loss中的square().sum()改成pow(2).sum()\n",
    "\n",
    "把.T改成.t.()\n",
    "\n",
    "cpoy()改成clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26885818.0\n",
      "1 22448052.0\n",
      "2 20989944.0\n",
      "3 19723376.0\n",
      "4 17423772.0\n",
      "5 13961550.0\n",
      "6 10161246.0\n",
      "7 6856897.0\n",
      "8 4455252.5\n",
      "9 2886399.25\n",
      "10 1922614.875\n",
      "11 1338861.75\n",
      "12 980932.0625\n",
      "13 753397.875\n",
      "14 601740.625\n",
      "15 495335.3125\n",
      "16 416653.9375\n",
      "17 355890.6875\n",
      "18 307334.53125\n",
      "19 267486.6875\n",
      "20 234244.3125\n",
      "21 206109.546875\n",
      "22 182081.8125\n",
      "23 161420.875\n",
      "24 143527.8125\n",
      "25 127950.5\n",
      "26 114347.515625\n",
      "27 102417.078125\n",
      "28 91918.484375\n",
      "29 82659.890625\n",
      "30 74483.3046875\n",
      "31 67234.53125\n",
      "32 60798.10546875\n",
      "33 55073.69921875\n",
      "34 49976.34765625\n",
      "35 45417.4765625\n",
      "36 41332.140625\n",
      "37 37665.2421875\n",
      "38 34369.93359375\n",
      "39 31403.318359375\n",
      "40 28725.955078125\n",
      "41 26305.66796875\n",
      "42 24116.2734375\n",
      "43 22136.712890625\n",
      "44 20339.1171875\n",
      "45 18706.779296875\n",
      "46 17221.123046875\n",
      "47 15865.9765625\n",
      "48 14629.68359375\n",
      "49 13500.9677734375\n",
      "50 12470.9296875\n",
      "51 11527.927734375\n",
      "52 10664.607421875\n",
      "53 9873.1015625\n",
      "54 9146.6474609375\n",
      "55 8478.8701171875\n",
      "56 7864.47509765625\n",
      "57 7299.1552734375\n",
      "58 6778.15625\n",
      "59 6297.68359375\n",
      "60 5854.30078125\n",
      "61 5444.955078125\n",
      "62 5066.888671875\n",
      "63 4717.39892578125\n",
      "64 4393.7177734375\n",
      "65 4093.858154296875\n",
      "66 3816.0302734375\n",
      "67 3558.558837890625\n",
      "68 3319.724609375\n",
      "69 3098.11181640625\n",
      "70 2892.38134765625\n",
      "71 2701.350341796875\n",
      "72 2523.83984375\n",
      "73 2358.6904296875\n",
      "74 2205.033447265625\n",
      "75 2062.018798828125\n",
      "76 1928.9127197265625\n",
      "77 1805.091552734375\n",
      "78 1689.6846923828125\n",
      "79 1582.148193359375\n",
      "80 1481.8231201171875\n",
      "81 1388.22119140625\n",
      "82 1300.916015625\n",
      "83 1219.328369140625\n",
      "84 1143.19287109375\n",
      "85 1072.054931640625\n",
      "86 1005.5802001953125\n",
      "87 943.4146728515625\n",
      "88 885.2779541015625\n",
      "89 830.922607421875\n",
      "90 780.0596923828125\n",
      "91 732.459716796875\n",
      "92 687.9547729492188\n",
      "93 646.3031005859375\n",
      "94 607.2721557617188\n",
      "95 570.7338256835938\n",
      "96 536.4779052734375\n",
      "97 504.38836669921875\n",
      "98 474.30078125\n",
      "99 446.07574462890625\n",
      "100 419.59075927734375\n",
      "101 394.75439453125\n",
      "102 371.4436340332031\n",
      "103 349.56591796875\n",
      "104 329.01904296875\n",
      "105 309.7321472167969\n",
      "106 291.68426513671875\n",
      "107 274.71551513671875\n",
      "108 258.76641845703125\n",
      "109 243.7901611328125\n",
      "110 229.70648193359375\n",
      "111 216.46141052246094\n",
      "112 204.00576782226562\n",
      "113 192.2908172607422\n",
      "114 181.2752685546875\n",
      "115 170.910888671875\n",
      "116 161.1585693359375\n",
      "117 151.97515869140625\n",
      "118 143.33233642578125\n",
      "119 135.20413208007812\n",
      "120 127.54498291015625\n",
      "121 120.33467102050781\n",
      "122 113.5423583984375\n",
      "123 107.15048217773438\n",
      "124 101.12294006347656\n",
      "125 95.44706726074219\n",
      "126 90.10454559326172\n",
      "127 85.06744384765625\n",
      "128 80.32148742675781\n",
      "129 75.84513092041016\n",
      "130 71.62690734863281\n",
      "131 67.64913177490234\n",
      "132 63.89927673339844\n",
      "133 60.3618049621582\n",
      "134 57.02525329589844\n",
      "135 53.87554931640625\n",
      "136 50.907745361328125\n",
      "137 48.106849670410156\n",
      "138 45.46348571777344\n",
      "139 42.969200134277344\n",
      "140 40.61578369140625\n",
      "141 38.394447326660156\n",
      "142 36.29438781738281\n",
      "143 34.31268310546875\n",
      "144 32.44341278076172\n",
      "145 30.677379608154297\n",
      "146 29.0107364654541\n",
      "147 27.435710906982422\n",
      "148 25.94955062866211\n",
      "149 24.543899536132812\n",
      "150 23.216636657714844\n",
      "151 21.96267318725586\n",
      "152 20.778858184814453\n",
      "153 19.65972900390625\n",
      "154 18.600921630859375\n",
      "155 17.601390838623047\n",
      "156 16.65683937072754\n",
      "157 15.763557434082031\n",
      "158 14.919206619262695\n",
      "159 14.120719909667969\n",
      "160 13.36607551574707\n",
      "161 12.652515411376953\n",
      "162 11.97789192199707\n",
      "163 11.340072631835938\n",
      "164 10.73702335357666\n",
      "165 10.166252136230469\n",
      "166 9.626663208007812\n",
      "167 9.115848541259766\n",
      "168 8.633050918579102\n",
      "169 8.176336288452148\n",
      "170 7.743907451629639\n",
      "171 7.334582328796387\n",
      "172 6.947877883911133\n",
      "173 6.581592559814453\n",
      "174 6.234961986541748\n",
      "175 5.906986236572266\n",
      "176 5.596206188201904\n",
      "177 5.302419662475586\n",
      "178 5.024519443511963\n",
      "179 4.76141357421875\n",
      "180 4.512053489685059\n",
      "181 4.276048183441162\n",
      "182 4.052499771118164\n",
      "183 3.8410263061523438\n",
      "184 3.6405410766601562\n",
      "185 3.450712203979492\n",
      "186 3.271005630493164\n",
      "187 3.100829601287842\n",
      "188 2.9394524097442627\n",
      "189 2.7868399620056152\n",
      "190 2.642482042312622\n",
      "191 2.5052926540374756\n",
      "192 2.3755221366882324\n",
      "193 2.252610683441162\n",
      "194 2.1362743377685547\n",
      "195 2.0258944034576416\n",
      "196 1.9212861061096191\n",
      "197 1.822141170501709\n",
      "198 1.7283161878585815\n",
      "199 1.6391750574111938\n",
      "200 1.5548067092895508\n",
      "201 1.4747917652130127\n",
      "202 1.3990087509155273\n",
      "203 1.3272572755813599\n",
      "204 1.2591651678085327\n",
      "205 1.1945185661315918\n",
      "206 1.133415699005127\n",
      "207 1.0753471851348877\n",
      "208 1.0202850103378296\n",
      "209 0.9682266116142273\n",
      "210 0.9187799692153931\n",
      "211 0.8718628883361816\n",
      "212 0.8274608850479126\n",
      "213 0.7852531671524048\n",
      "214 0.7453271150588989\n",
      "215 0.7072873115539551\n",
      "216 0.6713693737983704\n",
      "217 0.6372544169425964\n",
      "218 0.6048963069915771\n",
      "219 0.5742229223251343\n",
      "220 0.5451149940490723\n",
      "221 0.5173963904380798\n",
      "222 0.49119678139686584\n",
      "223 0.46637290716171265\n",
      "224 0.4428400993347168\n",
      "225 0.4204491972923279\n",
      "226 0.3992394804954529\n",
      "227 0.37905484437942505\n",
      "228 0.35992082953453064\n",
      "229 0.3417799770832062\n",
      "230 0.3245704174041748\n",
      "231 0.30820542573928833\n",
      "232 0.29274553060531616\n",
      "233 0.2780424654483795\n",
      "234 0.2640273869037628\n",
      "235 0.25083568692207336\n",
      "236 0.2382015585899353\n",
      "237 0.22624973952770233\n",
      "238 0.214929461479187\n",
      "239 0.20417186617851257\n",
      "240 0.19392557442188263\n",
      "241 0.18420490622520447\n",
      "242 0.17504358291625977\n",
      "243 0.16626355051994324\n",
      "244 0.15796534717082977\n",
      "245 0.1501064896583557\n",
      "246 0.14261147379875183\n",
      "247 0.1354852318763733\n",
      "248 0.12877608835697174\n",
      "249 0.1223558783531189\n",
      "250 0.11625600606203079\n",
      "251 0.11046242713928223\n",
      "252 0.10495173186063766\n",
      "253 0.09974721074104309\n",
      "254 0.09478369355201721\n",
      "255 0.09009978920221329\n",
      "256 0.08563077449798584\n",
      "257 0.08138474822044373\n",
      "258 0.07736599445343018\n",
      "259 0.0735420286655426\n",
      "260 0.06987205147743225\n",
      "261 0.06643526256084442\n",
      "262 0.06314443051815033\n",
      "263 0.06005118787288666\n",
      "264 0.057079508900642395\n",
      "265 0.0542580708861351\n",
      "266 0.05159661918878555\n",
      "267 0.04904620349407196\n",
      "268 0.04663892090320587\n",
      "269 0.04431639611721039\n",
      "270 0.042144525796175\n",
      "271 0.040100641548633575\n",
      "272 0.03811813145875931\n",
      "273 0.03625642880797386\n",
      "274 0.034477412700653076\n",
      "275 0.032790541648864746\n",
      "276 0.03118031658232212\n",
      "277 0.029649533331394196\n",
      "278 0.028194740414619446\n",
      "279 0.026833321899175644\n",
      "280 0.02552158758044243\n",
      "281 0.02427765727043152\n",
      "282 0.023089619353413582\n",
      "283 0.02197318896651268\n",
      "284 0.020911797881126404\n",
      "285 0.019902389496564865\n",
      "286 0.018936121836304665\n",
      "287 0.018016736954450607\n",
      "288 0.017155174165964127\n",
      "289 0.01632542535662651\n",
      "290 0.015534520149230957\n",
      "291 0.014787431806325912\n",
      "292 0.014073066413402557\n",
      "293 0.013395072892308235\n",
      "294 0.012755857780575752\n",
      "295 0.01214353647083044\n",
      "296 0.011562390252947807\n",
      "297 0.011013947427272797\n",
      "298 0.01048269309103489\n",
      "299 0.009984356351196766\n",
      "300 0.009512731805443764\n",
      "301 0.00906376913189888\n",
      "302 0.008637085556983948\n",
      "303 0.008227705955505371\n",
      "304 0.007843740284442902\n",
      "305 0.007481634616851807\n",
      "306 0.0071281855925917625\n",
      "307 0.006797470152378082\n",
      "308 0.006482982542365789\n",
      "309 0.006177556701004505\n",
      "310 0.005892973393201828\n",
      "311 0.005621479824185371\n",
      "312 0.00536494143307209\n",
      "313 0.005119136068969965\n",
      "314 0.004885777831077576\n",
      "315 0.004666009917855263\n",
      "316 0.004456597380340099\n",
      "317 0.004256889224052429\n",
      "318 0.004062395542860031\n",
      "319 0.003885496873408556\n",
      "320 0.003710656426846981\n",
      "321 0.003544274717569351\n",
      "322 0.0033898567780852318\n",
      "323 0.0032376679591834545\n",
      "324 0.0030998089350759983\n",
      "325 0.0029649813659489155\n",
      "326 0.002836429513990879\n",
      "327 0.0027167184744030237\n",
      "328 0.0025986209511756897\n",
      "329 0.0024902820587158203\n",
      "330 0.002383674494922161\n",
      "331 0.00228361738845706\n",
      "332 0.0021872082725167274\n",
      "333 0.002095729112625122\n",
      "334 0.002008659765124321\n",
      "335 0.0019273910438641906\n",
      "336 0.0018492185045033693\n",
      "337 0.001776941237039864\n",
      "338 0.001706666429527104\n",
      "339 0.001637644600123167\n",
      "340 0.0015734316548332572\n",
      "341 0.0015106183709576726\n",
      "342 0.0014520366676151752\n",
      "343 0.001395265106111765\n",
      "344 0.0013409770326688886\n",
      "345 0.001290747313760221\n",
      "346 0.0012414772063493729\n",
      "347 0.0011946067679673433\n",
      "348 0.001149351941421628\n",
      "349 0.001107312273234129\n",
      "350 0.0010671330383047462\n",
      "351 0.001029145554639399\n",
      "352 0.000990997301414609\n",
      "353 0.0009549164096824825\n",
      "354 0.0009226185502484441\n",
      "355 0.0008906993316486478\n",
      "356 0.0008589081116952002\n",
      "357 0.0008291028207167983\n",
      "358 0.000800505222287029\n",
      "359 0.0007729446515440941\n",
      "360 0.0007471702992916107\n",
      "361 0.0007221632404252887\n",
      "362 0.0006971188704483211\n",
      "363 0.000674994254950434\n",
      "364 0.0006537347799167037\n",
      "365 0.0006322297849692404\n",
      "366 0.0006129438406787813\n",
      "367 0.000593475648202002\n",
      "368 0.0005749046104028821\n",
      "369 0.0005569719360210001\n",
      "370 0.0005384093383327127\n",
      "371 0.0005231055547483265\n",
      "372 0.0005069100297987461\n",
      "373 0.0004913673037663102\n",
      "374 0.0004764751356560737\n",
      "375 0.0004634188371710479\n",
      "376 0.00044844031799584627\n",
      "377 0.0004351986281108111\n",
      "378 0.00042203039629384875\n",
      "379 0.0004101867089048028\n",
      "380 0.0003986756782978773\n",
      "381 0.0003874095855280757\n",
      "382 0.00037644003168679774\n",
      "383 0.0003660954535007477\n",
      "384 0.00035563757410272956\n",
      "385 0.00034556403988972306\n",
      "386 0.0003369329497218132\n",
      "387 0.0003273713227827102\n",
      "388 0.0003191727737430483\n",
      "389 0.0003107571101281792\n",
      "390 0.00030223559588193893\n",
      "391 0.00029448411078192294\n",
      "392 0.0002873355115298182\n",
      "393 0.0002793599560391158\n",
      "394 0.0002724254736676812\n",
      "395 0.0002654820855241269\n",
      "396 0.00025863636983558536\n",
      "397 0.0002535130479373038\n",
      "398 0.0002468142774887383\n",
      "399 0.0002403703547315672\n",
      "400 0.00023501735995523632\n",
      "401 0.00022981807705946267\n",
      "402 0.00022384097974281758\n",
      "403 0.00021883298177272081\n",
      "404 0.0002137023548129946\n",
      "405 0.00020852047600783408\n",
      "406 0.00020370948186609894\n",
      "407 0.00019913060532417148\n",
      "408 0.00019456000882200897\n",
      "409 0.00019006640650331974\n",
      "410 0.00018576315778773278\n",
      "411 0.000181891635293141\n",
      "412 0.00017784026567824185\n",
      "413 0.00017410160216968507\n",
      "414 0.00017002425738610327\n",
      "415 0.00016706445603631437\n",
      "416 0.00016300019342452288\n",
      "417 0.00015971441462170333\n",
      "418 0.0001567681465530768\n",
      "419 0.00015363932470791042\n",
      "420 0.00015017905388958752\n",
      "421 0.00014717213343828917\n",
      "422 0.0001441390486434102\n",
      "423 0.00014118773106019944\n",
      "424 0.0001385754585498944\n",
      "425 0.00013616698561236262\n",
      "426 0.00013307899644132704\n",
      "427 0.00013063102960586548\n",
      "428 0.00012824036821257323\n",
      "429 0.00012590712867677212\n",
      "430 0.00012316412176005542\n",
      "431 0.00012098150909878314\n",
      "432 0.00011854170588776469\n",
      "433 0.00011619395081652328\n",
      "434 0.00011410873412387446\n",
      "435 0.00011215345875825733\n",
      "436 0.00011011813330696896\n",
      "437 0.0001080787624232471\n",
      "438 0.00010624596325214952\n",
      "439 0.00010419073078082874\n",
      "440 0.00010220423428108916\n",
      "441 0.00010068417759612203\n",
      "442 9.90300759440288e-05\n",
      "443 9.753296035341918e-05\n",
      "444 9.613260044716299e-05\n",
      "445 9.404131560586393e-05\n",
      "446 9.267406130675226e-05\n",
      "447 9.094660344999284e-05\n",
      "448 8.947528840508312e-05\n",
      "449 8.78087303135544e-05\n",
      "450 8.647456706967205e-05\n",
      "451 8.517269452568144e-05\n",
      "452 8.345847891177982e-05\n",
      "453 8.22541187517345e-05\n",
      "454 8.110667113214731e-05\n",
      "455 7.983933755895123e-05\n",
      "456 7.867951353546232e-05\n",
      "457 7.739372085779905e-05\n",
      "458 7.632362394360825e-05\n",
      "459 7.504860695917159e-05\n",
      "460 7.392243423964828e-05\n",
      "461 7.275167445186526e-05\n",
      "462 7.139062654459849e-05\n",
      "463 7.040031778160483e-05\n",
      "464 6.956778815947473e-05\n",
      "465 6.878140266053379e-05\n",
      "466 6.788782047806308e-05\n",
      "467 6.679721991531551e-05\n",
      "468 6.573844439117238e-05\n",
      "469 6.504826887976378e-05\n",
      "470 6.396082608262077e-05\n",
      "471 6.307764851953834e-05\n",
      "472 6.226375990081578e-05\n",
      "473 6.162236240925267e-05\n",
      "474 6.086257781134918e-05\n",
      "475 5.990763020236045e-05\n",
      "476 5.9176374634262174e-05\n",
      "477 5.8394110965309665e-05\n",
      "478 5.770874849986285e-05\n",
      "479 5.682613482349552e-05\n",
      "480 5.60977277928032e-05\n",
      "481 5.548647095565684e-05\n",
      "482 5.477106606122106e-05\n",
      "483 5.388523277360946e-05\n",
      "484 5.3211377235129476e-05\n",
      "485 5.271168993203901e-05\n",
      "486 5.183641769690439e-05\n",
      "487 5.1452512707328424e-05\n",
      "488 5.052233609603718e-05\n",
      "489 4.9885507905855775e-05\n",
      "490 4.944368629367091e-05\n",
      "491 4.852381243836135e-05\n",
      "492 4.8026857257355005e-05\n",
      "493 4.7243029257515445e-05\n",
      "494 4.674046067520976e-05\n",
      "495 4.624128632713109e-05\n",
      "496 4.570707824314013e-05\n",
      "497 4.510518556344323e-05\n",
      "498 4.4630622141994536e-05\n",
      "499 4.412285124999471e-05\n",
      "运行时间1 7.478307008743286\n",
      "运行时间2 7.478216886520386\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "time3=time.time()\n",
    "\n",
    "dtype = torch.float\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:2\") # Uncomment this to run on GPU\n",
    "time1=time.time()\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "\n",
    "time2=time.time()\n",
    "print('运行时间1',(time2-time3))\n",
    "print('运行时间2',(time2-time1))\n",
    "#因为网络的层数太潜了，而Tensor和model搬到GPU都是需要花费时间的。所以第一次运行可能慢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31278110.0\n",
      "1 26692464.0\n",
      "2 24068524.0\n",
      "3 20323956.0\n",
      "4 15489600.0\n",
      "5 10524489.0\n",
      "6 6685817.5\n",
      "7 4118059.25\n",
      "8 2596228.25\n",
      "9 1713848.0\n",
      "10 1203708.0\n",
      "11 895102.625\n",
      "12 698473.375\n",
      "13 564609.75\n",
      "14 468121.6875\n",
      "15 395108.6875\n",
      "16 337729.40625\n",
      "17 291369.5\n",
      "18 253225.40625\n",
      "19 221379.09375\n",
      "20 194491.53125\n",
      "21 171602.53125\n",
      "22 151980.8125\n",
      "23 135068.71875\n",
      "24 120412.6484375\n",
      "25 107656.53125\n",
      "26 96512.234375\n",
      "27 86747.890625\n",
      "28 78159.03125\n",
      "29 70572.8125\n",
      "30 63852.60546875\n",
      "31 57895.51171875\n",
      "32 52594.7578125\n",
      "33 47857.03125\n",
      "34 43615.51953125\n",
      "35 39817.9375\n",
      "36 36403.49609375\n",
      "37 33327.96875\n",
      "38 30552.75\n",
      "39 28043.25390625\n",
      "40 25771.333984375\n",
      "41 23711.0\n",
      "42 21840.119140625\n",
      "43 20136.677734375\n",
      "44 18582.69921875\n",
      "45 17164.673828125\n",
      "46 15868.615234375\n",
      "47 14683.666015625\n",
      "48 13597.314453125\n",
      "49 12601.453125\n",
      "50 11686.583984375\n",
      "51 10848.03125\n",
      "52 10076.689453125\n",
      "53 9366.8203125\n",
      "54 8712.4365234375\n",
      "55 8109.5830078125\n",
      "56 7553.33642578125\n",
      "57 7039.5390625\n",
      "58 6564.11572265625\n",
      "59 6124.05859375\n",
      "60 5716.1376953125\n",
      "61 5338.2978515625\n",
      "62 4987.580078125\n",
      "63 4661.97265625\n",
      "64 4359.5078125\n",
      "65 4078.599853515625\n",
      "66 3817.31103515625\n",
      "67 3574.09521484375\n",
      "68 3347.79736328125\n",
      "69 3136.9619140625\n",
      "70 2940.466796875\n",
      "71 2757.238037109375\n",
      "72 2586.2236328125\n",
      "73 2426.56689453125\n",
      "74 2277.537353515625\n",
      "75 2138.212890625\n",
      "76 2008.037353515625\n",
      "77 1886.4080810546875\n",
      "78 1772.7030029296875\n",
      "79 1666.327880859375\n",
      "80 1566.7489013671875\n",
      "81 1473.4940185546875\n",
      "82 1386.097412109375\n",
      "83 1304.248291015625\n",
      "84 1227.5029296875\n",
      "85 1155.50732421875\n",
      "86 1087.9739990234375\n",
      "87 1024.6351318359375\n",
      "88 965.1836547851562\n",
      "89 909.34619140625\n",
      "90 856.9057006835938\n",
      "91 807.6472778320312\n",
      "92 761.3499145507812\n",
      "93 717.830078125\n",
      "94 676.9386596679688\n",
      "95 638.474853515625\n",
      "96 602.3104858398438\n",
      "97 568.2840576171875\n",
      "98 536.2617797851562\n",
      "99 506.1307067871094\n",
      "100 477.751220703125\n",
      "101 451.03387451171875\n",
      "102 425.8875732421875\n",
      "103 402.19097900390625\n",
      "104 379.86102294921875\n",
      "105 358.8179626464844\n",
      "106 338.9952392578125\n",
      "107 320.3108215332031\n",
      "108 302.694091796875\n",
      "109 286.0963134765625\n",
      "110 270.43408203125\n",
      "111 255.66046142578125\n",
      "112 241.71722412109375\n",
      "113 228.56613159179688\n",
      "114 216.15927124023438\n",
      "115 204.4482421875\n",
      "116 193.38836669921875\n",
      "117 182.94857788085938\n",
      "118 173.09645080566406\n",
      "119 163.78817749023438\n",
      "120 154.9959716796875\n",
      "121 146.69302368164062\n",
      "122 138.84942626953125\n",
      "123 131.43292236328125\n",
      "124 124.4269027709961\n",
      "125 117.80461120605469\n",
      "126 111.55013275146484\n",
      "127 105.63438415527344\n",
      "128 100.04136657714844\n",
      "129 94.75403594970703\n",
      "130 89.75499725341797\n",
      "131 85.02518463134766\n",
      "132 80.55211639404297\n",
      "133 76.32051086425781\n",
      "134 72.31900024414062\n",
      "135 68.53274536132812\n",
      "136 64.94873046875\n",
      "137 61.557106018066406\n",
      "138 58.34825897216797\n",
      "139 55.31066131591797\n",
      "140 52.43510437011719\n",
      "141 49.712608337402344\n",
      "142 47.13682556152344\n",
      "143 44.697357177734375\n",
      "144 42.38776779174805\n",
      "145 40.20033264160156\n",
      "146 38.12700653076172\n",
      "147 36.16463851928711\n",
      "148 34.304359436035156\n",
      "149 32.54306411743164\n",
      "150 30.87376594543457\n",
      "151 29.29347801208496\n",
      "152 27.794509887695312\n",
      "153 26.373836517333984\n",
      "154 25.02815818786621\n",
      "155 23.75223159790039\n",
      "156 22.543481826782227\n",
      "157 21.39694595336914\n",
      "158 20.309959411621094\n",
      "159 19.279611587524414\n",
      "160 18.302988052368164\n",
      "161 17.37668800354004\n",
      "162 16.497480392456055\n",
      "163 15.664814949035645\n",
      "164 14.874544143676758\n",
      "165 14.124977111816406\n",
      "166 13.414009094238281\n",
      "167 12.739497184753418\n",
      "168 12.099786758422852\n",
      "169 11.492589950561523\n",
      "170 10.916963577270508\n",
      "171 10.370235443115234\n",
      "172 9.851521492004395\n",
      "173 9.359223365783691\n",
      "174 8.891862869262695\n",
      "175 8.448526382446289\n",
      "176 8.027585983276367\n",
      "177 7.628319263458252\n",
      "178 7.249027252197266\n",
      "179 6.888842582702637\n",
      "180 6.5469207763671875\n",
      "181 6.222511291503906\n",
      "182 5.914306163787842\n",
      "183 5.621703147888184\n",
      "184 5.343696117401123\n",
      "185 5.079861640930176\n",
      "186 4.829256057739258\n",
      "187 4.591001510620117\n",
      "188 4.365149974822998\n",
      "189 4.150082588195801\n",
      "190 3.9460439682006836\n",
      "191 3.7521884441375732\n",
      "192 3.5680437088012695\n",
      "193 3.393103837966919\n",
      "194 3.2269232273101807\n",
      "195 3.0689361095428467\n",
      "196 2.9187369346618652\n",
      "197 2.7761926651000977\n",
      "198 2.640446186065674\n",
      "199 2.5117321014404297\n",
      "200 2.3892250061035156\n",
      "201 2.2729389667510986\n",
      "202 2.1622867584228516\n",
      "203 2.0571417808532715\n",
      "204 1.9571059942245483\n",
      "205 1.862009882926941\n",
      "206 1.7717219591140747\n",
      "207 1.6858046054840088\n",
      "208 1.6040723323822021\n",
      "209 1.5264744758605957\n",
      "210 1.4526005983352661\n",
      "211 1.3823721408843994\n",
      "212 1.3155796527862549\n",
      "213 1.2520220279693604\n",
      "214 1.1916779279708862\n",
      "215 1.1342109441757202\n",
      "216 1.0795578956604004\n",
      "217 1.0275905132293701\n",
      "218 0.9782535433769226\n",
      "219 0.93116694688797\n",
      "220 0.8865249156951904\n",
      "221 0.8439211845397949\n",
      "222 0.8034354448318481\n",
      "223 0.7649391889572144\n",
      "224 0.7283099293708801\n",
      "225 0.6934255361557007\n",
      "226 0.6602432727813721\n",
      "227 0.6286658048629761\n",
      "228 0.5986399054527283\n",
      "229 0.570125937461853\n",
      "230 0.542849600315094\n",
      "231 0.5169662237167358\n",
      "232 0.49231719970703125\n",
      "233 0.46892115473747253\n",
      "234 0.44660666584968567\n",
      "235 0.4253350794315338\n",
      "236 0.4050767719745636\n",
      "237 0.385840505361557\n",
      "238 0.3675307631492615\n",
      "239 0.3501076102256775\n",
      "240 0.3335043787956238\n",
      "241 0.3177182078361511\n",
      "242 0.30265623331069946\n",
      "243 0.28830432891845703\n",
      "244 0.27466437220573425\n",
      "245 0.26168376207351685\n",
      "246 0.2493172436952591\n",
      "247 0.2375320941209793\n",
      "248 0.22629067301750183\n",
      "249 0.2156495451927185\n",
      "250 0.2054307460784912\n",
      "251 0.19577616453170776\n",
      "252 0.1865200698375702\n",
      "253 0.17774684727191925\n",
      "254 0.16940857470035553\n",
      "255 0.16142529249191284\n",
      "256 0.15385109186172485\n",
      "257 0.14660371840000153\n",
      "258 0.13974149525165558\n",
      "259 0.13317367434501648\n",
      "260 0.12689055502414703\n",
      "261 0.12095960974693298\n",
      "262 0.11529170721769333\n",
      "263 0.10990676283836365\n",
      "264 0.1047549620270729\n",
      "265 0.09986253082752228\n",
      "266 0.09518477320671082\n",
      "267 0.0907287448644638\n",
      "268 0.08648419380187988\n",
      "269 0.0824427604675293\n",
      "270 0.07859674841165543\n",
      "271 0.0749344527721405\n",
      "272 0.07141295075416565\n",
      "273 0.06810802221298218\n",
      "274 0.0649033784866333\n",
      "275 0.06188777834177017\n",
      "276 0.058990851044654846\n",
      "277 0.0562698170542717\n",
      "278 0.053650956600904465\n",
      "279 0.05117259919643402\n",
      "280 0.048790886998176575\n",
      "281 0.04653503745794296\n",
      "282 0.04437742382287979\n",
      "283 0.042306460440158844\n",
      "284 0.04036250337958336\n",
      "285 0.03848094120621681\n",
      "286 0.036707013845443726\n",
      "287 0.035006117075681686\n",
      "288 0.03339463472366333\n",
      "289 0.03184817358851433\n",
      "290 0.030376750975847244\n",
      "291 0.028984082862734795\n",
      "292 0.027653895318508148\n",
      "293 0.026383332908153534\n",
      "294 0.025165177881717682\n",
      "295 0.024004656821489334\n",
      "296 0.022904828190803528\n",
      "297 0.021860290318727493\n",
      "298 0.020856019109487534\n",
      "299 0.019898518919944763\n",
      "300 0.018989916890859604\n",
      "301 0.018124835565686226\n",
      "302 0.017302580177783966\n",
      "303 0.016504086554050446\n",
      "304 0.015753867104649544\n",
      "305 0.015032260678708553\n",
      "306 0.014347209595143795\n",
      "307 0.013701347634196281\n",
      "308 0.01307952031493187\n",
      "309 0.01249920204281807\n",
      "310 0.011935154907405376\n",
      "311 0.011397910304367542\n",
      "312 0.010875740088522434\n",
      "313 0.010385229252278805\n",
      "314 0.00992918387055397\n",
      "315 0.009477568790316582\n",
      "316 0.009060937911272049\n",
      "317 0.008654169738292694\n",
      "318 0.008270720951259136\n",
      "319 0.007902280427515507\n",
      "320 0.007551988586783409\n",
      "321 0.007222934626042843\n",
      "322 0.006902267225086689\n",
      "323 0.0065948497503995895\n",
      "324 0.006309588439762592\n",
      "325 0.006032219156622887\n",
      "326 0.005761837121099234\n",
      "327 0.005515276454389095\n",
      "328 0.005279257893562317\n",
      "329 0.005046503152698278\n",
      "330 0.004833413287997246\n",
      "331 0.004622019827365875\n",
      "332 0.004425703082233667\n",
      "333 0.0042327530682086945\n",
      "334 0.00405543576925993\n",
      "335 0.0038780048489570618\n",
      "336 0.0037187945563346148\n",
      "337 0.0035624117590487003\n",
      "338 0.0034171948209404945\n",
      "339 0.0032771111000329256\n",
      "340 0.003136399667710066\n",
      "341 0.003006731392815709\n",
      "342 0.0028821711894124746\n",
      "343 0.0027656813617795706\n",
      "344 0.0026532753836363554\n",
      "345 0.002548310672864318\n",
      "346 0.002444931771606207\n",
      "347 0.0023476649075746536\n",
      "348 0.0022534611634910107\n",
      "349 0.0021660327911376953\n",
      "350 0.0020823231898248196\n",
      "351 0.001998830121010542\n",
      "352 0.0019197050714865327\n",
      "353 0.0018471672665327787\n",
      "354 0.0017751696286723018\n",
      "355 0.0017075205687433481\n",
      "356 0.001641164068132639\n",
      "357 0.0015818825922906399\n",
      "358 0.0015220505883917212\n",
      "359 0.0014642763417214155\n",
      "360 0.0014117377577349544\n",
      "361 0.0013592977775260806\n",
      "362 0.0013118931092321873\n",
      "363 0.001261826022528112\n",
      "364 0.0012169468682259321\n",
      "365 0.0011745125520974398\n",
      "366 0.0011335656745359302\n",
      "367 0.0010931594297289848\n",
      "368 0.0010545817203819752\n",
      "369 0.0010172553593292832\n",
      "370 0.0009845253080129623\n",
      "371 0.0009486152557656169\n",
      "372 0.000917986617423594\n",
      "373 0.0008860814850777388\n",
      "374 0.000857355073094368\n",
      "375 0.0008285448420792818\n",
      "376 0.00080005859490484\n",
      "377 0.0007748962379992008\n",
      "378 0.00074920424958691\n",
      "379 0.0007235694210976362\n",
      "380 0.0007007925305515528\n",
      "381 0.0006789408507756889\n",
      "382 0.0006577085005119443\n",
      "383 0.0006371966446749866\n",
      "384 0.0006184403318911791\n",
      "385 0.000600180123001337\n",
      "386 0.0005814246251247823\n",
      "387 0.0005638154689222574\n",
      "388 0.0005462300614453852\n",
      "389 0.0005309452535584569\n",
      "390 0.0005160372820682824\n",
      "391 0.0005005557904951274\n",
      "392 0.0004860973567701876\n",
      "393 0.0004725998733192682\n",
      "394 0.00045873012277297676\n",
      "395 0.0004460770287550986\n",
      "396 0.0004330684896558523\n",
      "397 0.0004217115929350257\n",
      "398 0.00040979747427627444\n",
      "399 0.00039824366103857756\n",
      "400 0.00038819931796751916\n",
      "401 0.0003778650425374508\n",
      "402 0.0003686481504701078\n",
      "403 0.0003584428923204541\n",
      "404 0.00034880146267823875\n",
      "405 0.00033978105057030916\n",
      "406 0.00033025824814103544\n",
      "407 0.00032239779829978943\n",
      "408 0.00031366682378575206\n",
      "409 0.0003061798634007573\n",
      "410 0.00029807331156916916\n",
      "411 0.0002909775357693434\n",
      "412 0.00028376508271321654\n",
      "413 0.0002765911631286144\n",
      "414 0.00027044612215831876\n",
      "415 0.00026411586441099644\n",
      "416 0.0002580751897767186\n",
      "417 0.00025090135750360787\n",
      "418 0.0002452785265631974\n",
      "419 0.0002397997595835477\n",
      "420 0.00023369601694867015\n",
      "421 0.00022866028302814811\n",
      "422 0.00022315100068226457\n",
      "423 0.00021875911625102162\n",
      "424 0.0002135448739863932\n",
      "425 0.00020913434855174273\n",
      "426 0.00020453750039450824\n",
      "427 0.00020006940758321434\n",
      "428 0.0001954608305823058\n",
      "429 0.0001912320003611967\n",
      "430 0.00018772011389955878\n",
      "431 0.0001833874557632953\n",
      "432 0.00017890964227262884\n",
      "433 0.00017582879809197038\n",
      "434 0.00017190500511787832\n",
      "435 0.00016862672055140138\n",
      "436 0.00016487311222590506\n",
      "437 0.00016176048666238785\n",
      "438 0.0001587372098583728\n",
      "439 0.0001558146468596533\n",
      "440 0.00015196498134173453\n",
      "441 0.00014919444220140576\n",
      "442 0.00014651913079433143\n",
      "443 0.00014360385830514133\n",
      "444 0.00014116347301751375\n",
      "445 0.00013846243382431567\n",
      "446 0.0001357597066089511\n",
      "447 0.00013365776976570487\n",
      "448 0.0001310668303631246\n",
      "449 0.0001288137282244861\n",
      "450 0.00012645329115912318\n",
      "451 0.00012398610124364495\n",
      "452 0.00012145459913881496\n",
      "453 0.00011904849816346541\n",
      "454 0.00011753969010896981\n",
      "455 0.00011524432920850813\n",
      "456 0.00011348997941240668\n",
      "457 0.00011105392331955954\n",
      "458 0.00010872595885302871\n",
      "459 0.00010695194941945374\n",
      "460 0.00010492866567801684\n",
      "461 0.00010313956590835005\n",
      "462 0.00010140247468370944\n",
      "463 9.977087029255927e-05\n",
      "464 9.785294241737574e-05\n",
      "465 9.620923810871318e-05\n",
      "466 9.489752119407058e-05\n",
      "467 9.367479651700705e-05\n",
      "468 9.189090633299202e-05\n",
      "469 9.043232421390712e-05\n",
      "470 8.918123785406351e-05\n",
      "471 8.764256199356169e-05\n",
      "472 8.618470747023821e-05\n",
      "473 8.483654528390616e-05\n",
      "474 8.34110687719658e-05\n",
      "475 8.213270484702662e-05\n",
      "476 8.101262210402638e-05\n",
      "477 7.953072781674564e-05\n",
      "478 7.819909660611302e-05\n",
      "479 7.728465425316244e-05\n",
      "480 7.597431977046654e-05\n",
      "481 7.507075497414917e-05\n",
      "482 7.38995659048669e-05\n",
      "483 7.261838618433103e-05\n",
      "484 7.184757123468444e-05\n",
      "485 7.069702405715361e-05\n",
      "486 6.976672739256173e-05\n",
      "487 6.85753402649425e-05\n",
      "488 6.761980330338702e-05\n",
      "489 6.674745964119211e-05\n",
      "490 6.61289959680289e-05\n",
      "491 6.508603837573901e-05\n",
      "492 6.403095903806388e-05\n",
      "493 6.303333066171035e-05\n",
      "494 6.216963811311871e-05\n",
      "495 6.14034797763452e-05\n",
      "496 6.060501982574351e-05\n",
      "497 5.9622707340167835e-05\n",
      "498 5.8896035625366494e-05\n",
      "499 5.806097760796547e-05\n",
      "运行时间1 0.33659815788269043\n",
      "运行时间2 0.3365061283111572\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "time3=time.time()\n",
    "\n",
    "dtype = torch.float\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:2\") # Uncomment this to run on GPU\n",
    "time1=time.time()\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "'''\n",
    "\n",
    "不指名device可能随便搬到某个设备上，\n",
    "\n",
    "因为有很多显卡，会造成严重后果\n",
    "\n",
    "'''\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "\n",
    "time2=time.time()\n",
    "print('运行时间1',(time2-time3))\n",
    "print('运行时间2',(time2-time1))\n",
    "#因为网络的层数太潜了，而Tensor和model搬到GPU都是需要花费时间的。所以第一次运行可能慢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch的自动求梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create tensors.\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "#######################\n",
    "y.backward()\n",
    "#也封装好了矩阵的梯度\n",
    "\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(28013664., device='cuda:2', grad_fn=<SumBackward0>)\n",
      "1 tensor(24719758., device='cuda:2', grad_fn=<SumBackward0>)\n",
      "2 tensor(23291084., device='cuda:2', grad_fn=<SumBackward0>)\n",
      "3 tensor(21086440., device='cuda:2', grad_fn=<SumBackward0>)\n",
      "4 tensor(17376824., device='cuda:2', grad_fn=<SumBackward0>)\n",
      "5 tensor(12767366., device='cuda:2', grad_fn=<SumBackward0>)\n",
      "6 tensor(8543939., device='cuda:2', grad_fn=<SumBackward0>)\n",
      "7 tensor(5413558., device='cuda:2', grad_fn=<SumBackward0>)\n",
      "8 tensor(3418919.5000, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "9 tensor(2232504.5000, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "10 tensor(1543615.5000, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "11 tensor(1133737.6250, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "12 tensor(878258.5000, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "13 tensor(708800.4375, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "14 tensor(588744.7500, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "15 tensor(498790.6875, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "16 tensor(428415.5625, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "17 tensor(371507.1250, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "18 tensor(324556.5312, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "19 tensor(285096.1875, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "20 tensor(251567.2500, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "21 tensor(222811.6562, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "22 tensor(198043.8125, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "23 tensor(176609.7969, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "24 tensor(157933.3281, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "25 tensor(141595.2812, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "26 tensor(127239.6328, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "27 tensor(114590.0625, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "28 tensor(103412.2656, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "29 tensor(93511.7031, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "30 tensor(84712.7969, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "31 tensor(76876.6797, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "32 tensor(69879.9688, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "33 tensor(63623.5859, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "34 tensor(58016.4805, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "35 tensor(52976.3906, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "36 tensor(48437.9609, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "37 tensor(44347.5625, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "38 tensor(40657.9453, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "39 tensor(37322.3516, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "40 tensor(34299.1562, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "41 tensor(31555.1055, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "42 tensor(29059.5332, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "43 tensor(26787.9023, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "44 tensor(24717.4707, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "45 tensor(22828.4922, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "46 tensor(21101.7832, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "47 tensor(19522.7910, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "48 tensor(18076.8984, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "49 tensor(16750.6562, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "50 tensor(15533.4229, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "51 tensor(14415.1328, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "52 tensor(13386.6445, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "53 tensor(12441.9414, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "54 tensor(11571.9980, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "55 tensor(10769.6572, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "56 tensor(10028.3730, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "57 tensor(9343.6172, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "58 tensor(8710.7246, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "59 tensor(8125.1196, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "60 tensor(7582.9126, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "61 tensor(7080.5576, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "62 tensor(6614.7568, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "63 tensor(6182.5879, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "64 tensor(5781.6758, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "65 tensor(5409.2925, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "66 tensor(5063.3682, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "67 tensor(4741.5176, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "68 tensor(4441.9819, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "69 tensor(4163.0361, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "70 tensor(3903.4885, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "71 tensor(3661.5488, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "72 tensor(3435.9695, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "73 tensor(3225.4746, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "74 tensor(3029.0586, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "75 tensor(2845.5063, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "76 tensor(2673.9873, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "77 tensor(2513.5664, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "78 tensor(2363.5381, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "79 tensor(2223.1250, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "80 tensor(2091.7068, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "81 tensor(1968.6833, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "82 tensor(1853.4434, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "83 tensor(1745.4404, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "84 tensor(1644.1619, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "85 tensor(1549.2067, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "86 tensor(1460.1436, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "87 tensor(1376.5540, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "88 tensor(1298.0859, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "89 tensor(1224.3685, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "90 tensor(1155.0787, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "91 tensor(1090.0020, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "92 tensor(1028.7960, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "93 tensor(971.2423, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "94 tensor(917.0958, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "95 tensor(866.1514, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "96 tensor(818.2046, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "97 tensor(773.0724, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "98 tensor(730.5663, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "99 tensor(690.5382, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "100 tensor(652.8229, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "101 tensor(617.2723, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "102 tensor(583.7616, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "103 tensor(552.1779, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "104 tensor(522.3863, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "105 tensor(494.2919, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "106 tensor(467.7765, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "107 tensor(442.7567, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "108 tensor(419.1446, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "109 tensor(396.8561, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "110 tensor(375.8054, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "111 tensor(355.9345, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "112 tensor(337.1619, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "113 tensor(319.4196, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "114 tensor(302.6577, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "115 tensor(286.8177, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "116 tensor(271.8397, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "117 tensor(257.6765, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "118 tensor(244.2860, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "119 tensor(231.6269, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "120 tensor(219.6488, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "121 tensor(208.3172, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "122 tensor(197.5957, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "123 tensor(187.4468, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "124 tensor(177.8409, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "125 tensor(168.7473, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "126 tensor(160.1352, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "127 tensor(151.9830, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "128 tensor(144.2612, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "129 tensor(136.9463, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "130 tensor(130.0170, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "131 tensor(123.4523, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "132 tensor(117.2362, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "133 tensor(111.3482, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "134 tensor(105.7624, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "135 tensor(100.4688, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "136 tensor(95.4502, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "137 tensor(90.6908, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "138 tensor(86.1770, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "139 tensor(81.8965, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "140 tensor(77.8380, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "141 tensor(73.9853, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "142 tensor(70.3307, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "143 tensor(66.8625, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "144 tensor(63.5718, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "145 tensor(60.4477, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "146 tensor(57.4840, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "147 tensor(54.6679, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "148 tensor(51.9953, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "149 tensor(49.4575, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "150 tensor(47.0478, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "151 tensor(44.7587, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "152 tensor(42.5846, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "153 tensor(40.5193, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "154 tensor(38.5582, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "155 tensor(36.6936, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "156 tensor(34.9233, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "157 tensor(33.2397, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "158 tensor(31.6400, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "159 tensor(30.1196, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "160 tensor(28.6748, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "161 tensor(27.3009, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "162 tensor(25.9943, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "163 tensor(24.7525, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "164 tensor(23.5713, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "165 tensor(22.4486, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "166 tensor(21.3804, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "167 tensor(20.3646, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "168 tensor(19.3987, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "169 tensor(18.4792, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "170 tensor(17.6043, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "171 tensor(16.7725, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "172 tensor(15.9808, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "173 tensor(15.2276, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "174 tensor(14.5107, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "175 tensor(13.8283, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "176 tensor(13.1788, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "177 tensor(12.5608, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "178 tensor(11.9722, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "179 tensor(11.4122, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "180 tensor(10.8787, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "181 tensor(10.3707, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "182 tensor(9.8873, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "183 tensor(9.4270, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "184 tensor(8.9885, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "185 tensor(8.5707, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "186 tensor(8.1730, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "187 tensor(7.7940, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "188 tensor(7.4330, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "189 tensor(7.0892, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "190 tensor(6.7614, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "191 tensor(6.4495, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "192 tensor(6.1521, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "193 tensor(5.8687, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "194 tensor(5.5985, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "195 tensor(5.3410, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "196 tensor(5.0959, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "197 tensor(4.8621, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "198 tensor(4.6392, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "199 tensor(4.4268, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "200 tensor(4.2243, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "201 tensor(4.0312, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "202 tensor(3.8471, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "203 tensor(3.6717, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "204 tensor(3.5044, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "205 tensor(3.3448, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "206 tensor(3.1927, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "207 tensor(3.0475, device='cuda:2', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 tensor(2.9090, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "209 tensor(2.7771, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "210 tensor(2.6513, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "211 tensor(2.5312, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "212 tensor(2.4166, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "213 tensor(2.3074, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "214 tensor(2.2031, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "215 tensor(2.1036, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "216 tensor(2.0088, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "217 tensor(1.9183, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "218 tensor(1.8320, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "219 tensor(1.7495, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "220 tensor(1.6709, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "221 tensor(1.5958, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "222 tensor(1.5241, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "223 tensor(1.4558, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "224 tensor(1.3905, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "225 tensor(1.3282, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "226 tensor(1.2688, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "227 tensor(1.2120, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "228 tensor(1.1580, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "229 tensor(1.1062, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "230 tensor(1.0568, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "231 tensor(1.0097, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "232 tensor(0.9647, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "233 tensor(0.9217, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "234 tensor(0.8807, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "235 tensor(0.8416, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "236 tensor(0.8041, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "237 tensor(0.7685, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "238 tensor(0.7344, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "239 tensor(0.7018, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "240 tensor(0.6707, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "241 tensor(0.6410, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "242 tensor(0.6126, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "243 tensor(0.5855, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "244 tensor(0.5596, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "245 tensor(0.5349, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "246 tensor(0.5113, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "247 tensor(0.4887, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "248 tensor(0.4671, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "249 tensor(0.4465, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "250 tensor(0.4269, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "251 tensor(0.4081, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "252 tensor(0.3902, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "253 tensor(0.3730, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "254 tensor(0.3567, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "255 tensor(0.3410, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "256 tensor(0.3260, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "257 tensor(0.3117, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "258 tensor(0.2980, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "259 tensor(0.2849, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "260 tensor(0.2724, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "261 tensor(0.2605, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "262 tensor(0.2491, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "263 tensor(0.2383, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "264 tensor(0.2278, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "265 tensor(0.2179, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "266 tensor(0.2084, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "267 tensor(0.1993, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "268 tensor(0.1906, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "269 tensor(0.1823, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "270 tensor(0.1743, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "271 tensor(0.1667, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "272 tensor(0.1595, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "273 tensor(0.1526, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "274 tensor(0.1459, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "275 tensor(0.1396, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "276 tensor(0.1335, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "277 tensor(0.1277, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "278 tensor(0.1222, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "279 tensor(0.1169, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "280 tensor(0.1118, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "281 tensor(0.1070, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "282 tensor(0.1023, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "283 tensor(0.0979, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "284 tensor(0.0937, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "285 tensor(0.0896, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "286 tensor(0.0857, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "287 tensor(0.0820, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "288 tensor(0.0785, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "289 tensor(0.0751, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "290 tensor(0.0719, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "291 tensor(0.0688, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "292 tensor(0.0658, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "293 tensor(0.0630, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "294 tensor(0.0603, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "295 tensor(0.0577, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "296 tensor(0.0552, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "297 tensor(0.0528, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "298 tensor(0.0505, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "299 tensor(0.0484, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "300 tensor(0.0463, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "301 tensor(0.0443, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "302 tensor(0.0424, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "303 tensor(0.0406, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "304 tensor(0.0389, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "305 tensor(0.0372, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "306 tensor(0.0356, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "307 tensor(0.0341, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "308 tensor(0.0326, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "309 tensor(0.0312, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "310 tensor(0.0299, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "311 tensor(0.0286, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "312 tensor(0.0274, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "313 tensor(0.0262, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "314 tensor(0.0251, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "315 tensor(0.0241, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "316 tensor(0.0230, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "317 tensor(0.0221, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "318 tensor(0.0211, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "319 tensor(0.0202, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "320 tensor(0.0194, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "321 tensor(0.0186, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "322 tensor(0.0178, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "323 tensor(0.0170, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "324 tensor(0.0163, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "325 tensor(0.0156, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "326 tensor(0.0150, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "327 tensor(0.0143, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "328 tensor(0.0137, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "329 tensor(0.0132, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "330 tensor(0.0126, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "331 tensor(0.0121, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "332 tensor(0.0116, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "333 tensor(0.0111, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "334 tensor(0.0107, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "335 tensor(0.0102, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "336 tensor(0.0098, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "337 tensor(0.0094, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "338 tensor(0.0090, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "339 tensor(0.0086, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "340 tensor(0.0083, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "341 tensor(0.0079, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "342 tensor(0.0076, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "343 tensor(0.0073, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "344 tensor(0.0070, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "345 tensor(0.0067, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "346 tensor(0.0065, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "347 tensor(0.0062, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "348 tensor(0.0059, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "349 tensor(0.0057, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "350 tensor(0.0055, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "351 tensor(0.0053, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "352 tensor(0.0051, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "353 tensor(0.0048, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "354 tensor(0.0047, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "355 tensor(0.0045, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "356 tensor(0.0043, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "357 tensor(0.0041, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "358 tensor(0.0040, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "359 tensor(0.0038, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "360 tensor(0.0037, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "361 tensor(0.0035, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "362 tensor(0.0034, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "363 tensor(0.0033, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "364 tensor(0.0031, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "365 tensor(0.0030, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "366 tensor(0.0029, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "367 tensor(0.0028, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "368 tensor(0.0027, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "369 tensor(0.0026, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "370 tensor(0.0025, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "371 tensor(0.0024, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "372 tensor(0.0023, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "373 tensor(0.0022, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "374 tensor(0.0022, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "375 tensor(0.0021, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "376 tensor(0.0020, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "377 tensor(0.0019, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "378 tensor(0.0019, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "379 tensor(0.0018, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "380 tensor(0.0017, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "381 tensor(0.0017, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "382 tensor(0.0016, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "383 tensor(0.0016, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "384 tensor(0.0015, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "385 tensor(0.0015, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "386 tensor(0.0014, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "387 tensor(0.0014, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "388 tensor(0.0013, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "389 tensor(0.0013, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "390 tensor(0.0012, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "391 tensor(0.0012, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "392 tensor(0.0012, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "393 tensor(0.0011, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "394 tensor(0.0011, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "395 tensor(0.0010, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "396 tensor(0.0010, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "397 tensor(0.0010, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "398 tensor(0.0009, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "399 tensor(0.0009, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "400 tensor(0.0009, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "401 tensor(0.0009, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "402 tensor(0.0008, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "403 tensor(0.0008, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "404 tensor(0.0008, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "405 tensor(0.0008, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "406 tensor(0.0007, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "407 tensor(0.0007, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "408 tensor(0.0007, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "409 tensor(0.0007, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "410 tensor(0.0007, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "411 tensor(0.0006, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "412 tensor(0.0006, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "413 tensor(0.0006, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "414 tensor(0.0006, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "415 tensor(0.0006, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "416 tensor(0.0006, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "417 tensor(0.0005, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "418 tensor(0.0005, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "419 tensor(0.0005, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "420 tensor(0.0005, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "421 tensor(0.0005, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "422 tensor(0.0005, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "423 tensor(0.0005, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "424 tensor(0.0004, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "425 tensor(0.0004, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "426 tensor(0.0004, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "427 tensor(0.0004, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "428 tensor(0.0004, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "429 tensor(0.0004, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "430 tensor(0.0004, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "431 tensor(0.0004, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "432 tensor(0.0004, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "433 tensor(0.0004, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "434 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "435 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "436 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "437 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "439 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "440 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "441 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "442 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "443 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "444 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "445 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "446 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "447 tensor(0.0003, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "448 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "449 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "450 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "451 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "452 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "453 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "454 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "455 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "456 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "457 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "458 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "459 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "460 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "461 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "462 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "463 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "464 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "465 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "466 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "467 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "468 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "469 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "470 tensor(0.0002, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "471 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "472 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "473 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "474 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "475 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "476 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "477 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "478 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "479 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "480 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "481 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "482 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "483 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "484 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "485 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "486 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "487 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "488 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "489 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "490 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "491 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "492 tensor(0.0001, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "493 tensor(9.9911e-05, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "494 tensor(9.8017e-05, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "495 tensor(9.6482e-05, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "496 tensor(9.4883e-05, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "497 tensor(9.3126e-05, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "498 tensor(9.1725e-05, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "499 tensor(9.0140e-05, device='cuda:2', grad_fn=<SumBackward0>)\n",
      "运行时间1 0.9095587730407715\n",
      "运行时间2 0.909447431564331\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "time3=time.time()\n",
    "\n",
    "dtype = torch.float\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:2\") # Uncomment this to run on GPU\n",
    "time1=time.time()\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "'''\n",
    "\n",
    "不指名device可能随便搬到某个设备上，\n",
    "\n",
    "因为有很多显卡，会造成严重后果\n",
    "\n",
    "'''\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)#Torch默认没有grad，为了节约内存\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)#x,y是训练数据，不需要知道梯度\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype,requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype,requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    #h = x.mm(w1)\n",
    "    #h_relu = h.clamp(min=0)\n",
    "    #y_pred = h_relu.mm(w2)\n",
    "    #一行写完\n",
    "    y_pred=x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    #loss不是公式，其实是计算图computation graph，把所有相关运算都包含了\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    #grad_y_pred = 2.0 * (y_pred - y)\n",
    "    #grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    #grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    #grad_h = grad_h_relu.clone()\n",
    "    #grad_h[h < 0] = 0\n",
    "    #grad_w1 = x.t().mm(grad_h)\n",
    "    #一行代替上面所有求导步骤\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    # 我们可以手动做gradient descent(后面我们会介绍自动的方法)。\n",
    "    # 用torch.no_grad()包含以下statements，因为w1和w2都是requires_grad=True，\n",
    "    # 但是在更新weights之后我们并不需要再做autograd。\n",
    "    # 另一种方法是在weight.data和weight.grad.data上做操作，这样就不会对grad产生影响。\n",
    "    # tensor.data会我们一个tensor，这个tensor和原来的tensor指向相同的内存空间，\n",
    "    # 但是不会记录计算图的历史。\n",
    "    with torch.no_grad():#不会把w1,w2的梯度记录下来，节省内存\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()#必须清零，不然会一直叠加\n",
    "        w2.grad.zero_()\n",
    "\n",
    "time2=time.time()\n",
    "print('运行时间1',(time2-time3))\n",
    "print('运行时间2',(time2-time1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Torch.nn搭建神经网络\n",
    "# Torch.nn中包含大量神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(30616406., device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "1 tensor(26209274., device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "2 tensor(26859306., device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "3 tensor(28114846., device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "4 tensor(26741040., device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "5 tensor(21320704., device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "6 tensor(14214398., device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "7 tensor(8161496., device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "8 tensor(4471921.5000, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "9 tensor(2541075.5000, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "10 tensor(1599552.2500, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "11 tensor(1123288.6250, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "12 tensor(860702.5625, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "13 tensor(697667.6250, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "14 tensor(584630., device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "15 tensor(499502.4062, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "16 tensor(431862.8125, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "17 tensor(376364.8750, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "18 tensor(330001.4062, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "19 tensor(290892.3750, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "20 tensor(257467.3438, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "21 tensor(228758.2812, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "22 tensor(203962.1562, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "23 tensor(182460.8281, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "24 tensor(163685.8281, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "25 tensor(147199.0469, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "26 tensor(132697.6875, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "27 tensor(119908.3750, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "28 tensor(108590.0703, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "29 tensor(98538.9375, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "30 tensor(89595.2500, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "31 tensor(81608.5234, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "32 tensor(74453.2500, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "33 tensor(68030.6953, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "34 tensor(62265.5352, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "35 tensor(57073.5273, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "36 tensor(52385.1328, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "37 tensor(48140.8125, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "38 tensor(44295.5352, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "39 tensor(40803.1367, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "40 tensor(37624.7344, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "41 tensor(34730.1133, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "42 tensor(32090.8887, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "43 tensor(29676.8984, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "44 tensor(27469.5000, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "45 tensor(25446.5703, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "46 tensor(23591.7207, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "47 tensor(21888.3672, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "48 tensor(20322.4219, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "49 tensor(18882.1172, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "50 tensor(17555.5820, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "51 tensor(16331.7656, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "52 tensor(15203.1250, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "53 tensor(14160.9229, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "54 tensor(13197.1406, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "55 tensor(12305.6455, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "56 tensor(11480.4102, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "57 tensor(10716.1318, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "58 tensor(10006.9502, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "59 tensor(9349.9111, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "60 tensor(8739.9102, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "61 tensor(8173.3252, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "62 tensor(7646.1470, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "63 tensor(7156.7593, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "64 tensor(6701.1460, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "65 tensor(6276.6431, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "66 tensor(5881.0791, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "67 tensor(5513.1284, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "68 tensor(5170.3809, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "69 tensor(4850.4609, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "70 tensor(4551.9414, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "71 tensor(4272.8682, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "72 tensor(4012.1038, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "73 tensor(3768.3040, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "74 tensor(3540.3091, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "75 tensor(3327.0220, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "76 tensor(3127.5005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "77 tensor(2940.6067, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "78 tensor(2765.4712, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "79 tensor(2601.5479, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "80 tensor(2447.7795, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "81 tensor(2303.5415, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "82 tensor(2168.2664, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "83 tensor(2041.4017, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "84 tensor(1922.3920, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "85 tensor(1810.6213, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "86 tensor(1705.7185, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "87 tensor(1607.1244, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "88 tensor(1514.5635, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "89 tensor(1427.5911, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "90 tensor(1345.7874, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "91 tensor(1268.8892, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "92 tensor(1196.5879, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "93 tensor(1128.5638, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "94 tensor(1064.5739, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "95 tensor(1004.3760, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "96 tensor(947.7169, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "97 tensor(894.3740, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "98 tensor(844.2028, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "99 tensor(796.8901, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "100 tensor(752.3378, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "101 tensor(710.3586, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "102 tensor(670.8102, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "103 tensor(633.5423, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "104 tensor(598.4259, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "105 tensor(565.3183, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "106 tensor(534.0999, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "107 tensor(504.6608, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "108 tensor(476.9243, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "109 tensor(450.7356, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "110 tensor(426.0291, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "111 tensor(402.7126, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "112 tensor(380.7103, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "113 tensor(359.9440, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "114 tensor(340.3491, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "115 tensor(321.8698, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "116 tensor(304.4371, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "117 tensor(287.9754, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "118 tensor(272.4447, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "119 tensor(257.7595, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "120 tensor(243.8887, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "121 tensor(230.7796, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "122 tensor(218.3914, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "123 tensor(206.6874, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "124 tensor(195.6293, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "125 tensor(185.1770, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "126 tensor(175.2952, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "127 tensor(165.9515, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "128 tensor(157.1241, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "129 tensor(148.7757, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "130 tensor(140.8790, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "131 tensor(133.4059, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "132 tensor(126.3423, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "133 tensor(119.6616, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "134 tensor(113.3415, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "135 tensor(107.3575, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "136 tensor(101.7006, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "137 tensor(96.3455, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "138 tensor(91.2800, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "139 tensor(86.4874, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "140 tensor(81.9510, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "141 tensor(77.6560, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "142 tensor(73.5892, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "143 tensor(69.7396, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "144 tensor(66.0942, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "145 tensor(62.6446, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "146 tensor(59.3798, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "147 tensor(56.2865, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "148 tensor(53.3579, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "149 tensor(50.5866, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "150 tensor(47.9584, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "151 tensor(45.4692, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "152 tensor(43.1128, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "153 tensor(40.8798, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "154 tensor(38.7640, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "155 tensor(36.7600, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "156 tensor(34.8610, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "157 tensor(33.0618, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "158 tensor(31.3564, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "159 tensor(29.7420, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "160 tensor(28.2111, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "161 tensor(26.7608, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "162 tensor(25.3849, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "163 tensor(24.0812, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "164 tensor(22.8455, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "165 tensor(21.6742, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "166 tensor(20.5637, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "167 tensor(19.5107, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "168 tensor(18.5131, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "169 tensor(17.5663, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "170 tensor(16.6701, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "171 tensor(15.8185, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "172 tensor(15.0124, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "173 tensor(14.2467, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "174 tensor(13.5210, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "175 tensor(12.8329, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "176 tensor(12.1801, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "177 tensor(11.5612, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "178 tensor(10.9737, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "179 tensor(10.4167, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "180 tensor(9.8887, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "181 tensor(9.3875, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "182 tensor(8.9118, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "183 tensor(8.4608, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "184 tensor(8.0327, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "185 tensor(7.6266, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "186 tensor(7.2413, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "187 tensor(6.8755, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "188 tensor(6.5285, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "189 tensor(6.1991, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "190 tensor(5.8866, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "191 tensor(5.5905, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "192 tensor(5.3089, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "193 tensor(5.0417, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "194 tensor(4.7884, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "195 tensor(4.5477, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "196 tensor(4.3191, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "197 tensor(4.1024, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "198 tensor(3.8966, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "199 tensor(3.7012, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "200 tensor(3.5156, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "201 tensor(3.3393, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "202 tensor(3.1724, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "203 tensor(3.0137, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "204 tensor(2.8630, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "205 tensor(2.7198, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "206 tensor(2.5840, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "207 tensor(2.4549, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "208 tensor(2.3324, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "209 tensor(2.2161, device='cuda:2', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 tensor(2.1056, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "211 tensor(2.0007, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "212 tensor(1.9010, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "213 tensor(1.8065, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "214 tensor(1.7166, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "215 tensor(1.6311, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "216 tensor(1.5501, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "217 tensor(1.4732, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "218 tensor(1.4000, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "219 tensor(1.3304, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "220 tensor(1.2644, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "221 tensor(1.2017, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "222 tensor(1.1421, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "223 tensor(1.0854, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "224 tensor(1.0318, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "225 tensor(0.9808, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "226 tensor(0.9322, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "227 tensor(0.8861, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "228 tensor(0.8423, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "229 tensor(0.8007, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "230 tensor(0.7611, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "231 tensor(0.7235, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "232 tensor(0.6879, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "233 tensor(0.6540, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "234 tensor(0.6217, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "235 tensor(0.5911, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "236 tensor(0.5620, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "237 tensor(0.5343, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "238 tensor(0.5080, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "239 tensor(0.4830, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "240 tensor(0.4592, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "241 tensor(0.4367, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "242 tensor(0.4152, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "243 tensor(0.3948, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "244 tensor(0.3754, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "245 tensor(0.3570, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "246 tensor(0.3395, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "247 tensor(0.3228, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "248 tensor(0.3070, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "249 tensor(0.2919, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "250 tensor(0.2776, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "251 tensor(0.2640, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "252 tensor(0.2511, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "253 tensor(0.2389, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "254 tensor(0.2272, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "255 tensor(0.2161, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "256 tensor(0.2055, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "257 tensor(0.1954, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "258 tensor(0.1859, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "259 tensor(0.1768, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "260 tensor(0.1682, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "261 tensor(0.1600, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "262 tensor(0.1522, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "263 tensor(0.1448, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "264 tensor(0.1377, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "265 tensor(0.1310, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "266 tensor(0.1246, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "267 tensor(0.1186, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "268 tensor(0.1128, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "269 tensor(0.1074, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "270 tensor(0.1021, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "271 tensor(0.0971, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "272 tensor(0.0924, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "273 tensor(0.0880, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "274 tensor(0.0837, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "275 tensor(0.0796, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "276 tensor(0.0758, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "277 tensor(0.0721, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "278 tensor(0.0686, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "279 tensor(0.0653, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "280 tensor(0.0621, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "281 tensor(0.0591, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "282 tensor(0.0563, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "283 tensor(0.0535, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "284 tensor(0.0509, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "285 tensor(0.0485, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "286 tensor(0.0461, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "287 tensor(0.0439, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "288 tensor(0.0418, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "289 tensor(0.0398, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "290 tensor(0.0379, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "291 tensor(0.0361, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "292 tensor(0.0343, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "293 tensor(0.0327, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "294 tensor(0.0311, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "295 tensor(0.0296, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "296 tensor(0.0282, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "297 tensor(0.0268, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "298 tensor(0.0255, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "299 tensor(0.0243, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "300 tensor(0.0231, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "301 tensor(0.0221, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "302 tensor(0.0210, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "303 tensor(0.0200, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "304 tensor(0.0190, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "305 tensor(0.0181, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "306 tensor(0.0173, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "307 tensor(0.0164, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "308 tensor(0.0157, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "309 tensor(0.0149, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "310 tensor(0.0142, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "311 tensor(0.0135, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "312 tensor(0.0129, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "313 tensor(0.0123, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "314 tensor(0.0117, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "315 tensor(0.0112, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "316 tensor(0.0107, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "317 tensor(0.0102, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "318 tensor(0.0097, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "319 tensor(0.0092, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "320 tensor(0.0088, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "321 tensor(0.0084, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "322 tensor(0.0080, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "323 tensor(0.0076, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "324 tensor(0.0073, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "325 tensor(0.0069, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "326 tensor(0.0066, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "327 tensor(0.0063, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "328 tensor(0.0060, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "329 tensor(0.0058, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "330 tensor(0.0055, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "331 tensor(0.0053, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "332 tensor(0.0050, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "333 tensor(0.0048, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "334 tensor(0.0046, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "335 tensor(0.0044, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "336 tensor(0.0042, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "337 tensor(0.0040, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "338 tensor(0.0038, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "339 tensor(0.0036, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "340 tensor(0.0035, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "341 tensor(0.0033, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "342 tensor(0.0032, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "343 tensor(0.0031, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "344 tensor(0.0029, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "345 tensor(0.0028, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "346 tensor(0.0027, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "347 tensor(0.0026, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "348 tensor(0.0025, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "349 tensor(0.0024, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "350 tensor(0.0023, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "351 tensor(0.0022, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "352 tensor(0.0021, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "353 tensor(0.0020, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "354 tensor(0.0019, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "355 tensor(0.0018, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "356 tensor(0.0018, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "357 tensor(0.0017, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "358 tensor(0.0016, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "359 tensor(0.0016, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "360 tensor(0.0015, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "361 tensor(0.0014, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "362 tensor(0.0014, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "363 tensor(0.0013, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "364 tensor(0.0013, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "365 tensor(0.0012, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "366 tensor(0.0012, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "367 tensor(0.0011, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "368 tensor(0.0011, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "369 tensor(0.0011, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "370 tensor(0.0010, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "371 tensor(0.0010, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "372 tensor(0.0010, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "373 tensor(0.0009, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "374 tensor(0.0009, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "375 tensor(0.0009, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "376 tensor(0.0008, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "377 tensor(0.0008, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "378 tensor(0.0008, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "379 tensor(0.0007, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "380 tensor(0.0007, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "381 tensor(0.0007, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "382 tensor(0.0007, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "383 tensor(0.0006, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "384 tensor(0.0006, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "385 tensor(0.0006, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "386 tensor(0.0006, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "387 tensor(0.0006, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "388 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "389 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "390 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "391 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "392 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "393 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "394 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "395 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "396 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "397 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "398 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "399 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "400 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "401 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "402 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "403 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "404 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "405 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "406 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "407 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "408 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "409 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "410 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "411 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "412 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "413 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "414 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "415 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "416 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "417 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "418 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "419 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "420 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "421 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "422 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "423 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "424 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "425 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "426 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "427 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "428 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "429 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "430 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "431 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "432 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "433 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "434 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "435 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "436 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "437 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "438 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "439 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "440 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "442 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "443 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "444 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "445 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "446 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "447 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "448 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "449 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "450 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "451 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "452 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "453 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "454 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "455 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "456 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "457 tensor(9.8720e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "458 tensor(9.6931e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "459 tensor(9.5195e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "460 tensor(9.2962e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "461 tensor(9.1529e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "462 tensor(8.9850e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "463 tensor(8.8414e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "464 tensor(8.6427e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "465 tensor(8.5096e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "466 tensor(8.3749e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "467 tensor(8.2391e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "468 tensor(8.1208e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "469 tensor(8.0065e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "470 tensor(7.8708e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "471 tensor(7.7051e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "472 tensor(7.5955e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "473 tensor(7.4791e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "474 tensor(7.3316e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "475 tensor(7.1871e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "476 tensor(7.0992e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "477 tensor(6.9808e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "478 tensor(6.8516e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "479 tensor(6.7743e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "480 tensor(6.6581e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "481 tensor(6.5938e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "482 tensor(6.4831e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "483 tensor(6.4227e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "484 tensor(6.3015e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "485 tensor(6.1863e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "486 tensor(6.1060e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "487 tensor(6.0162e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "488 tensor(5.9182e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "489 tensor(5.8402e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "490 tensor(5.7447e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "491 tensor(5.6948e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "492 tensor(5.6103e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "493 tensor(5.5247e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "494 tensor(5.4686e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "495 tensor(5.3918e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "496 tensor(5.3048e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "497 tensor(5.2305e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "498 tensor(5.1624e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "499 tensor(5.1178e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "运行时间1 0.9163131713867188\n",
      "运行时间2 0.9162366390228271\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn#各种神经网络定义方法\n",
    "import time\n",
    "time3=time.time()\n",
    "\n",
    "dtype = torch.float\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:2\") # Uncomment this to run on GPU\n",
    "time1=time.time()\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "'''\n",
    "\n",
    "不指名device可能随便搬到某个设备上，\n",
    "\n",
    "因为有很多显卡，会造成严重后果\n",
    "\n",
    "'''\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)#Torch默认没有grad，为了节约内存\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)#x,y是训练数据，不需要知道梯度\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "'''\n",
    "定义model\n",
    "\n",
    "'''\n",
    "# Randomly initialize weights\n",
    "#w1 = torch.randn(D_in, H, device=device, dtype=dtype,requires_grad=True)\n",
    "#w2 = torch.randn(H, D_out, device=device, dtype=dtype,requires_grad=True)\n",
    "#Model代替了W1,w2\n",
    "#Sequential代表一串模型在一起\n",
    "model=torch.nn.Sequential(\n",
    "    \n",
    "    #一个线性层\n",
    "    torch.nn.Linear(D_in, H),#从D_in到H，相当于w1*x+b_1自动有偏置项,不想要就bias=False\n",
    "    \n",
    "    #CLASS  torch.nn.Linear(in_feature,out_feature,bias=True)\n",
    "    \n",
    "    #一个ReLU激活层\n",
    "    torch.nn.ReLU(),#使用ReLU激活\n",
    "    \n",
    "    #一个线性层\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "#Normal均一化\n",
    "\n",
    "'''\n",
    "本次不Normal，学习率e-6时效果很差\n",
    "'''\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "#当然，直接学习率e-4也可以变好\n",
    "#其他尝试可以bias=False等，很神奇\n",
    "#B站视频109.38处\n",
    "\n",
    "model=model.cuda(\"cuda:2\")#使用GPU一定要写\n",
    "\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "\n",
    "#用了Model后，loss在循环外面定义好\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    #h = x.mm(w1)\n",
    "    #h_relu = h.clamp(min=0)\n",
    "    #y_pred = h_relu.mm(w2)\n",
    "    #一行写完\n",
    "    #y_pred=x.mm(w1).clamp(min=0).mm(w2)\n",
    "    #使用Model后更简单\n",
    "    model.zero_grad()#把所有grad全部清零\n",
    "    \n",
    "    y_pred=model(x)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    #loss = (y_pred - y).pow(2).sum()\n",
    "    #用Model更简单\n",
    "    loss=loss_fn(y_pred,y)\n",
    "    \n",
    "    #loss不是公式，其实是计算图computation graph，把所有相关运算都包含了\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    #grad_y_pred = 2.0 * (y_pred - y)\n",
    "    #grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    #grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    #grad_h = grad_h_relu.clone()\n",
    "    #grad_h[h < 0] = 0\n",
    "    #grad_w1 = x.t().mm(grad_h)\n",
    "    #一行代替上面所有求导步骤\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    # 我们可以手动做gradient descent(后面我们会介绍自动的方法)。\n",
    "    # 用torch.no_grad()包含以下statements，因为w1和w2都是requires_grad=True，\n",
    "    # 但是在更新weights之后我们并不需要再做autograd。\n",
    "    # 另一种方法是在weight.data和weight.grad.data上做操作，这样就不会对grad产生影响。\n",
    "    # tensor.data会我们一个tensor，这个tensor和原来的tensor指向相同的内存空间，\n",
    "    # 但是不会记录计算图的历史。\n",
    "    \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():#不会把w1,w2的梯度记录下来，节省内存\n",
    "        for param in model.parameters():#model.parameters封装了很多可迭代数据,所有参数都在param里\n",
    "            param-=learning_rate*param.grad#不能写grad，要写param.grad\n",
    "            #一行写完\n",
    "    \n",
    "    \n",
    "    \n",
    "time2=time.time()\n",
    "print('运行时间1',(time2-time3))\n",
    "print('运行时间2',(time2-time1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer 自动更新参数\n",
    "# 优化器也封装得更好\n",
    "如SGD+momenturn RMSProp,Adam等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(713.7021, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "1 tensor(695.8423, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "2 tensor(678.3995, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "3 tensor(661.5470, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "4 tensor(645.2319, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "5 tensor(629.3463, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "6 tensor(613.9067, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "7 tensor(599.0119, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "8 tensor(584.5263, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "9 tensor(570.4528, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "10 tensor(556.7977, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "11 tensor(543.5728, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "12 tensor(530.7241, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "13 tensor(518.2008, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "14 tensor(506.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "15 tensor(494.1777, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "16 tensor(482.6862, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "17 tensor(471.6244, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "18 tensor(460.9138, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "19 tensor(450.4459, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "20 tensor(440.2689, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "21 tensor(430.3930, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "22 tensor(420.7449, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "23 tensor(411.2962, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "24 tensor(402.0386, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "25 tensor(393.0252, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "26 tensor(384.2363, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "27 tensor(375.6527, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "28 tensor(367.3104, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "29 tensor(359.1577, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "30 tensor(351.1769, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "31 tensor(343.3862, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "32 tensor(335.7350, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "33 tensor(328.2611, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "34 tensor(320.9187, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "35 tensor(313.7149, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "36 tensor(306.6651, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "37 tensor(299.7808, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "38 tensor(293.0374, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "39 tensor(286.4218, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "40 tensor(279.9176, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "41 tensor(273.5377, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "42 tensor(267.2781, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "43 tensor(261.1822, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "44 tensor(255.2244, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "45 tensor(249.3943, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "46 tensor(243.6905, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "47 tensor(238.0947, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "48 tensor(232.5871, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "49 tensor(227.1706, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "50 tensor(221.8594, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "51 tensor(216.6601, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "52 tensor(211.5780, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "53 tensor(206.5808, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "54 tensor(201.6673, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "55 tensor(196.8469, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "56 tensor(192.1034, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "57 tensor(187.4489, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "58 tensor(182.8827, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "59 tensor(178.3999, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "60 tensor(174.0086, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "61 tensor(169.6964, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "62 tensor(165.4660, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "63 tensor(161.3116, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "64 tensor(157.2282, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "65 tensor(153.2235, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "66 tensor(149.2866, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "67 tensor(145.4230, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "68 tensor(141.6471, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "69 tensor(137.9362, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "70 tensor(134.3005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "71 tensor(130.7345, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "72 tensor(127.2411, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "73 tensor(123.8136, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "74 tensor(120.4613, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "75 tensor(117.1736, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "76 tensor(113.9547, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "77 tensor(110.8166, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "78 tensor(107.7496, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "79 tensor(104.7456, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "80 tensor(101.8050, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "81 tensor(98.9285, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "82 tensor(96.1215, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "83 tensor(93.3670, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "84 tensor(90.6822, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "85 tensor(88.0544, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "86 tensor(85.4803, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "87 tensor(82.9682, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "88 tensor(80.5149, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "89 tensor(78.1158, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "90 tensor(75.7776, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "91 tensor(73.4938, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "92 tensor(71.2687, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "93 tensor(69.0936, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "94 tensor(66.9738, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "95 tensor(64.9087, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "96 tensor(62.8981, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "97 tensor(60.9384, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "98 tensor(59.0269, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "99 tensor(57.1634, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "100 tensor(55.3495, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "101 tensor(53.5811, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "102 tensor(51.8611, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "103 tensor(50.1896, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "104 tensor(48.5610, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "105 tensor(46.9771, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "106 tensor(45.4385, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "107 tensor(43.9420, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "108 tensor(42.4859, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "109 tensor(41.0720, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "110 tensor(39.6957, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "111 tensor(38.3587, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "112 tensor(37.0616, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "113 tensor(35.8020, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "114 tensor(34.5776, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "115 tensor(33.3891, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "116 tensor(32.2372, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "117 tensor(31.1187, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "118 tensor(30.0338, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "119 tensor(28.9828, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "120 tensor(27.9633, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "121 tensor(26.9734, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "122 tensor(26.0149, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "123 tensor(25.0863, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "124 tensor(24.1873, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "125 tensor(23.3159, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "126 tensor(22.4712, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "127 tensor(21.6544, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "128 tensor(20.8627, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "129 tensor(20.0974, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "130 tensor(19.3564, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "131 tensor(18.6406, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "132 tensor(17.9471, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "133 tensor(17.2772, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "134 tensor(16.6295, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "135 tensor(16.0043, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "136 tensor(15.3993, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "137 tensor(14.8148, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "138 tensor(14.2502, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "139 tensor(13.7046, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "140 tensor(13.1782, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "141 tensor(12.6706, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "142 tensor(12.1799, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "143 tensor(11.7062, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "144 tensor(11.2508, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "145 tensor(10.8122, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "146 tensor(10.3891, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "147 tensor(9.9815, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "148 tensor(9.5896, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "149 tensor(9.2116, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "150 tensor(8.8467, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "151 tensor(8.4958, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "152 tensor(8.1579, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "153 tensor(7.8325, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "154 tensor(7.5192, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "155 tensor(7.2176, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "156 tensor(6.9272, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "157 tensor(6.6473, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "158 tensor(6.3781, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "159 tensor(6.1192, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "160 tensor(5.8698, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "161 tensor(5.6297, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "162 tensor(5.3989, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "163 tensor(5.1777, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "164 tensor(4.9644, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "165 tensor(4.7595, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "166 tensor(4.5628, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "167 tensor(4.3733, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "168 tensor(4.1912, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "169 tensor(4.0160, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "170 tensor(3.8476, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "171 tensor(3.6859, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "172 tensor(3.5304, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "173 tensor(3.3811, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "174 tensor(3.2378, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "175 tensor(3.1001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "176 tensor(2.9681, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "177 tensor(2.8414, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "178 tensor(2.7198, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "179 tensor(2.6030, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "180 tensor(2.4911, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "181 tensor(2.3838, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "182 tensor(2.2810, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "183 tensor(2.1823, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "184 tensor(2.0879, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "185 tensor(1.9972, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "186 tensor(1.9103, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "187 tensor(1.8270, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "188 tensor(1.7472, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "189 tensor(1.6707, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "190 tensor(1.5975, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "191 tensor(1.5274, device='cuda:2', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 tensor(1.4603, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "193 tensor(1.3960, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "194 tensor(1.3343, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "195 tensor(1.2754, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "196 tensor(1.2190, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "197 tensor(1.1649, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "198 tensor(1.1132, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "199 tensor(1.0637, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "200 tensor(1.0164, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "201 tensor(0.9711, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "202 tensor(0.9278, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "203 tensor(0.8864, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "204 tensor(0.8467, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "205 tensor(0.8088, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "206 tensor(0.7725, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "207 tensor(0.7379, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "208 tensor(0.7047, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "209 tensor(0.6730, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "210 tensor(0.6427, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "211 tensor(0.6138, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "212 tensor(0.5861, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "213 tensor(0.5596, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "214 tensor(0.5343, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "215 tensor(0.5102, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "216 tensor(0.4870, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "217 tensor(0.4650, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "218 tensor(0.4439, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "219 tensor(0.4237, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "220 tensor(0.4044, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "221 tensor(0.3860, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "222 tensor(0.3684, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "223 tensor(0.3517, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "224 tensor(0.3357, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "225 tensor(0.3203, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "226 tensor(0.3057, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "227 tensor(0.2918, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "228 tensor(0.2785, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "229 tensor(0.2657, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "230 tensor(0.2536, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "231 tensor(0.2420, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "232 tensor(0.2309, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "233 tensor(0.2203, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "234 tensor(0.2102, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "235 tensor(0.2006, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "236 tensor(0.1914, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "237 tensor(0.1826, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "238 tensor(0.1743, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "239 tensor(0.1663, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "240 tensor(0.1586, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "241 tensor(0.1514, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "242 tensor(0.1444, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "243 tensor(0.1378, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "244 tensor(0.1315, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "245 tensor(0.1255, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "246 tensor(0.1197, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "247 tensor(0.1143, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "248 tensor(0.1091, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "249 tensor(0.1041, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "250 tensor(0.0994, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "251 tensor(0.0949, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "252 tensor(0.0905, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "253 tensor(0.0864, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "254 tensor(0.0825, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "255 tensor(0.0788, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "256 tensor(0.0752, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "257 tensor(0.0718, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "258 tensor(0.0685, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "259 tensor(0.0654, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "260 tensor(0.0624, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "261 tensor(0.0596, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "262 tensor(0.0569, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "263 tensor(0.0544, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "264 tensor(0.0519, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "265 tensor(0.0496, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "266 tensor(0.0474, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "267 tensor(0.0452, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "268 tensor(0.0432, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "269 tensor(0.0413, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "270 tensor(0.0394, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "271 tensor(0.0376, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "272 tensor(0.0359, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "273 tensor(0.0343, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "274 tensor(0.0328, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "275 tensor(0.0313, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "276 tensor(0.0299, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "277 tensor(0.0286, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "278 tensor(0.0273, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "279 tensor(0.0261, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "280 tensor(0.0249, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "281 tensor(0.0238, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "282 tensor(0.0227, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "283 tensor(0.0217, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "284 tensor(0.0207, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "285 tensor(0.0198, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "286 tensor(0.0189, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "287 tensor(0.0181, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "288 tensor(0.0173, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "289 tensor(0.0165, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "290 tensor(0.0157, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "291 tensor(0.0150, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "292 tensor(0.0144, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "293 tensor(0.0137, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "294 tensor(0.0131, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "295 tensor(0.0125, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "296 tensor(0.0119, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "297 tensor(0.0114, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "298 tensor(0.0109, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "299 tensor(0.0104, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "300 tensor(0.0099, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "301 tensor(0.0095, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "302 tensor(0.0090, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "303 tensor(0.0086, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "304 tensor(0.0082, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "305 tensor(0.0079, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "306 tensor(0.0075, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "307 tensor(0.0072, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "308 tensor(0.0068, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "309 tensor(0.0065, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "310 tensor(0.0062, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "311 tensor(0.0059, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "312 tensor(0.0057, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "313 tensor(0.0054, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "314 tensor(0.0051, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "315 tensor(0.0049, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "316 tensor(0.0047, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "317 tensor(0.0045, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "318 tensor(0.0043, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "319 tensor(0.0041, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "320 tensor(0.0039, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "321 tensor(0.0037, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "322 tensor(0.0035, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "323 tensor(0.0033, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "324 tensor(0.0032, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "325 tensor(0.0030, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "326 tensor(0.0029, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "327 tensor(0.0028, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "328 tensor(0.0026, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "329 tensor(0.0025, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "330 tensor(0.0024, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "331 tensor(0.0023, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "332 tensor(0.0022, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "333 tensor(0.0021, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "334 tensor(0.0020, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "335 tensor(0.0019, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "336 tensor(0.0018, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "337 tensor(0.0017, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "338 tensor(0.0016, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "339 tensor(0.0015, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "340 tensor(0.0015, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "341 tensor(0.0014, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "342 tensor(0.0013, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "343 tensor(0.0013, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "344 tensor(0.0012, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "345 tensor(0.0011, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "346 tensor(0.0011, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "347 tensor(0.0010, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "348 tensor(0.0010, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "349 tensor(0.0009, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "350 tensor(0.0009, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "351 tensor(0.0008, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "352 tensor(0.0008, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "353 tensor(0.0008, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "354 tensor(0.0007, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "355 tensor(0.0007, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "356 tensor(0.0007, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "357 tensor(0.0006, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "358 tensor(0.0006, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "359 tensor(0.0006, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "360 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "361 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "362 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "363 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "364 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "365 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "366 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "367 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "368 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "369 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "370 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "371 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "372 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "373 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "374 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "375 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "376 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "377 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "378 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "379 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "380 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "381 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "382 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "383 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "384 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "385 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "386 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "388 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "389 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "390 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "391 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "392 tensor(9.7907e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "393 tensor(9.2691e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "394 tensor(8.7750e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "395 tensor(8.3057e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "396 tensor(7.8611e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "397 tensor(7.4392e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "398 tensor(7.0391e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "399 tensor(6.6596e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "400 tensor(6.3004e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "401 tensor(5.9598e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "402 tensor(5.6367e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "403 tensor(5.3308e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "404 tensor(5.0403e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "405 tensor(4.7655e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "406 tensor(4.5056e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "407 tensor(4.2590e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "408 tensor(4.0254e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "409 tensor(3.8044e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "410 tensor(3.5948e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "411 tensor(3.3966e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "412 tensor(3.2088e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "413 tensor(3.0310e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "414 tensor(2.8632e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "415 tensor(2.7038e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "416 tensor(2.5534e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "417 tensor(2.4111e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "418 tensor(2.2762e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "419 tensor(2.1486e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "420 tensor(2.0280e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "421 tensor(1.9139e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "422 tensor(1.8061e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "423 tensor(1.7041e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "424 tensor(1.6077e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "425 tensor(1.5166e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "426 tensor(1.4305e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "427 tensor(1.3491e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "428 tensor(1.2722e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "429 tensor(1.1995e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "430 tensor(1.1310e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "431 tensor(1.0660e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "432 tensor(1.0048e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "433 tensor(9.4696e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "434 tensor(8.9235e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "435 tensor(8.4075e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "436 tensor(7.9208e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "437 tensor(7.4605e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "438 tensor(7.0262e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "439 tensor(6.6179e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "440 tensor(6.2309e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "441 tensor(5.8669e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "442 tensor(5.5234e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "443 tensor(5.1993e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "444 tensor(4.8936e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "445 tensor(4.6051e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "446 tensor(4.3327e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "447 tensor(4.0774e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "448 tensor(3.8351e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "449 tensor(3.6078e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "450 tensor(3.3930e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "451 tensor(3.1905e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "452 tensor(2.9995e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "453 tensor(2.8198e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "454 tensor(2.6508e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "455 tensor(2.4917e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "456 tensor(2.3414e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "457 tensor(2.2007e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "458 tensor(2.0673e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "459 tensor(1.9423e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "460 tensor(1.8239e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "461 tensor(1.7130e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "462 tensor(1.6089e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "463 tensor(1.5104e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "464 tensor(1.4186e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "465 tensor(1.3315e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "466 tensor(1.2494e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "467 tensor(1.1729e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "468 tensor(1.1008e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "469 tensor(1.0326e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "470 tensor(9.6873e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "471 tensor(9.0868e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "472 tensor(8.5217e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "473 tensor(7.9926e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "474 tensor(7.4953e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "475 tensor(7.0230e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "476 tensor(6.5838e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "477 tensor(6.1721e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "478 tensor(5.7834e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "479 tensor(5.4197e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "480 tensor(5.0786e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "481 tensor(4.7590e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "482 tensor(4.4572e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "483 tensor(4.1739e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "484 tensor(3.9078e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "485 tensor(3.6578e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "486 tensor(3.4252e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "487 tensor(3.2055e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "488 tensor(3.0015e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "489 tensor(2.8078e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "490 tensor(2.6278e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "491 tensor(2.4567e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "492 tensor(2.3004e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "493 tensor(2.1501e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "494 tensor(2.0100e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "495 tensor(1.8799e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "496 tensor(1.7585e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "497 tensor(1.6429e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "498 tensor(1.5351e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "499 tensor(1.4353e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "运行时间1 1.0715577602386475\n",
      "运行时间2 1.0714173316955566\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#一般optimizer的学习率在e-3~-6之间，普遍e-4\n",
    "learning_rate=1e-4\n",
    "#优化器拿到所有参数和学习率\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "#不用zero也不用with_no_grad了\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn#各种神经网络定义方法\n",
    "import time\n",
    "time3=time.time()\n",
    "\n",
    "dtype = torch.float\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:2\") # Uncomment this to run on GPU\n",
    "time1=time.time()\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "'''\n",
    "\n",
    "不指名device可能随便搬到某个设备上，\n",
    "\n",
    "因为有很多显卡，会造成严重后果\n",
    "\n",
    "'''\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)#Torch默认没有grad，为了节约内存\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)#x,y是训练数据，不需要知道梯度\n",
    "\n",
    "#因为模型很简单，不需要很小的学习率\n",
    "learning_rate = 1e-4\n",
    "\n",
    "'''\n",
    "定义model\n",
    "\n",
    "'''\n",
    "# Randomly initialize weights\n",
    "#w1 = torch.randn(D_in, H, device=device, dtype=dtype,requires_grad=True)\n",
    "#w2 = torch.randn(H, D_out, device=device, dtype=dtype,requires_grad=True)\n",
    "#Model代替了W1,w2\n",
    "#Sequential代表一串模型在一起\n",
    "model=torch.nn.Sequential(\n",
    "    \n",
    "    #一个线性层\n",
    "    torch.nn.Linear(D_in, H),#从D_in到H，相当于w1*x+b_1自动有偏置项,不想要就bias=False\n",
    "    \n",
    "    #CLASS  torch.nn.Linear(in_feature,out_feature,bias=True)\n",
    "    \n",
    "    #一个ReLU激活层\n",
    "    torch.nn.ReLU(),#使用ReLU激活\n",
    "    \n",
    "    #一个线性层\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "#Normal均一化\n",
    "\n",
    "'''\n",
    "上次不Normal，很差\n",
    "发现本次不需要Normal\n",
    "'''\n",
    "\n",
    "#当然，直接学习率e-4也可以变好\n",
    "#其他尝试可以bias=False等，很神奇\n",
    "#B站视频109.38处\n",
    "\n",
    "model=model.cuda(\"cuda:2\")#使用GPU一定要写\n",
    "\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "\n",
    "#用了Model后，loss在循环外面定义好\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "#定义优化器\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    #h = x.mm(w1)\n",
    "    #h_relu = h.clamp(min=0)\n",
    "    #y_pred = h_relu.mm(w2)\n",
    "    #一行写完\n",
    "    #y_pred=x.mm(w1).clamp(min=0).mm(w2)\n",
    "    #使用Model后更简单\n",
    "    model.zero_grad()#把所有grad全部清零\n",
    "    \n",
    "    y_pred=model(x)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    #loss = (y_pred - y).pow(2).sum()\n",
    "    #用Model更简单\n",
    "    loss=loss_fn(y_pred,y)\n",
    "    \n",
    "    #loss不是公式，其实是计算图computation graph，把所有相关运算都包含了\n",
    "    print(t, loss)\n",
    "    \n",
    "    optimizer.zero_grad()#反向传播之前清空梯度\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    #grad_y_pred = 2.0 * (y_pred - y)\n",
    "    #grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    #grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    #grad_h = grad_h_relu.clone()\n",
    "    #grad_h[h < 0] = 0\n",
    "    #grad_w1 = x.t().mm(grad_h)\n",
    "    #一行代替上面所有求导步骤\n",
    "    loss.backward()\n",
    "    \n",
    "    #一步更新\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "time2=time.time()\n",
    "print('运行时间1',(time2-time3))\n",
    "print('运行时间2',(time2-time1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 两层神经网络\n",
    "# 设计方法为CLASS继承\n",
    "# Forward为模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(725.4304, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "1 tensor(707.2980, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "2 tensor(689.6609, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "3 tensor(672.5133, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "4 tensor(655.9084, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "5 tensor(639.7986, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "6 tensor(624.1733, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "7 tensor(608.9854, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "8 tensor(594.1759, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "9 tensor(579.8203, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "10 tensor(565.8364, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "11 tensor(552.2241, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "12 tensor(538.9123, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "13 tensor(526.0163, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "14 tensor(513.5248, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "15 tensor(501.4027, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "16 tensor(489.6035, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "17 tensor(478.1480, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "18 tensor(466.9716, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "19 tensor(456.1045, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "20 tensor(445.5340, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "21 tensor(435.2168, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "22 tensor(425.1277, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "23 tensor(415.2598, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "24 tensor(405.5876, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "25 tensor(396.1019, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "26 tensor(386.8524, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "27 tensor(377.8820, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "28 tensor(369.1455, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "29 tensor(360.6308, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "30 tensor(352.3305, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "31 tensor(344.2619, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "32 tensor(336.3760, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "33 tensor(328.6445, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "34 tensor(321.0800, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "35 tensor(313.6914, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "36 tensor(306.4875, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "37 tensor(299.4057, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "38 tensor(292.4302, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "39 tensor(285.5751, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "40 tensor(278.8463, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "41 tensor(272.2510, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "42 tensor(265.7682, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "43 tensor(259.3984, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "44 tensor(253.1414, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "45 tensor(246.9982, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "46 tensor(240.9842, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "47 tensor(235.0962, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "48 tensor(229.3161, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "49 tensor(223.6572, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "50 tensor(218.1246, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "51 tensor(212.7184, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "52 tensor(207.4342, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "53 tensor(202.2436, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "54 tensor(197.1675, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "55 tensor(192.2027, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "56 tensor(187.3372, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "57 tensor(182.5686, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "58 tensor(177.8863, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "59 tensor(173.2870, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "60 tensor(168.7921, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "61 tensor(164.3863, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "62 tensor(160.0622, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "63 tensor(155.8399, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "64 tensor(151.7063, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "65 tensor(147.6600, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "66 tensor(143.6960, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "67 tensor(139.8255, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "68 tensor(136.0470, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "69 tensor(132.3475, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "70 tensor(128.7076, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "71 tensor(125.1376, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "72 tensor(121.6453, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "73 tensor(118.2283, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "74 tensor(114.8838, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "75 tensor(111.6057, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "76 tensor(108.3984, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "77 tensor(105.2671, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "78 tensor(102.2031, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "79 tensor(99.2060, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "80 tensor(96.2836, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "81 tensor(93.4285, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "82 tensor(90.6441, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "83 tensor(87.9281, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "84 tensor(85.2822, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "85 tensor(82.6970, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "86 tensor(80.1697, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "87 tensor(77.6990, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "88 tensor(75.2854, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "89 tensor(72.9372, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "90 tensor(70.6504, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "91 tensor(68.4180, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "92 tensor(66.2419, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "93 tensor(64.1225, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "94 tensor(62.0557, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "95 tensor(60.0417, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "96 tensor(58.0806, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "97 tensor(56.1702, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "98 tensor(54.3136, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "99 tensor(52.5042, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "100 tensor(50.7468, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "101 tensor(49.0379, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "102 tensor(47.3748, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "103 tensor(45.7582, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "104 tensor(44.1870, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "105 tensor(42.6597, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "106 tensor(41.1754, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "107 tensor(39.7358, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "108 tensor(38.3375, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "109 tensor(36.9808, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "110 tensor(35.6685, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "111 tensor(34.3922, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "112 tensor(33.1539, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "113 tensor(31.9544, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "114 tensor(30.7959, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "115 tensor(29.6725, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "116 tensor(28.5852, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "117 tensor(27.5335, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "118 tensor(26.5145, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "119 tensor(25.5268, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "120 tensor(24.5701, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "121 tensor(23.6437, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "122 tensor(22.7462, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "123 tensor(21.8801, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "124 tensor(21.0418, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "125 tensor(20.2314, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "126 tensor(19.4488, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "127 tensor(18.6957, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "128 tensor(17.9666, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "129 tensor(17.2626, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "130 tensor(16.5842, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "131 tensor(15.9280, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "132 tensor(15.2961, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "133 tensor(14.6871, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "134 tensor(14.1005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "135 tensor(13.5328, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "136 tensor(12.9874, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "137 tensor(12.4614, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "138 tensor(11.9538, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "139 tensor(11.4639, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "140 tensor(10.9915, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "141 tensor(10.5362, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "142 tensor(10.0971, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "143 tensor(9.6748, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "144 tensor(9.2679, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "145 tensor(8.8758, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "146 tensor(8.4996, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "147 tensor(8.1378, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "148 tensor(7.7898, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "149 tensor(7.4544, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "150 tensor(7.1316, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "151 tensor(6.8219, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "152 tensor(6.5239, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "153 tensor(6.2375, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "154 tensor(5.9627, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "155 tensor(5.6986, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "156 tensor(5.4451, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "157 tensor(5.2019, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "158 tensor(4.9686, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "159 tensor(4.7452, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "160 tensor(4.5305, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "161 tensor(4.3250, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "162 tensor(4.1280, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "163 tensor(3.9393, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "164 tensor(3.7586, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "165 tensor(3.5855, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "166 tensor(3.4199, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "167 tensor(3.2614, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "168 tensor(3.1095, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "169 tensor(2.9642, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "170 tensor(2.8254, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "171 tensor(2.6926, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "172 tensor(2.5656, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "173 tensor(2.4441, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "174 tensor(2.3284, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "175 tensor(2.2178, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "176 tensor(2.1122, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "177 tensor(2.0113, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "178 tensor(1.9150, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "179 tensor(1.8231, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "180 tensor(1.7355, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "181 tensor(1.6520, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "182 tensor(1.5725, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "183 tensor(1.4965, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "184 tensor(1.4240, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "185 tensor(1.3550, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "186 tensor(1.2892, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "187 tensor(1.2265, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "188 tensor(1.1668, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "189 tensor(1.1098, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "190 tensor(1.0556, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "191 tensor(1.0039, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "192 tensor(0.9546, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "193 tensor(0.9077, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "194 tensor(0.8630, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "195 tensor(0.8205, device='cuda:2', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 tensor(0.7798, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "197 tensor(0.7413, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "198 tensor(0.7044, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "199 tensor(0.6694, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "200 tensor(0.6361, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "201 tensor(0.6043, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "202 tensor(0.5741, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "203 tensor(0.5454, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "204 tensor(0.5181, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "205 tensor(0.4921, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "206 tensor(0.4675, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "207 tensor(0.4440, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "208 tensor(0.4217, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "209 tensor(0.4005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "210 tensor(0.3803, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "211 tensor(0.3611, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "212 tensor(0.3428, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "213 tensor(0.3255, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "214 tensor(0.3090, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "215 tensor(0.2933, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "216 tensor(0.2784, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "217 tensor(0.2643, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "218 tensor(0.2509, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "219 tensor(0.2381, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "220 tensor(0.2260, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "221 tensor(0.2144, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "222 tensor(0.2035, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "223 tensor(0.1931, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "224 tensor(0.1832, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "225 tensor(0.1739, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "226 tensor(0.1650, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "227 tensor(0.1566, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "228 tensor(0.1486, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "229 tensor(0.1410, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "230 tensor(0.1338, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "231 tensor(0.1270, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "232 tensor(0.1205, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "233 tensor(0.1144, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "234 tensor(0.1085, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "235 tensor(0.1030, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "236 tensor(0.0977, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "237 tensor(0.0927, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "238 tensor(0.0879, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "239 tensor(0.0834, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "240 tensor(0.0792, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "241 tensor(0.0751, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "242 tensor(0.0712, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "243 tensor(0.0676, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "244 tensor(0.0641, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "245 tensor(0.0608, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "246 tensor(0.0577, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "247 tensor(0.0547, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "248 tensor(0.0519, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "249 tensor(0.0492, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "250 tensor(0.0467, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "251 tensor(0.0443, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "252 tensor(0.0420, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "253 tensor(0.0398, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "254 tensor(0.0377, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "255 tensor(0.0358, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "256 tensor(0.0339, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "257 tensor(0.0322, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "258 tensor(0.0305, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "259 tensor(0.0289, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "260 tensor(0.0274, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "261 tensor(0.0260, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "262 tensor(0.0246, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "263 tensor(0.0233, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "264 tensor(0.0221, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "265 tensor(0.0210, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "266 tensor(0.0199, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "267 tensor(0.0188, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "268 tensor(0.0178, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "269 tensor(0.0169, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "270 tensor(0.0160, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "271 tensor(0.0152, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "272 tensor(0.0144, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "273 tensor(0.0136, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "274 tensor(0.0129, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "275 tensor(0.0122, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "276 tensor(0.0116, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "277 tensor(0.0110, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "278 tensor(0.0104, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "279 tensor(0.0098, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "280 tensor(0.0093, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "281 tensor(0.0088, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "282 tensor(0.0083, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "283 tensor(0.0079, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "284 tensor(0.0075, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "285 tensor(0.0071, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "286 tensor(0.0067, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "287 tensor(0.0063, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "288 tensor(0.0060, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "289 tensor(0.0057, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "290 tensor(0.0054, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "291 tensor(0.0051, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "292 tensor(0.0048, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "293 tensor(0.0045, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "294 tensor(0.0043, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "295 tensor(0.0041, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "296 tensor(0.0038, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "297 tensor(0.0036, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "298 tensor(0.0034, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "299 tensor(0.0033, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "300 tensor(0.0031, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "301 tensor(0.0029, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "302 tensor(0.0028, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "303 tensor(0.0026, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "304 tensor(0.0025, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "305 tensor(0.0023, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "306 tensor(0.0022, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "307 tensor(0.0021, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "308 tensor(0.0020, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "309 tensor(0.0019, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "310 tensor(0.0018, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "311 tensor(0.0017, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "312 tensor(0.0016, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "313 tensor(0.0015, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "314 tensor(0.0014, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "315 tensor(0.0013, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "316 tensor(0.0012, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "317 tensor(0.0012, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "318 tensor(0.0011, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "319 tensor(0.0010, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "320 tensor(0.0010, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "321 tensor(0.0009, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "322 tensor(0.0009, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "323 tensor(0.0008, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "324 tensor(0.0008, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "325 tensor(0.0007, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "326 tensor(0.0007, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "327 tensor(0.0007, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "328 tensor(0.0006, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "329 tensor(0.0006, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "330 tensor(0.0006, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "331 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "332 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "333 tensor(0.0005, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "334 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "335 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "336 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "337 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "338 tensor(0.0004, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "339 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "340 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "341 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "342 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "343 tensor(0.0003, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "344 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "345 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "346 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "347 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "348 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "349 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "350 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "351 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "352 tensor(0.0002, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "353 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "354 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "355 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "356 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "357 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "358 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "359 tensor(0.0001, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "360 tensor(9.8113e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "361 tensor(9.2498e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "362 tensor(8.7192e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "363 tensor(8.2188e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "364 tensor(7.7463e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "365 tensor(7.2999e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "366 tensor(6.8792e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "367 tensor(6.4821e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "368 tensor(6.1071e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "369 tensor(5.7539e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "370 tensor(5.4201e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "371 tensor(5.1054e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "372 tensor(4.8083e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "373 tensor(4.5283e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "374 tensor(4.2644e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "375 tensor(4.0149e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "376 tensor(3.7802e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "377 tensor(3.5584e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "378 tensor(3.3497e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "379 tensor(3.1524e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "380 tensor(2.9669e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "381 tensor(2.7919e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "382 tensor(2.6268e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "383 tensor(2.4713e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "384 tensor(2.3247e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "385 tensor(2.1866e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "386 tensor(2.0566e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "387 tensor(1.9340e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "388 tensor(1.8184e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "389 tensor(1.7096e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "390 tensor(1.6070e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "391 tensor(1.5107e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "392 tensor(1.4197e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "393 tensor(1.3342e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "394 tensor(1.2535e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "395 tensor(1.1778e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "396 tensor(1.1064e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "397 tensor(1.0392e-05, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "398 tensor(9.7581e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "399 tensor(9.1637e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "400 tensor(8.6041e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "401 tensor(8.0776e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "402 tensor(7.5829e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "403 tensor(7.1169e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "404 tensor(6.6782e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "405 tensor(6.2665e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "406 tensor(5.8787e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "407 tensor(5.5142e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "408 tensor(5.1713e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409 tensor(4.8502e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "410 tensor(4.5479e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "411 tensor(4.2641e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "412 tensor(3.9972e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "413 tensor(3.7461e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "414 tensor(3.5103e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "415 tensor(3.2887e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "416 tensor(3.0813e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "417 tensor(2.8865e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "418 tensor(2.7037e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "419 tensor(2.5317e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "420 tensor(2.3702e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "421 tensor(2.2194e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "422 tensor(2.0773e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "423 tensor(1.9444e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "424 tensor(1.8189e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "425 tensor(1.7020e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "426 tensor(1.5924e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "427 tensor(1.4893e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "428 tensor(1.3926e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "429 tensor(1.3019e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "430 tensor(1.2177e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "431 tensor(1.1377e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "432 tensor(1.0633e-06, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "433 tensor(9.9384e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "434 tensor(9.2824e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "435 tensor(8.6755e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "436 tensor(8.0978e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "437 tensor(7.5646e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "438 tensor(7.0598e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "439 tensor(6.5903e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "440 tensor(6.1528e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "441 tensor(5.7417e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "442 tensor(5.3569e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "443 tensor(4.9980e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "444 tensor(4.6624e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "445 tensor(4.3506e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "446 tensor(4.0552e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "447 tensor(3.7788e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "448 tensor(3.5218e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "449 tensor(3.2822e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "450 tensor(3.0567e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "451 tensor(2.8493e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "452 tensor(2.6552e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "453 tensor(2.4709e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "454 tensor(2.3011e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "455 tensor(2.1414e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "456 tensor(1.9941e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "457 tensor(1.8562e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "458 tensor(1.7273e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "459 tensor(1.6068e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "460 tensor(1.4953e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "461 tensor(1.3888e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "462 tensor(1.2929e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "463 tensor(1.2011e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "464 tensor(1.1172e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "465 tensor(1.0374e-07, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "466 tensor(9.6354e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "467 tensor(8.9561e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "468 tensor(8.3162e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "469 tensor(7.7263e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "470 tensor(7.1789e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "471 tensor(6.6775e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "472 tensor(6.1865e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "473 tensor(5.7493e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "474 tensor(5.3353e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "475 tensor(4.9440e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "476 tensor(4.5927e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "477 tensor(4.2581e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "478 tensor(3.9451e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "479 tensor(3.6625e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "480 tensor(3.4024e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "481 tensor(3.1479e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "482 tensor(2.9234e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "483 tensor(2.7093e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "484 tensor(2.5060e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "485 tensor(2.3284e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "486 tensor(2.1546e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "487 tensor(1.9988e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "488 tensor(1.8525e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "489 tensor(1.7140e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "490 tensor(1.5869e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "491 tensor(1.4766e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "492 tensor(1.3634e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "493 tensor(1.2669e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "494 tensor(1.1720e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "495 tensor(1.0848e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "496 tensor(1.0095e-08, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "497 tensor(9.3150e-09, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "498 tensor(8.6265e-09, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "499 tensor(8.0137e-09, device='cuda:2', grad_fn=<MseLossBackward>)\n",
      "运行时间1 0.9742746353149414\n",
      "运行时间2 0.9742128849029541\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#把初始化和激活层放在CLASS里\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self,D_in,H,D_out):\n",
    "        super(TwoLayerNet,self).__init__()\n",
    "        #定义模型框架，两个线性层\n",
    "        self.linear1=torch.nn.Linear(D_in,H,bias=False)\n",
    "        self.linear2=torch.nn.Linear(H,D_out,bias=False)\n",
    "    def forward(self,x):\n",
    "        y_pred=self.linear2(self.linear1(x).clamp(min=0))\n",
    "        return y_pred\n",
    "model=TwoLayerNet(D_in,H,D_out)\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn#各种神经网络定义方法\n",
    "import time\n",
    "time3=time.time()\n",
    "\n",
    "dtype = torch.float\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:2\") # Uncomment this to run on GPU\n",
    "time1=time.time()\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "'''\n",
    "\n",
    "不指名device可能随便搬到某个设备上，\n",
    "\n",
    "因为有很多显卡，会造成严重后果\n",
    "\n",
    "'''\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)#Torch默认没有grad，为了节约内存\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)#x,y是训练数据，不需要知道梯度\n",
    "\n",
    "#因为模型很简单，不需要很小的学习率\n",
    "learning_rate = 1e-4\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    '''\n",
    "    super:调用父类方法，指超级继承\n",
    "    定义类，实例化的时候会先走init，但是你继承了别的类，自己类里的init覆盖了原来类的init，\n",
    "    为了让原来的类里的init不失效，就要用super来超级继承？？？\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,D_in,H,D_out):\n",
    "        super(TwoLayerNet,self).__init__()\n",
    "        #定义模型框架，两个线性层\n",
    "        self.linear1=torch.nn.Linear(D_in,H,bias=False)\n",
    "        self.linear2=torch.nn.Linear(H,D_out,bias=False)\n",
    "    def forward(self,x):\n",
    "        y_pred=self.linear2(self.linear1(x).clamp(min=0))\n",
    "        return y_pred\n",
    "model=TwoLayerNet(D_in,H,D_out)\n",
    "\n",
    "\n",
    "model=model.cuda(\"cuda:2\")#使用GPU一定要写\n",
    "\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "\n",
    "#用了Model后，loss在循环外面定义好\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "#定义优化器\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    #h = x.mm(w1)\n",
    "    #h_relu = h.clamp(min=0)\n",
    "    #y_pred = h_relu.mm(w2)\n",
    "    #一行写完\n",
    "    #y_pred=x.mm(w1).clamp(min=0).mm(w2)\n",
    "    #使用Model后更简单\n",
    "    model.zero_grad()#把所有grad全部清零\n",
    "    \n",
    "    y_pred=model(x)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    #loss = (y_pred - y).pow(2).sum()\n",
    "    #用Model更简单\n",
    "    loss=loss_fn(y_pred,y)\n",
    "    \n",
    "    #loss不是公式，其实是计算图computation graph，把所有相关运算都包含了\n",
    "    print(t, loss)\n",
    "    \n",
    "    optimizer.zero_grad()#反向传播之前清空梯度\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    #grad_y_pred = 2.0 * (y_pred - y)\n",
    "    #grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    #grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    #grad_h = grad_h_relu.clone()\n",
    "    #grad_h[h < 0] = 0\n",
    "    #grad_w1 = x.t().mm(grad_h)\n",
    "    #一行代替上面所有求导步骤\n",
    "    loss.backward()\n",
    "    \n",
    "    #一步更新\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "time2=time.time()\n",
    "print('运行时间1',(time2-time3))\n",
    "print('运行时间2',(time2-time1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结回顾\n",
    "__init__要把每一个有导数的层都定义好，搭建框架\n",
    "\n",
    "forward()是前向传播\n",
    "\n",
    "无论代码多长，只做几件事\n",
    "\n",
    "1、定义输入输出数据，定义模型（init和forward）\n",
    "\n",
    "2、定义loss_fn\n",
    "\n",
    "3、把model交给optimizer\n",
    "\n",
    "4、进入训练过程，y_pred=model(x),再把y_pred和y比较得到loss,清零梯度，loss.backward(),优化器优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
