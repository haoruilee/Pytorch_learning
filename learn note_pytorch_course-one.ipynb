{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd #读取csv文件的库\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "#让输出图形直接在Notebook中显示\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat a nn model\n",
    "def neu(x):\n",
    "    torch.nn.Sequential(\n",
    "    torch.nn.Linar(input_size,hidden_size),#from input to hidden,yi ge xianxing yunsuan,shuru weidu inputsize,yinhanceng weidu hiddensize\n",
    "    torch.nn.Sigmoid(),#Sigmoid zuoyong dao meige yinhanceng shenjingyuan\n",
    "    torch.nn.Linear(hidden_size,output_size),#xianxing yunsuan shuchu\n",
    ")#Sequential bingbu duiying mouge danyuan, duiying de shi danyuan zhijian de lianjie\n",
    "\n",
    "#define loss\n",
    "cost=torch.nn.MSELoss()#MSE dengjiayu torch.mean((y-y*)^2)\n",
    "#cost shi hanshu zhizhen\n",
    "\n",
    "#define Optimizer\n",
    "optimizer=torch.optim.SGD(neu.parameters(),lr=0.01)#SGD: suiji tidu xiajiang\n",
    "#neu.parameters() fanhui neu zhong suoyoude quanzhong/pianzhi canshu\n",
    "\n",
    "\n",
    "# 定义神经网络架构，features.shape[1]个输入层单元，10个隐含层，1个输出层\n",
    "input_size = features.shape[1] #输入层单元个数\n",
    "hidden_size = 10 #隐含层单元个数\n",
    "output_size = 1 #输出层单元个数\n",
    "batch_size = 128 #每隔batch的记录数\n",
    "weights1 = Variable(torch.randn([input_size, hidden_size]), requires_grad = True) #第一到二层权重\n",
    "biases1 = Variable(torch.randn([hidden_size]), requires_grad = True) #隐含层偏置\n",
    "weights2 = Variable(torch.randn([hidden_size, output_size]), requires_grad = True) #隐含层到输出层权重\n",
    "\n",
    "\n",
    "\n",
    "# 神经网络训练循环\n",
    "losses = []\n",
    "for i in range(1000):\n",
    "    # 每128个样本点被划分为一个撮，在循环的时候一批一批地读取\n",
    "    batch_loss = []\n",
    "    # start和end分别是提取一个batch数据的起始和终止下标\n",
    "    for start in range(0, len(X), batch_size):\n",
    "        end = start + batch_size if start + batch_size < len(X) else len(X)\n",
    "        xx = Variable(torch.FloatTensor(X[start:end]))#X daibiao zhenshi shuju\n",
    "        yy = Variable(torch.FloatTensor(Y[start:end]))#Y daibiao yuce shuju\n",
    "        predict = neu(xx)#yunxing yici yuce\n",
    "        loss = cost(predict, yy)#jisuan loss\n",
    "        zero_grad()\n",
    "        loss.backward()#ba loss fanxiang chuanbo\n",
    "        optimizer_step(0.01)#youhuaqi qianjin yibu\n",
    "        batch_loss.append(loss.data.numpy())\n",
    "    \n",
    "    # 每隔100步输出一下损失值（loss）\n",
    "    if i % 100==0:\n",
    "        losses.append(np.mean(batch_loss))\n",
    "        print(i, np.mean(batch_loss))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
